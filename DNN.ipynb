{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ob8SuLIDb0_S",
        "outputId": "11f083cb-878c-45e6-b768-4ec050c3ee01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ardhtq8ocpmK"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install DNN"
      ],
      "metadata": {
        "id": "VCNvnphXNIqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import DNN\n",
        "\n",
        "import requests\n",
        "response = requests.get('https://example.com/intents.json')\n",
        "intents = json.loads(response.content)\n",
        "\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?']\n",
        "data_file = open('intents.json', encoding=\"utf8\").read()\n",
        "intents = json.loads(data_file)\n",
        "\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    words.extend(w)\n",
        "    documents.append((w, intent['tag']))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)\n",
        "\n",
        "pickle.dump(words,open('texts1.pkl','wb'))\n",
        "pickle.dump(classes,open('labels1.pkl','wb'))\n",
        "\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "output_empty = [0]*len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "    pattern_words = doc[0]\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data Created\")\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "model.fit(train_x, train_y, n_epoch=100, batch_size=8, show_metric=True)\n",
        "model.save('model.tflearn')\n",
        "print(\"model created\")\n",
        "\n",
        "\n",
        "nltk.download('popular')\n",
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "model.load('./model.tflearn')\n",
        "import json\n",
        "import random\n",
        "intents = json.loads(open('intents.json', encoding=\"utf8\").read())\n",
        "words = pickle.load(open('texts1.pkl','rb'))\n",
        "classes = pickle.load(open('labels1.pkl','rb'))\n",
        "\n",
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "  return sentence_words\n",
        "\n",
        "def bow(sentence, words, show_details=False):\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  bag = [0]*len(words)\n",
        "  for s in sentence_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w == s:\n",
        "        bag[i] = 1\n",
        "        if show_details:\n",
        "          print(\"found in bag: %s\" % w)\n",
        "\n",
        "  return(np.array(bag))\n",
        "\n",
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "\n",
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res\n",
        "\n",
        "\n",
        "from flask import Flask, render_template, request\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.static_folder = 'static'\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/get\")\n",
        "def get_bot_response():\n",
        "    userText = request.args.get('msg')\n",
        "    return chatbot_response(userText)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug = False, host = '0.0.0.0', port = '5000')\n",
        ""
      ],
      "metadata": {
        "id": "O4P3L5NKM2G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oxGUhRXdZpw"
      },
      "outputs": [],
      "source": [
        "with open('intents.json') as json_data:\n",
        "  intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moZ7QdxJjZxE"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?']\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    w = nltk.word_tokenize(pattern)\n",
        "    words.extend(w)\n",
        "    documents.append((w, intent['tag']))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUqR5Vy5dZyH",
        "outputId": "62a23995-39fc-41e8-9c7f-0a847c99b0b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "963 documents\n",
            "210 classes [' Global_Industry_Classification_Standard', ' financial statements', 'Common Stock ', 'Corporation', 'Financial_Industry_Regulatory_Authority', 'Government_stability', 'Index', 'Over_the_Counter', 'Risk_lover ', 'Sectors_are_there_to_invest_in_Stock_Market ', 'Self_Regulatory_Organization', '_DVR_shares_', '_FINRA_financial_SRO_', '_HDFC-stocks-failing_', '_IPO_', '_IPO_investment_right_for_an_investor_', '_IPO_works_in_companies', '_Invest_ment', '_Investment_analyst_', '_OTC_stocks_publicly_traded_', '_advanatages_of_derivatives_', '_advantages_of_delisting_', '_advantages_of_nsdl_', '_bank_nifty_', '_beginners-buy-stocks_', '_benefits_of_nse_', '_bid_in_ipo_', '_bigger_NSE_or_BSE_', '_biggest_ipo_in_india_', '_bombay_stock_exchange_', '_broker_dealer_', '_buy-hdfc-stock_', '_buy-share-any-time_', '_cancel-multiple-orders_', '_change_in_budget_', '_charges_and_taxes_on_derivative_contracts_', '_check-my-margin-details_', '_common_stocks_referred_by_companies_', '_communication_services_sector_', '_companies_on_stock_market_', '_companies_under_nifty_50_', '_company_be_delisted_', '_consumer_discretionary_sector_', '_consumer_staples_sector_', '_convert-and-square off-orders_', '_creditor_', '_cut_off_price_', '_delisted_stock_be_relisted_', '_demat_account_with_nsdl_', '_depository_', '_derivative_', '_derivative_trading_', '_difference_between_listed stock_and_IPO_', '_do_companies_get_delisted_', '_does_share_holder_do_', '_eligibility_criteria_of_nifty_index_', '_energy_sector_', '_financial_market_system_is_classified_', '_financials_sector_', '_find_good_companies_', '_finra_discipline_offenders_', '_first_stock_exchange_', '_first_stock_market_', '_floor_price_', '_forms_of_stocks_', '_function_of_nsdl_', '_fundamental_analysis_', '_future-of-hdfc-share_', '_gdp_data_', '_gics_stock_market_sectors_', '_gold_prices_', '_good_to_be_risk_averse_', '_good_to_invest_in_IPO_', '_government_stability_', '_happen_after_we_invested_in_IPO_', '_happen_if_we_invest_in_IPO_', '_happens_when_stock_delist_', '_healthcare_sector_', '_impact_of_nse_stock_market_', '_important_points_to_consider_before_investing_in_an_IPO_', \"_india's_biggest_ipo's_\", '_indias_major_stock_exchange_', '_industrials_sector_', '_industry_production_index_', '_information-about-trading-accounts_', '_information_technology_sector_', '_invest-in-hdfc-share_', '_investors_impacted_by_delisting_', '_involuntary_delisting_', '_ipo_bidding_process_', '_key_features_of_nifty_futures_', '_key_terms_of_understanding_options_', '_listed_companies_in_NSE_', '_listed_in_stock_exchange_', '_listing_date_', '_listing_requirements_', '_listing_requirements_does_nasdaq_have_', '_london_stock_exchange_', '_look-for-when-buying-stock_', '_lot_size_', '_margin_money_in_derivatives_trading_', '_market_', '_market_order_', '_market_order_should_be_placed_', '_materials_sector_', '_measures_need_to_take_before_inesting_in_IPO_', '_modify-and-cancel-orders_', '_nasdaq_', '_national_association_of_securities_dealers_', '_national_securities_depository_limited_', '_national_stock_exchange_', '_needs_to_be_considered_while_participating_in_IPO_', '_nifty_', '_nifty_50_', '_nifty_based_derivatives_', '_nifty_computed_', '_nifty_futures_contract_work_', '_notification-after-placing-order_', '_nsdl_account_', '_nsdl_safe_', '_offerings_done_by_IPO_', '_open_interest_tells_us_', '_option_trading_', '_options_', '_options_check_on_pricing_', '_options_trading_works_', '_order-history_', '_order-types_', '_outstanding_shares_', '_owning_of_shares_', '_performance_of_international_markets_', '_place-order_', '_predict-of-stock-prices_', '_prerequisites_for_trading_in_derivatives_', '_present_stock_exchanges_india_', '_product-types_', '_quick-quote_', '_rating_upgrade_or_downgrade', '_real_estate_sector_', '_recession_or_economic_boom_', '_regulators_in_stock_market_', '_return_of_ipo_in_an_image_', '_rewards_of_investing_in_an_ipo_in_india_', '_risk_averse_', '_risk_lover_', '_safe-invest-through-hdfc-securities', '_sec_accountable_to_', '_sec_an_sro_', '_sec_make_new_rules_', '_securities_exchange_commission_', '_sell_delisted_shares_', '_shares_and_stock_same_', '_should-invest-in-hdfc', '_six_day_ipo_bid_process_', '_sort_of_investment_does_share_do_', '_sro_do_', '_start_trading_options_', '_stock_exchange_better_', '_stock_exchanges_work_', '_stock_market_allows_', '_stocks_quote_work_', '_subscription_of_shares_is_full_', '_technical_analysis_', '_total_companies_listed_in_BSE_', '_trade_derivatives_', '_trading_work_in_stock_exchange_', '_types_of_derivatives_', '_underwriter_', \"_upcoming_ipo's_india_\", '_utilities_sector_', '_volatility_in_fuel_prices_', '_voluntary_delisting_', '_work_of_broker_dealer_', '_work_of_stocks_', \"_world's_biggest_ipo's_\", 'asset', 'company stock in stock market', 'compare_common_shares_to_preferred_shares', 'convertible preferred stock', 'earnings', 'equity_shares', 'factors_affect_Nifty', 'general_to_shares_and_stocks_', 'goodbye', 'greetings', 'hold_shares_in_companies', 'if_Nifty_increases', 'inflation_', 'invest_in_DVR_shares ', 'lia_bility', 'nifty_decreases ', 'options', 'preemptive_rights', 'preference_shares', 'price_and_OI', 'primary_market', 'share', 'share holder', 'shares_issued_by_companies', 'stock', 'stock brokers', 'stock market', 'stock work', 'stock_certificate', 'stocks_choose_by_company_owners', 'types of stocks', 'venture_capital', 'version_update', 'voting_rights', 'work_of_shares_in_company']\n",
            "358 unique stemmed words ['&', \"'s\", '(', ')', ',', '50', 'a', 'about', 'account', 'accout', 'adav', 'adv', 'advanatg', 'affect', 'aft', 'al', 'allow', 'an', 'analys', 'analyst', 'and', 'any', 'anytim', 'ar', 'as', 'asset', 'assocy', 'auth', 'avail', 'avers', 'bank', 'bas', 'be', 'bef', 'begin', 'benefit', 'bet', 'betw', 'between', 'bid', 'big', 'biggest', 'bombay', 'boom', 'brief', 'brok', 'broker-deal', 'bse', 'budget', 'buy', 'by', 'bye', 'can', 'cancel', 'capit', 'cert', 'certifiact', 'chang', 'charg', 'chat', 'check', 'choos', 'class', 'com', 'comapany', 'commit', 'common', 'commonstock', 'commun', 'comp', 'company', 'comparison', 'comput', 'consid', 'consum', 'contract', 'convert', 'coontract', 'copany', 'corp', 'could', 'count', 'credit', 'criter', 'cut', 'cutoff', 'dat', 'day', 'deal', 'decreas', 'defin', 'dein', 'del', 'dem', 'demand', 'deposit', 'der', 'deriva', 'dery', 'describ', 'detail', 'diff', 'disciplin', 'discret', 'disret', 'do', 'doe', 'don', 'downgard', 'dvr', 'earn', 'econom', 'elig', 'energy', 'equ', 'equitysh', 'est', 'exchang', 'fact', 'fail', 'feat', 'fin', 'find', 'finr', 'first', 'flo', 'fo', 'for', 'form', 'fuel', 'ful', 'funct', 'funda', 'fut', 'gdp', 'gen', 'get', 'gic', 'glob', 'gold', 'good', 'goodby', 'govern', 'governemnt', 'greet', 'hap', 'hav', 'hdfc', 'healthc', 'hello', 'help', 'hey', 'hi', 'hist', 'hold', 'how', 'i', 'if', 'iip', 'im', 'impact', 'in', 'increas', 'ind', 'index', 'india', 'indust', 'industrail', 'industry', 'infl', 'inform', 'interest', 'intern', 'invest', 'investmet', 'involunt', 'involv', 'ipo', 'is', 'issu', 'it', 'key', 'lat', 'levy', 'liabl', 'limit', 'limt', 'list', 'london', 'look', 'lot', 'lov', 'maj', 'mak', 'many', 'margin', 'market', 'mat', 'me', 'mean', 'meant', 'meas', 'mod', 'money', 'mor', 'much', 'multipl', 'my', 'nasadq', 'nasd', 'nasdaq', 'nat', 'nee', 'new', 'next', 'nic', 'nifty', 'nifty50', 'noney', 'not', 'nsdl', 'nse', 'of', 'off', 'offend', 'oi', 'on', 'ont', 'op', 'opt', 'or', 'ord', 'org', 'otc', 'oustand', 'outstand', 'ov', 'over-the-count', 'own', 'particip', 'perform', 'plac', 'point', 'predict', 'preempt', 'preemptiveright', 'pref', 'prefer', 'prerequisit', 'pres', 'pric', 'prim', 'process', 'produc', 'provid', 'publ', 'quick', 'quot', 'rat', 'real', 'recess', 'refer', 'reg', 'rel', 'requir', 'resl', 'return', 'reward', 'right', 'risk', 'risl', 'rul', 'saf', 'sam', 'sec', 'secc', 'sect', 'see', 'sel', 'self-regulatory', 'ser', 'serv', 'shar', 'sharehold', 'should', 'six', 'siz', 'sock', 'softw', 'sort', 'sqa', 'squ', 'sro', 'stabl', 'standard', 'stapl', 'start', 'stat', 'stock', 'stockbrok', 'stockcert', 'stockmarket', 'subscrib', 'system', 'tak', 'tax', 'techn', 'technolog', 'tel', 'term', 'th', 'the', 'ther', 'thing', 'through', 'til', 'tim', 'to', 'today', 'tot', 'trad', 'typ', 'uitil', 'und', 'understand', 'underwrit', 'unerwrit', 'up', 'upcom', 'upd', 'upgrad', 'upgrade/downgrad', 'us', 'util', 'vc', 'vent', 'venturecapit', 'vert', 'volatil', 'volunt', 'voluntray', 'vot', 'was', 'we', 'wer', 'what', 'when', 'wher', 'which', 'whil', 'who', 'why', 'wil', 'with', 'work', 'world', 'yo', 'you']\n"
          ]
        }
      ],
      "source": [
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAJWcxZVvV-r"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(words,open('texts1.pkl','wb'))\n",
        "pickle.dump(classes,open('labels1.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eTRInjudZ2B",
        "outputId": "646abf41-532a-4781-92b1-60036edb1cd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-5c442f400a69>:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training)\n"
          ]
        }
      ],
      "source": [
        "training = []\n",
        "output = []\n",
        "\n",
        "output_empty = [0]*len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "  bag = []\n",
        "  pattern_words = doc[0]\n",
        "  pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "  for w in words:\n",
        "    bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "  training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbXWye8ZdZ4j",
        "outputId": "e87b3ee9-0812-4358-ea34-431716ca6211"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tflearn/initializations.py:164: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Step: 21712  | total loss: \u001b[1m\u001b[32m0.01352\u001b[0m\u001b[0m | time: 0.150s\n",
            "| Adam | epoch: 180 | loss: 0.01352 - acc: 0.9923 -- iter: 424/963\n",
            "Training Step: 21713  | total loss: \u001b[1m\u001b[32m0.01222\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 180 | loss: 0.01222 - acc: 0.9930 -- iter: 432/963\n",
            "Training Step: 21714  | total loss: \u001b[1m\u001b[32m0.01101\u001b[0m\u001b[0m | time: 0.160s\n",
            "| Adam | epoch: 180 | loss: 0.01101 - acc: 0.9937 -- iter: 440/963\n",
            "Training Step: 21715  | total loss: \u001b[1m\u001b[32m0.00993\u001b[0m\u001b[0m | time: 0.165s\n",
            "| Adam | epoch: 180 | loss: 0.00993 - acc: 0.9944 -- iter: 448/963\n",
            "Training Step: 21716  | total loss: \u001b[1m\u001b[32m0.00895\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 180 | loss: 0.00895 - acc: 0.9949 -- iter: 456/963\n",
            "Training Step: 21717  | total loss: \u001b[1m\u001b[32m0.00807\u001b[0m\u001b[0m | time: 0.172s\n",
            "| Adam | epoch: 180 | loss: 0.00807 - acc: 0.9954 -- iter: 464/963\n",
            "Training Step: 21718  | total loss: \u001b[1m\u001b[32m0.00731\u001b[0m\u001b[0m | time: 0.175s\n",
            "| Adam | epoch: 180 | loss: 0.00731 - acc: 0.9959 -- iter: 472/963\n",
            "Training Step: 21719  | total loss: \u001b[1m\u001b[32m0.00707\u001b[0m\u001b[0m | time: 0.178s\n",
            "| Adam | epoch: 180 | loss: 0.00707 - acc: 0.9963 -- iter: 480/963\n",
            "Training Step: 21720  | total loss: \u001b[1m\u001b[32m0.00639\u001b[0m\u001b[0m | time: 0.181s\n",
            "| Adam | epoch: 180 | loss: 0.00639 - acc: 0.9967 -- iter: 488/963\n",
            "Training Step: 21721  | total loss: \u001b[1m\u001b[32m0.00578\u001b[0m\u001b[0m | time: 0.185s\n",
            "| Adam | epoch: 180 | loss: 0.00578 - acc: 0.9970 -- iter: 496/963\n",
            "Training Step: 21722  | total loss: \u001b[1m\u001b[32m0.00523\u001b[0m\u001b[0m | time: 0.188s\n",
            "| Adam | epoch: 180 | loss: 0.00523 - acc: 0.9973 -- iter: 504/963\n",
            "Training Step: 21723  | total loss: \u001b[1m\u001b[32m0.00472\u001b[0m\u001b[0m | time: 0.192s\n",
            "| Adam | epoch: 180 | loss: 0.00472 - acc: 0.9976 -- iter: 512/963\n",
            "Training Step: 21724  | total loss: \u001b[1m\u001b[32m0.00428\u001b[0m\u001b[0m | time: 0.195s\n",
            "| Adam | epoch: 180 | loss: 0.00428 - acc: 0.9978 -- iter: 520/963\n",
            "Training Step: 21725  | total loss: \u001b[1m\u001b[32m0.00674\u001b[0m\u001b[0m | time: 0.199s\n",
            "| Adam | epoch: 180 | loss: 0.00674 - acc: 0.9980 -- iter: 528/963\n",
            "Training Step: 21726  | total loss: \u001b[1m\u001b[32m0.00622\u001b[0m\u001b[0m | time: 0.205s\n",
            "| Adam | epoch: 180 | loss: 0.00622 - acc: 0.9982 -- iter: 536/963\n",
            "Training Step: 21727  | total loss: \u001b[1m\u001b[32m0.00560\u001b[0m\u001b[0m | time: 0.208s\n",
            "| Adam | epoch: 180 | loss: 0.00560 - acc: 0.9984 -- iter: 544/963\n",
            "Training Step: 21728  | total loss: \u001b[1m\u001b[32m0.00507\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 180 | loss: 0.00507 - acc: 0.9986 -- iter: 552/963\n",
            "Training Step: 21729  | total loss: \u001b[1m\u001b[32m0.00457\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 180 | loss: 0.00457 - acc: 0.9987 -- iter: 560/963\n",
            "Training Step: 21730  | total loss: \u001b[1m\u001b[32m0.00428\u001b[0m\u001b[0m | time: 0.222s\n",
            "| Adam | epoch: 180 | loss: 0.00428 - acc: 0.9988 -- iter: 568/963\n",
            "Training Step: 21731  | total loss: \u001b[1m\u001b[32m0.00388\u001b[0m\u001b[0m | time: 0.226s\n",
            "| Adam | epoch: 180 | loss: 0.00388 - acc: 0.9990 -- iter: 576/963\n",
            "Training Step: 21732  | total loss: \u001b[1m\u001b[32m0.00351\u001b[0m\u001b[0m | time: 0.229s\n",
            "| Adam | epoch: 180 | loss: 0.00351 - acc: 0.9991 -- iter: 584/963\n",
            "Training Step: 21733  | total loss: \u001b[1m\u001b[32m0.01430\u001b[0m\u001b[0m | time: 0.232s\n",
            "| Adam | epoch: 180 | loss: 0.01430 - acc: 0.9867 -- iter: 592/963\n",
            "Training Step: 21734  | total loss: \u001b[1m\u001b[32m0.01311\u001b[0m\u001b[0m | time: 0.235s\n",
            "| Adam | epoch: 180 | loss: 0.01311 - acc: 0.9880 -- iter: 600/963\n",
            "Training Step: 21735  | total loss: \u001b[1m\u001b[32m0.01181\u001b[0m\u001b[0m | time: 0.238s\n",
            "| Adam | epoch: 180 | loss: 0.01181 - acc: 0.9892 -- iter: 608/963\n",
            "Training Step: 21736  | total loss: \u001b[1m\u001b[32m0.03473\u001b[0m\u001b[0m | time: 0.244s\n",
            "| Adam | epoch: 180 | loss: 0.03473 - acc: 0.9778 -- iter: 616/963\n",
            "Training Step: 21737  | total loss: \u001b[1m\u001b[32m0.03129\u001b[0m\u001b[0m | time: 0.247s\n",
            "| Adam | epoch: 180 | loss: 0.03129 - acc: 0.9800 -- iter: 624/963\n",
            "Training Step: 21738  | total loss: \u001b[1m\u001b[32m0.02819\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 180 | loss: 0.02819 - acc: 0.9820 -- iter: 632/963\n",
            "Training Step: 21739  | total loss: \u001b[1m\u001b[32m0.02539\u001b[0m\u001b[0m | time: 0.254s\n",
            "| Adam | epoch: 180 | loss: 0.02539 - acc: 0.9838 -- iter: 640/963\n",
            "Training Step: 21740  | total loss: \u001b[1m\u001b[32m0.02286\u001b[0m\u001b[0m | time: 0.258s\n",
            "| Adam | epoch: 180 | loss: 0.02286 - acc: 0.9854 -- iter: 648/963\n",
            "Training Step: 21741  | total loss: \u001b[1m\u001b[32m0.02058\u001b[0m\u001b[0m | time: 0.266s\n",
            "| Adam | epoch: 180 | loss: 0.02058 - acc: 0.9869 -- iter: 656/963\n",
            "Training Step: 21742  | total loss: \u001b[1m\u001b[32m0.01856\u001b[0m\u001b[0m | time: 0.270s\n",
            "| Adam | epoch: 180 | loss: 0.01856 - acc: 0.9882 -- iter: 664/963\n",
            "Training Step: 21743  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.275s\n",
            "| Adam | epoch: 180 | loss: 0.01672 - acc: 0.9894 -- iter: 672/963\n",
            "Training Step: 21744  | total loss: \u001b[1m\u001b[32m0.02079\u001b[0m\u001b[0m | time: 0.280s\n",
            "| Adam | epoch: 180 | loss: 0.02079 - acc: 0.9904 -- iter: 680/963\n",
            "Training Step: 21745  | total loss: \u001b[1m\u001b[32m0.01872\u001b[0m\u001b[0m | time: 0.283s\n",
            "| Adam | epoch: 180 | loss: 0.01872 - acc: 0.9914 -- iter: 688/963\n",
            "Training Step: 21746  | total loss: \u001b[1m\u001b[32m0.03896\u001b[0m\u001b[0m | time: 0.290s\n",
            "| Adam | epoch: 180 | loss: 0.03896 - acc: 0.9797 -- iter: 696/963\n",
            "Training Step: 21747  | total loss: \u001b[1m\u001b[32m0.03518\u001b[0m\u001b[0m | time: 0.293s\n",
            "| Adam | epoch: 180 | loss: 0.03518 - acc: 0.9818 -- iter: 704/963\n",
            "Training Step: 21748  | total loss: \u001b[1m\u001b[32m0.03168\u001b[0m\u001b[0m | time: 0.298s\n",
            "| Adam | epoch: 180 | loss: 0.03168 - acc: 0.9836 -- iter: 712/963\n",
            "Training Step: 21749  | total loss: \u001b[1m\u001b[32m0.02854\u001b[0m\u001b[0m | time: 0.302s\n",
            "| Adam | epoch: 180 | loss: 0.02854 - acc: 0.9852 -- iter: 720/963\n",
            "Training Step: 21750  | total loss: \u001b[1m\u001b[32m0.02570\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 180 | loss: 0.02570 - acc: 0.9867 -- iter: 728/963\n",
            "Training Step: 21751  | total loss: \u001b[1m\u001b[32m0.02315\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 180 | loss: 0.02315 - acc: 0.9880 -- iter: 736/963\n",
            "Training Step: 21752  | total loss: \u001b[1m\u001b[32m0.02085\u001b[0m\u001b[0m | time: 0.312s\n",
            "| Adam | epoch: 180 | loss: 0.02085 - acc: 0.9892 -- iter: 744/963\n",
            "Training Step: 21753  | total loss: \u001b[1m\u001b[32m0.01878\u001b[0m\u001b[0m | time: 0.317s\n",
            "| Adam | epoch: 180 | loss: 0.01878 - acc: 0.9903 -- iter: 752/963\n",
            "Training Step: 21754  | total loss: \u001b[1m\u001b[32m0.01692\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 180 | loss: 0.01692 - acc: 0.9913 -- iter: 760/963\n",
            "Training Step: 21755  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.322s\n",
            "| Adam | epoch: 180 | loss: 0.01526 - acc: 0.9922 -- iter: 768/963\n",
            "Training Step: 21756  | total loss: \u001b[1m\u001b[32m0.01374\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 180 | loss: 0.01374 - acc: 0.9929 -- iter: 776/963\n",
            "Training Step: 21757  | total loss: \u001b[1m\u001b[32m0.01275\u001b[0m\u001b[0m | time: 0.327s\n",
            "| Adam | epoch: 180 | loss: 0.01275 - acc: 0.9936 -- iter: 784/963\n",
            "Training Step: 21758  | total loss: \u001b[1m\u001b[32m0.01151\u001b[0m\u001b[0m | time: 0.332s\n",
            "| Adam | epoch: 180 | loss: 0.01151 - acc: 0.9943 -- iter: 792/963\n",
            "Training Step: 21759  | total loss: \u001b[1m\u001b[32m0.02095\u001b[0m\u001b[0m | time: 0.335s\n",
            "| Adam | epoch: 180 | loss: 0.02095 - acc: 0.9824 -- iter: 800/963\n",
            "Training Step: 21760  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.337s\n",
            "| Adam | epoch: 180 | loss: 0.01888 - acc: 0.9841 -- iter: 808/963\n",
            "Training Step: 21761  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.340s\n",
            "| Adam | epoch: 180 | loss: 0.01701 - acc: 0.9857 -- iter: 816/963\n",
            "Training Step: 21762  | total loss: \u001b[1m\u001b[32m0.01542\u001b[0m\u001b[0m | time: 0.342s\n",
            "| Adam | epoch: 180 | loss: 0.01542 - acc: 0.9871 -- iter: 824/963\n",
            "Training Step: 21763  | total loss: \u001b[1m\u001b[32m0.01397\u001b[0m\u001b[0m | time: 0.345s\n",
            "| Adam | epoch: 180 | loss: 0.01397 - acc: 0.9884 -- iter: 832/963\n",
            "Training Step: 21764  | total loss: \u001b[1m\u001b[32m0.01261\u001b[0m\u001b[0m | time: 0.377s\n",
            "| Adam | epoch: 180 | loss: 0.01261 - acc: 0.9896 -- iter: 840/963\n",
            "Training Step: 21765  | total loss: \u001b[1m\u001b[32m0.01149\u001b[0m\u001b[0m | time: 0.382s\n",
            "| Adam | epoch: 180 | loss: 0.01149 - acc: 0.9906 -- iter: 848/963\n",
            "Training Step: 21766  | total loss: \u001b[1m\u001b[32m0.01037\u001b[0m\u001b[0m | time: 0.386s\n",
            "| Adam | epoch: 180 | loss: 0.01037 - acc: 0.9916 -- iter: 856/963\n",
            "Training Step: 21767  | total loss: \u001b[1m\u001b[32m0.00941\u001b[0m\u001b[0m | time: 0.389s\n",
            "| Adam | epoch: 180 | loss: 0.00941 - acc: 0.9924 -- iter: 864/963\n",
            "Training Step: 21768  | total loss: \u001b[1m\u001b[32m0.00849\u001b[0m\u001b[0m | time: 0.393s\n",
            "| Adam | epoch: 180 | loss: 0.00849 - acc: 0.9932 -- iter: 872/963\n",
            "Training Step: 21769  | total loss: \u001b[1m\u001b[32m0.00772\u001b[0m\u001b[0m | time: 0.397s\n",
            "| Adam | epoch: 180 | loss: 0.00772 - acc: 0.9938 -- iter: 880/963\n",
            "Training Step: 21770  | total loss: \u001b[1m\u001b[32m0.00698\u001b[0m\u001b[0m | time: 0.401s\n",
            "| Adam | epoch: 180 | loss: 0.00698 - acc: 0.9945 -- iter: 888/963\n",
            "Training Step: 21771  | total loss: \u001b[1m\u001b[32m0.00630\u001b[0m\u001b[0m | time: 0.405s\n",
            "| Adam | epoch: 180 | loss: 0.00630 - acc: 0.9950 -- iter: 896/963\n",
            "Training Step: 21772  | total loss: \u001b[1m\u001b[32m0.00578\u001b[0m\u001b[0m | time: 0.413s\n",
            "| Adam | epoch: 180 | loss: 0.00578 - acc: 0.9955 -- iter: 904/963\n",
            "Training Step: 21773  | total loss: \u001b[1m\u001b[32m0.00522\u001b[0m\u001b[0m | time: 0.427s\n",
            "| Adam | epoch: 180 | loss: 0.00522 - acc: 0.9960 -- iter: 912/963\n",
            "Training Step: 21774  | total loss: \u001b[1m\u001b[32m0.00473\u001b[0m\u001b[0m | time: 0.434s\n",
            "| Adam | epoch: 180 | loss: 0.00473 - acc: 0.9964 -- iter: 920/963\n",
            "Training Step: 21775  | total loss: \u001b[1m\u001b[32m0.02346\u001b[0m\u001b[0m | time: 0.441s\n",
            "| Adam | epoch: 180 | loss: 0.02346 - acc: 0.9842 -- iter: 928/963\n",
            "Training Step: 21776  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.450s\n",
            "| Adam | epoch: 180 | loss: 0.02113 - acc: 0.9858 -- iter: 936/963\n",
            "Training Step: 21777  | total loss: \u001b[1m\u001b[32m0.01724\u001b[0m\u001b[0m | time: 0.458s\n",
            "| Adam | epoch: 180 | loss: 0.01724 - acc: 0.9872 -- iter: 944/963\n",
            "Training Step: 21778  | total loss: \u001b[1m\u001b[32m0.01555\u001b[0m\u001b[0m | time: 0.465s\n",
            "| Adam | epoch: 180 | loss: 0.01555 - acc: 0.9885 -- iter: 952/963\n",
            "Training Step: 21779  | total loss: \u001b[1m\u001b[32m0.01555\u001b[0m\u001b[0m | time: 0.470s\n",
            "| Adam | epoch: 180 | loss: 0.01555 - acc: 0.9897 -- iter: 960/963\n",
            "Training Step: 21780  | total loss: \u001b[1m\u001b[32m0.01403\u001b[0m\u001b[0m | time: 0.474s\n",
            "| Adam | epoch: 180 | loss: 0.01403 - acc: 0.9907 -- iter: 963/963\n",
            "--\n",
            "Training Step: 21781  | total loss: \u001b[1m\u001b[32m0.01263\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 181 | loss: 0.01263 - acc: 0.9916 -- iter: 008/963\n",
            "Training Step: 21782  | total loss: \u001b[1m\u001b[32m0.01901\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 181 | loss: 0.01901 - acc: 0.9925 -- iter: 016/963\n",
            "Training Step: 21783  | total loss: \u001b[1m\u001b[32m0.01713\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 181 | loss: 0.01713 - acc: 0.9807 -- iter: 024/963\n",
            "Training Step: 21784  | total loss: \u001b[1m\u001b[32m0.03357\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 181 | loss: 0.03357 - acc: 0.9826 -- iter: 032/963\n",
            "Training Step: 21785  | total loss: \u001b[1m\u001b[32m0.03357\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 181 | loss: 0.03357 - acc: 0.9719 -- iter: 040/963\n",
            "Training Step: 21786  | total loss: \u001b[1m\u001b[32m0.03027\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 181 | loss: 0.03027 - acc: 0.9747 -- iter: 048/963\n",
            "Training Step: 21787  | total loss: \u001b[1m\u001b[32m0.02725\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 181 | loss: 0.02725 - acc: 0.9772 -- iter: 056/963\n",
            "Training Step: 21788  | total loss: \u001b[1m\u001b[32m0.02459\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 181 | loss: 0.02459 - acc: 0.9795 -- iter: 064/963\n",
            "Training Step: 21789  | total loss: \u001b[1m\u001b[32m0.02215\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 181 | loss: 0.02215 - acc: 0.9815 -- iter: 072/963\n",
            "Training Step: 21790  | total loss: \u001b[1m\u001b[32m0.01996\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 181 | loss: 0.01996 - acc: 0.9834 -- iter: 080/963\n",
            "Training Step: 21791  | total loss: \u001b[1m\u001b[32m0.02430\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 181 | loss: 0.02430 - acc: 0.9851 -- iter: 088/963\n",
            "Training Step: 21792  | total loss: \u001b[1m\u001b[32m0.02190\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 181 | loss: 0.02190 - acc: 0.9865 -- iter: 096/963\n",
            "Training Step: 21793  | total loss: \u001b[1m\u001b[32m0.02321\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 181 | loss: 0.02321 - acc: 0.9879 -- iter: 104/963\n",
            "Training Step: 21794  | total loss: \u001b[1m\u001b[32m0.02096\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 181 | loss: 0.02096 - acc: 0.9891 -- iter: 112/963\n",
            "Training Step: 21795  | total loss: \u001b[1m\u001b[32m0.01889\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 181 | loss: 0.01889 - acc: 0.9902 -- iter: 120/963\n",
            "Training Step: 21796  | total loss: \u001b[1m\u001b[32m0.01703\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 181 | loss: 0.01703 - acc: 0.9912 -- iter: 128/963\n",
            "Training Step: 21797  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 181 | loss: 0.01535 - acc: 0.9921 -- iter: 136/963\n",
            "Training Step: 21798  | total loss: \u001b[1m\u001b[32m0.03787\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 181 | loss: 0.03787 - acc: 0.9804 -- iter: 144/963\n",
            "Training Step: 21799  | total loss: \u001b[1m\u001b[32m0.03413\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 181 | loss: 0.03413 - acc: 0.9823 -- iter: 152/963\n",
            "Training Step: 21800  | total loss: \u001b[1m\u001b[32m0.03073\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 181 | loss: 0.03073 - acc: 0.9841 -- iter: 160/963\n",
            "Training Step: 21801  | total loss: \u001b[1m\u001b[32m0.02767\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 181 | loss: 0.02767 - acc: 0.9857 -- iter: 168/963\n",
            "Training Step: 21802  | total loss: \u001b[1m\u001b[32m0.02492\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 181 | loss: 0.02492 - acc: 0.9871 -- iter: 176/963\n",
            "Training Step: 21803  | total loss: \u001b[1m\u001b[32m0.02248\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 181 | loss: 0.02248 - acc: 0.9884 -- iter: 184/963\n",
            "Training Step: 21804  | total loss: \u001b[1m\u001b[32m0.02026\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 181 | loss: 0.02026 - acc: 0.9896 -- iter: 192/963\n",
            "Training Step: 21805  | total loss: \u001b[1m\u001b[32m0.01826\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 181 | loss: 0.01826 - acc: 0.9906 -- iter: 200/963\n",
            "Training Step: 21806  | total loss: \u001b[1m\u001b[32m0.01644\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 181 | loss: 0.01644 - acc: 0.9915 -- iter: 208/963\n",
            "Training Step: 21807  | total loss: \u001b[1m\u001b[32m0.01481\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 181 | loss: 0.01481 - acc: 0.9924 -- iter: 216/963\n",
            "Training Step: 21808  | total loss: \u001b[1m\u001b[32m0.01335\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 181 | loss: 0.01335 - acc: 0.9931 -- iter: 224/963\n",
            "Training Step: 21809  | total loss: \u001b[1m\u001b[32m0.01207\u001b[0m\u001b[0m | time: 0.142s\n",
            "| Adam | epoch: 181 | loss: 0.01207 - acc: 0.9938 -- iter: 232/963\n",
            "Training Step: 21810  | total loss: \u001b[1m\u001b[32m0.01088\u001b[0m\u001b[0m | time: 0.145s\n",
            "| Adam | epoch: 181 | loss: 0.01088 - acc: 0.9945 -- iter: 240/963\n",
            "Training Step: 21811  | total loss: \u001b[1m\u001b[32m0.00980\u001b[0m\u001b[0m | time: 0.148s\n",
            "| Adam | epoch: 181 | loss: 0.00980 - acc: 0.9950 -- iter: 248/963\n",
            "Training Step: 21812  | total loss: \u001b[1m\u001b[32m0.00883\u001b[0m\u001b[0m | time: 0.151s\n",
            "| Adam | epoch: 181 | loss: 0.00883 - acc: 0.9955 -- iter: 256/963\n",
            "Training Step: 21813  | total loss: \u001b[1m\u001b[32m0.00966\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 181 | loss: 0.00966 - acc: 0.9960 -- iter: 264/963\n",
            "Training Step: 21814  | total loss: \u001b[1m\u001b[32m0.03823\u001b[0m\u001b[0m | time: 0.158s\n",
            "| Adam | epoch: 181 | loss: 0.03823 - acc: 0.9839 -- iter: 272/963\n",
            "Training Step: 21815  | total loss: \u001b[1m\u001b[32m0.03442\u001b[0m\u001b[0m | time: 0.161s\n",
            "| Adam | epoch: 181 | loss: 0.03442 - acc: 0.9855 -- iter: 280/963\n",
            "Training Step: 21816  | total loss: \u001b[1m\u001b[32m0.03101\u001b[0m\u001b[0m | time: 0.164s\n",
            "| Adam | epoch: 181 | loss: 0.03101 - acc: 0.9869 -- iter: 288/963\n",
            "Training Step: 21817  | total loss: \u001b[1m\u001b[32m0.02829\u001b[0m\u001b[0m | time: 0.167s\n",
            "| Adam | epoch: 181 | loss: 0.02829 - acc: 0.9882 -- iter: 296/963\n",
            "Training Step: 21818  | total loss: \u001b[1m\u001b[32m0.02548\u001b[0m\u001b[0m | time: 0.170s\n",
            "| Adam | epoch: 181 | loss: 0.02548 - acc: 0.9894 -- iter: 304/963\n",
            "Training Step: 21819  | total loss: \u001b[1m\u001b[32m0.02296\u001b[0m\u001b[0m | time: 0.173s\n",
            "| Adam | epoch: 181 | loss: 0.02296 - acc: 0.9905 -- iter: 312/963\n",
            "Training Step: 21820  | total loss: \u001b[1m\u001b[32m0.02360\u001b[0m\u001b[0m | time: 0.176s\n",
            "| Adam | epoch: 181 | loss: 0.02360 - acc: 0.9914 -- iter: 320/963\n",
            "Training Step: 21821  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.179s\n",
            "| Adam | epoch: 181 | loss: 0.02125 - acc: 0.9923 -- iter: 328/963\n",
            "Training Step: 21822  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.183s\n",
            "| Adam | epoch: 181 | loss: 0.01917 - acc: 0.9931 -- iter: 336/963\n",
            "Training Step: 21823  | total loss: \u001b[1m\u001b[32m0.01745\u001b[0m\u001b[0m | time: 0.186s\n",
            "| Adam | epoch: 181 | loss: 0.01745 - acc: 0.9937 -- iter: 344/963\n",
            "Training Step: 21824  | total loss: \u001b[1m\u001b[32m0.01572\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 181 | loss: 0.01572 - acc: 0.9944 -- iter: 352/963\n",
            "Training Step: 21825  | total loss: \u001b[1m\u001b[32m0.01424\u001b[0m\u001b[0m | time: 0.194s\n",
            "| Adam | epoch: 181 | loss: 0.01424 - acc: 0.9949 -- iter: 360/963\n",
            "Training Step: 21826  | total loss: \u001b[1m\u001b[32m0.05951\u001b[0m\u001b[0m | time: 0.200s\n",
            "| Adam | epoch: 181 | loss: 0.05951 - acc: 0.9829 -- iter: 368/963\n",
            "Training Step: 21827  | total loss: \u001b[1m\u001b[32m0.05385\u001b[0m\u001b[0m | time: 0.204s\n",
            "| Adam | epoch: 181 | loss: 0.05385 - acc: 0.9846 -- iter: 376/963\n",
            "Training Step: 21828  | total loss: \u001b[1m\u001b[32m0.04850\u001b[0m\u001b[0m | time: 0.212s\n",
            "| Adam | epoch: 181 | loss: 0.04850 - acc: 0.9862 -- iter: 384/963\n",
            "Training Step: 21829  | total loss: \u001b[1m\u001b[32m0.04366\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 181 | loss: 0.04366 - acc: 0.9876 -- iter: 392/963\n",
            "Training Step: 21830  | total loss: \u001b[1m\u001b[32m0.03931\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 181 | loss: 0.03931 - acc: 0.9888 -- iter: 400/963\n",
            "Training Step: 21831  | total loss: \u001b[1m\u001b[32m0.03552\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 181 | loss: 0.03552 - acc: 0.9899 -- iter: 408/963\n",
            "Training Step: 21832  | total loss: \u001b[1m\u001b[32m0.03199\u001b[0m\u001b[0m | time: 0.226s\n",
            "| Adam | epoch: 181 | loss: 0.03199 - acc: 0.9909 -- iter: 416/963\n",
            "Training Step: 21833  | total loss: \u001b[1m\u001b[32m0.05726\u001b[0m\u001b[0m | time: 0.229s\n",
            "| Adam | epoch: 181 | loss: 0.05726 - acc: 0.9793 -- iter: 424/963\n",
            "Training Step: 21834  | total loss: \u001b[1m\u001b[32m0.05158\u001b[0m\u001b[0m | time: 0.231s\n",
            "| Adam | epoch: 181 | loss: 0.05158 - acc: 0.9814 -- iter: 432/963\n",
            "Training Step: 21835  | total loss: \u001b[1m\u001b[32m0.04644\u001b[0m\u001b[0m | time: 0.237s\n",
            "| Adam | epoch: 181 | loss: 0.04644 - acc: 0.9833 -- iter: 440/963\n",
            "Training Step: 21836  | total loss: \u001b[1m\u001b[32m0.04181\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 181 | loss: 0.04181 - acc: 0.9849 -- iter: 448/963\n",
            "Training Step: 21837  | total loss: \u001b[1m\u001b[32m0.03766\u001b[0m\u001b[0m | time: 0.245s\n",
            "| Adam | epoch: 181 | loss: 0.03766 - acc: 0.9864 -- iter: 456/963\n",
            "Training Step: 21838  | total loss: \u001b[1m\u001b[32m0.03391\u001b[0m\u001b[0m | time: 0.252s\n",
            "| Adam | epoch: 181 | loss: 0.03391 - acc: 0.9878 -- iter: 464/963\n",
            "Training Step: 21839  | total loss: \u001b[1m\u001b[32m0.03053\u001b[0m\u001b[0m | time: 0.256s\n",
            "| Adam | epoch: 181 | loss: 0.03053 - acc: 0.9890 -- iter: 472/963\n",
            "Training Step: 21840  | total loss: \u001b[1m\u001b[32m0.02749\u001b[0m\u001b[0m | time: 0.262s\n",
            "| Adam | epoch: 181 | loss: 0.02749 - acc: 0.9901 -- iter: 480/963\n",
            "Training Step: 21841  | total loss: \u001b[1m\u001b[32m0.02475\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 181 | loss: 0.02475 - acc: 0.9911 -- iter: 488/963\n",
            "Training Step: 21842  | total loss: \u001b[1m\u001b[32m0.02240\u001b[0m\u001b[0m | time: 0.269s\n",
            "| Adam | epoch: 181 | loss: 0.02240 - acc: 0.9920 -- iter: 496/963\n",
            "Training Step: 21843  | total loss: \u001b[1m\u001b[32m0.02019\u001b[0m\u001b[0m | time: 0.273s\n",
            "| Adam | epoch: 181 | loss: 0.02019 - acc: 0.9928 -- iter: 504/963\n",
            "Training Step: 21844  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.279s\n",
            "| Adam | epoch: 181 | loss: 0.01819 - acc: 0.9935 -- iter: 512/963\n",
            "Training Step: 21845  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.282s\n",
            "| Adam | epoch: 181 | loss: 0.01639 - acc: 0.9942 -- iter: 520/963\n",
            "Training Step: 21846  | total loss: \u001b[1m\u001b[32m0.01476\u001b[0m\u001b[0m | time: 0.285s\n",
            "| Adam | epoch: 181 | loss: 0.01476 - acc: 0.9947 -- iter: 528/963\n",
            "Training Step: 21847  | total loss: \u001b[1m\u001b[32m0.01331\u001b[0m\u001b[0m | time: 0.288s\n",
            "| Adam | epoch: 181 | loss: 0.01331 - acc: 0.9953 -- iter: 536/963\n",
            "Training Step: 21848  | total loss: \u001b[1m\u001b[32m0.01200\u001b[0m\u001b[0m | time: 0.291s\n",
            "| Adam | epoch: 181 | loss: 0.01200 - acc: 0.9957 -- iter: 544/963\n",
            "Training Step: 21849  | total loss: \u001b[1m\u001b[32m0.01082\u001b[0m\u001b[0m | time: 0.294s\n",
            "| Adam | epoch: 181 | loss: 0.01082 - acc: 0.9962 -- iter: 552/963\n",
            "Training Step: 21850  | total loss: \u001b[1m\u001b[32m0.00977\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 181 | loss: 0.00977 - acc: 0.9966 -- iter: 560/963\n",
            "Training Step: 21851  | total loss: \u001b[1m\u001b[32m0.00883\u001b[0m\u001b[0m | time: 0.299s\n",
            "| Adam | epoch: 181 | loss: 0.00883 - acc: 0.9969 -- iter: 568/963\n",
            "Training Step: 21852  | total loss: \u001b[1m\u001b[32m0.00796\u001b[0m\u001b[0m | time: 0.302s\n",
            "| Adam | epoch: 181 | loss: 0.00796 - acc: 0.9972 -- iter: 576/963\n",
            "Training Step: 21853  | total loss: \u001b[1m\u001b[32m0.00720\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 181 | loss: 0.00720 - acc: 0.9975 -- iter: 584/963\n",
            "Training Step: 21854  | total loss: \u001b[1m\u001b[32m0.01275\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 181 | loss: 0.01275 - acc: 0.9977 -- iter: 592/963\n",
            "Training Step: 21855  | total loss: \u001b[1m\u001b[32m0.01157\u001b[0m\u001b[0m | time: 0.312s\n",
            "| Adam | epoch: 181 | loss: 0.01157 - acc: 0.9980 -- iter: 600/963\n",
            "Training Step: 21856  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.316s\n",
            "| Adam | epoch: 181 | loss: 0.01535 - acc: 0.9982 -- iter: 608/963\n",
            "Training Step: 21857  | total loss: \u001b[1m\u001b[32m0.01382\u001b[0m\u001b[0m | time: 0.319s\n",
            "| Adam | epoch: 181 | loss: 0.01382 - acc: 0.9984 -- iter: 616/963\n",
            "Training Step: 21858  | total loss: \u001b[1m\u001b[32m0.01246\u001b[0m\u001b[0m | time: 0.323s\n",
            "| Adam | epoch: 181 | loss: 0.01246 - acc: 0.9985 -- iter: 624/963\n",
            "Training Step: 21859  | total loss: \u001b[1m\u001b[32m0.01124\u001b[0m\u001b[0m | time: 0.326s\n",
            "| Adam | epoch: 181 | loss: 0.01124 - acc: 0.9987 -- iter: 632/963\n",
            "Training Step: 21860  | total loss: \u001b[1m\u001b[32m0.01016\u001b[0m\u001b[0m | time: 0.329s\n",
            "| Adam | epoch: 181 | loss: 0.01016 - acc: 0.9988 -- iter: 640/963\n",
            "Training Step: 21861  | total loss: \u001b[1m\u001b[32m0.00915\u001b[0m\u001b[0m | time: 0.333s\n",
            "| Adam | epoch: 181 | loss: 0.00915 - acc: 0.9989 -- iter: 648/963\n",
            "Training Step: 21862  | total loss: \u001b[1m\u001b[32m0.03502\u001b[0m\u001b[0m | time: 0.339s\n",
            "| Adam | epoch: 181 | loss: 0.03502 - acc: 0.9865 -- iter: 656/963\n",
            "Training Step: 21863  | total loss: \u001b[1m\u001b[32m0.03153\u001b[0m\u001b[0m | time: 0.346s\n",
            "| Adam | epoch: 181 | loss: 0.03153 - acc: 0.9879 -- iter: 664/963\n",
            "Training Step: 21864  | total loss: \u001b[1m\u001b[32m0.02865\u001b[0m\u001b[0m | time: 0.352s\n",
            "| Adam | epoch: 181 | loss: 0.02865 - acc: 0.9891 -- iter: 672/963\n",
            "Training Step: 21865  | total loss: \u001b[1m\u001b[32m0.02581\u001b[0m\u001b[0m | time: 0.356s\n",
            "| Adam | epoch: 181 | loss: 0.02581 - acc: 0.9902 -- iter: 680/963\n",
            "Training Step: 21866  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 181 | loss: 0.02325 - acc: 0.9912 -- iter: 688/963\n",
            "Training Step: 21867  | total loss: \u001b[1m\u001b[32m0.02094\u001b[0m\u001b[0m | time: 0.366s\n",
            "| Adam | epoch: 181 | loss: 0.02094 - acc: 0.9920 -- iter: 696/963\n",
            "Training Step: 21868  | total loss: \u001b[1m\u001b[32m0.01896\u001b[0m\u001b[0m | time: 0.372s\n",
            "| Adam | epoch: 181 | loss: 0.01896 - acc: 0.9928 -- iter: 704/963\n",
            "Training Step: 21869  | total loss: \u001b[1m\u001b[32m0.01710\u001b[0m\u001b[0m | time: 0.377s\n",
            "| Adam | epoch: 181 | loss: 0.01710 - acc: 0.9936 -- iter: 712/963\n",
            "Training Step: 21870  | total loss: \u001b[1m\u001b[32m0.01540\u001b[0m\u001b[0m | time: 0.383s\n",
            "| Adam | epoch: 181 | loss: 0.01540 - acc: 0.9942 -- iter: 720/963\n",
            "Training Step: 21871  | total loss: \u001b[1m\u001b[32m0.01390\u001b[0m\u001b[0m | time: 0.393s\n",
            "| Adam | epoch: 181 | loss: 0.01390 - acc: 0.9948 -- iter: 728/963\n",
            "Training Step: 21872  | total loss: \u001b[1m\u001b[32m0.01255\u001b[0m\u001b[0m | time: 0.415s\n",
            "| Adam | epoch: 181 | loss: 0.01255 - acc: 0.9953 -- iter: 736/963\n",
            "Training Step: 21873  | total loss: \u001b[1m\u001b[32m0.01130\u001b[0m\u001b[0m | time: 0.419s\n",
            "| Adam | epoch: 181 | loss: 0.01130 - acc: 0.9958 -- iter: 744/963\n",
            "Training Step: 21874  | total loss: \u001b[1m\u001b[32m0.01020\u001b[0m\u001b[0m | time: 0.422s\n",
            "| Adam | epoch: 181 | loss: 0.01020 - acc: 0.9962 -- iter: 752/963\n",
            "Training Step: 21875  | total loss: \u001b[1m\u001b[32m0.00949\u001b[0m\u001b[0m | time: 0.434s\n",
            "| Adam | epoch: 181 | loss: 0.00949 - acc: 0.9966 -- iter: 760/963\n",
            "Training Step: 21876  | total loss: \u001b[1m\u001b[32m0.01063\u001b[0m\u001b[0m | time: 0.438s\n",
            "| Adam | epoch: 181 | loss: 0.01063 - acc: 0.9969 -- iter: 768/963\n",
            "Training Step: 21877  | total loss: \u001b[1m\u001b[32m0.00958\u001b[0m\u001b[0m | time: 0.443s\n",
            "| Adam | epoch: 181 | loss: 0.00958 - acc: 0.9972 -- iter: 776/963\n",
            "Training Step: 21878  | total loss: \u001b[1m\u001b[32m0.00863\u001b[0m\u001b[0m | time: 0.446s\n",
            "| Adam | epoch: 181 | loss: 0.00863 - acc: 0.9975 -- iter: 784/963\n",
            "Training Step: 21879  | total loss: \u001b[1m\u001b[32m0.00781\u001b[0m\u001b[0m | time: 0.448s\n",
            "| Adam | epoch: 181 | loss: 0.00781 - acc: 0.9978 -- iter: 792/963\n",
            "Training Step: 21880  | total loss: \u001b[1m\u001b[32m0.00704\u001b[0m\u001b[0m | time: 0.451s\n",
            "| Adam | epoch: 181 | loss: 0.00704 - acc: 0.9980 -- iter: 800/963\n",
            "Training Step: 21881  | total loss: \u001b[1m\u001b[32m0.00634\u001b[0m\u001b[0m | time: 0.453s\n",
            "| Adam | epoch: 181 | loss: 0.00634 - acc: 0.9982 -- iter: 808/963\n",
            "Training Step: 21882  | total loss: \u001b[1m\u001b[32m0.00572\u001b[0m\u001b[0m | time: 0.457s\n",
            "| Adam | epoch: 181 | loss: 0.00572 - acc: 0.9984 -- iter: 816/963\n",
            "Training Step: 21883  | total loss: \u001b[1m\u001b[32m0.01810\u001b[0m\u001b[0m | time: 0.459s\n",
            "| Adam | epoch: 181 | loss: 0.01810 - acc: 0.9860 -- iter: 824/963\n",
            "Training Step: 21884  | total loss: \u001b[1m\u001b[32m0.01632\u001b[0m\u001b[0m | time: 0.463s\n",
            "| Adam | epoch: 181 | loss: 0.01632 - acc: 0.9874 -- iter: 832/963\n",
            "Training Step: 21885  | total loss: \u001b[1m\u001b[32m0.01473\u001b[0m\u001b[0m | time: 0.465s\n",
            "| Adam | epoch: 181 | loss: 0.01473 - acc: 0.9887 -- iter: 840/963\n",
            "Training Step: 21886  | total loss: \u001b[1m\u001b[32m0.01331\u001b[0m\u001b[0m | time: 0.468s\n",
            "| Adam | epoch: 181 | loss: 0.01331 - acc: 0.9898 -- iter: 848/963\n",
            "Training Step: 21887  | total loss: \u001b[1m\u001b[32m0.01200\u001b[0m\u001b[0m | time: 0.470s\n",
            "| Adam | epoch: 181 | loss: 0.01200 - acc: 0.9908 -- iter: 856/963\n",
            "Training Step: 21888  | total loss: \u001b[1m\u001b[32m0.01084\u001b[0m\u001b[0m | time: 0.474s\n",
            "| Adam | epoch: 181 | loss: 0.01084 - acc: 0.9917 -- iter: 864/963\n",
            "Training Step: 21889  | total loss: \u001b[1m\u001b[32m0.01018\u001b[0m\u001b[0m | time: 0.477s\n",
            "| Adam | epoch: 181 | loss: 0.01018 - acc: 0.9926 -- iter: 872/963\n",
            "Training Step: 21890  | total loss: \u001b[1m\u001b[32m0.00917\u001b[0m\u001b[0m | time: 0.481s\n",
            "| Adam | epoch: 181 | loss: 0.00917 - acc: 0.9933 -- iter: 880/963\n",
            "Training Step: 21891  | total loss: \u001b[1m\u001b[32m0.00845\u001b[0m\u001b[0m | time: 0.483s\n",
            "| Adam | epoch: 181 | loss: 0.00845 - acc: 0.9940 -- iter: 888/963\n",
            "Training Step: 21892  | total loss: \u001b[1m\u001b[32m0.01486\u001b[0m\u001b[0m | time: 0.486s\n",
            "| Adam | epoch: 181 | loss: 0.01486 - acc: 0.9946 -- iter: 896/963\n",
            "Training Step: 21893  | total loss: \u001b[1m\u001b[32m0.01339\u001b[0m\u001b[0m | time: 0.489s\n",
            "| Adam | epoch: 181 | loss: 0.01339 - acc: 0.9951 -- iter: 904/963\n",
            "Training Step: 21894  | total loss: \u001b[1m\u001b[32m0.05160\u001b[0m\u001b[0m | time: 0.492s\n",
            "| Adam | epoch: 181 | loss: 0.05160 - acc: 0.9706 -- iter: 912/963\n",
            "Training Step: 21895  | total loss: \u001b[1m\u001b[32m0.04647\u001b[0m\u001b[0m | time: 0.495s\n",
            "| Adam | epoch: 181 | loss: 0.04647 - acc: 0.9736 -- iter: 920/963\n",
            "Training Step: 21896  | total loss: \u001b[1m\u001b[32m0.04186\u001b[0m\u001b[0m | time: 0.499s\n",
            "| Adam | epoch: 181 | loss: 0.04186 - acc: 0.9762 -- iter: 928/963\n",
            "Training Step: 21897  | total loss: \u001b[1m\u001b[32m0.03769\u001b[0m\u001b[0m | time: 0.501s\n",
            "| Adam | epoch: 181 | loss: 0.03769 - acc: 0.9786 -- iter: 936/963\n",
            "Training Step: 21898  | total loss: \u001b[1m\u001b[32m0.03394\u001b[0m\u001b[0m | time: 0.504s\n",
            "| Adam | epoch: 181 | loss: 0.03394 - acc: 0.9807 -- iter: 944/963\n",
            "Training Step: 21899  | total loss: \u001b[1m\u001b[32m0.03056\u001b[0m\u001b[0m | time: 0.506s\n",
            "| Adam | epoch: 181 | loss: 0.03056 - acc: 0.9826 -- iter: 952/963\n",
            "Training Step: 21900  | total loss: \u001b[1m\u001b[32m0.02756\u001b[0m\u001b[0m | time: 0.508s\n",
            "| Adam | epoch: 181 | loss: 0.02756 - acc: 0.9844 -- iter: 960/963\n",
            "Training Step: 21901  | total loss: \u001b[1m\u001b[32m0.02482\u001b[0m\u001b[0m | time: 0.511s\n",
            "| Adam | epoch: 181 | loss: 0.02482 - acc: 0.9859 -- iter: 963/963\n",
            "--\n",
            "Training Step: 21902  | total loss: \u001b[1m\u001b[32m0.02239\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 182 | loss: 0.02239 - acc: 0.9874 -- iter: 008/963\n",
            "Training Step: 21903  | total loss: \u001b[1m\u001b[32m0.02018\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 182 | loss: 0.02018 - acc: 0.9886 -- iter: 016/963\n",
            "Training Step: 21904  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 182 | loss: 0.01819 - acc: 0.9898 -- iter: 024/963\n",
            "Training Step: 21905  | total loss: \u001b[1m\u001b[32m0.01640\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 182 | loss: 0.01640 - acc: 0.9908 -- iter: 032/963\n",
            "Training Step: 21906  | total loss: \u001b[1m\u001b[32m0.01479\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 182 | loss: 0.01479 - acc: 0.9917 -- iter: 040/963\n",
            "Training Step: 21907  | total loss: \u001b[1m\u001b[32m0.01332\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 182 | loss: 0.01332 - acc: 0.9925 -- iter: 048/963\n",
            "Training Step: 21908  | total loss: \u001b[1m\u001b[32m0.02346\u001b[0m\u001b[0m | time: 0.026s\n",
            "| Adam | epoch: 182 | loss: 0.02346 - acc: 0.9808 -- iter: 056/963\n",
            "Training Step: 21909  | total loss: \u001b[1m\u001b[32m0.02113\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 182 | loss: 0.02113 - acc: 0.9827 -- iter: 064/963\n",
            "Training Step: 21910  | total loss: \u001b[1m\u001b[32m0.01902\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 182 | loss: 0.01902 - acc: 0.9844 -- iter: 072/963\n",
            "Training Step: 21911  | total loss: \u001b[1m\u001b[32m0.01714\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 182 | loss: 0.01714 - acc: 0.9860 -- iter: 080/963\n",
            "Training Step: 21912  | total loss: \u001b[1m\u001b[32m0.01546\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 182 | loss: 0.01546 - acc: 0.9874 -- iter: 088/963\n",
            "Training Step: 21913  | total loss: \u001b[1m\u001b[32m0.01393\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 182 | loss: 0.01393 - acc: 0.9886 -- iter: 096/963\n",
            "Training Step: 21914  | total loss: \u001b[1m\u001b[32m0.01257\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 182 | loss: 0.01257 - acc: 0.9898 -- iter: 104/963\n",
            "Training Step: 21915  | total loss: \u001b[1m\u001b[32m0.01135\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 182 | loss: 0.01135 - acc: 0.9908 -- iter: 112/963\n",
            "Training Step: 21916  | total loss: \u001b[1m\u001b[32m0.01023\u001b[0m\u001b[0m | time: 0.140s\n",
            "| Adam | epoch: 182 | loss: 0.01023 - acc: 0.9917 -- iter: 120/963\n",
            "Training Step: 21917  | total loss: \u001b[1m\u001b[32m0.00923\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 182 | loss: 0.00923 - acc: 0.9926 -- iter: 128/963\n",
            "Training Step: 21918  | total loss: \u001b[1m\u001b[32m0.03982\u001b[0m\u001b[0m | time: 0.195s\n",
            "| Adam | epoch: 182 | loss: 0.03982 - acc: 0.9808 -- iter: 136/963\n",
            "Training Step: 21919  | total loss: \u001b[1m\u001b[32m0.03588\u001b[0m\u001b[0m | time: 0.201s\n",
            "| Adam | epoch: 182 | loss: 0.03588 - acc: 0.9827 -- iter: 144/963\n",
            "Training Step: 21920  | total loss: \u001b[1m\u001b[32m0.03233\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 182 | loss: 0.03233 - acc: 0.9844 -- iter: 152/963\n",
            "Training Step: 21921  | total loss: \u001b[1m\u001b[32m0.02914\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 182 | loss: 0.02914 - acc: 0.9860 -- iter: 160/963\n",
            "Training Step: 21922  | total loss: \u001b[1m\u001b[32m0.02625\u001b[0m\u001b[0m | time: 0.218s\n",
            "| Adam | epoch: 182 | loss: 0.02625 - acc: 0.9874 -- iter: 168/963\n",
            "Training Step: 21923  | total loss: \u001b[1m\u001b[32m0.02364\u001b[0m\u001b[0m | time: 0.221s\n",
            "| Adam | epoch: 182 | loss: 0.02364 - acc: 0.9887 -- iter: 176/963\n",
            "Training Step: 21924  | total loss: \u001b[1m\u001b[32m0.02128\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 182 | loss: 0.02128 - acc: 0.9898 -- iter: 184/963\n",
            "Training Step: 21925  | total loss: \u001b[1m\u001b[32m0.01918\u001b[0m\u001b[0m | time: 0.261s\n",
            "| Adam | epoch: 182 | loss: 0.01918 - acc: 0.9908 -- iter: 192/963\n",
            "Training Step: 21926  | total loss: \u001b[1m\u001b[32m0.01730\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 182 | loss: 0.01730 - acc: 0.9917 -- iter: 200/963\n",
            "Training Step: 21927  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.271s\n",
            "| Adam | epoch: 182 | loss: 0.01559 - acc: 0.9926 -- iter: 208/963\n",
            "Training Step: 21928  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.276s\n",
            "| Adam | epoch: 182 | loss: 0.01836 - acc: 0.9933 -- iter: 216/963\n",
            "Training Step: 21929  | total loss: \u001b[1m\u001b[32m0.01654\u001b[0m\u001b[0m | time: 0.280s\n",
            "| Adam | epoch: 182 | loss: 0.01654 - acc: 0.9940 -- iter: 224/963\n",
            "Training Step: 21930  | total loss: \u001b[1m\u001b[32m0.01489\u001b[0m\u001b[0m | time: 0.283s\n",
            "| Adam | epoch: 182 | loss: 0.01489 - acc: 0.9946 -- iter: 232/963\n",
            "Training Step: 21931  | total loss: \u001b[1m\u001b[32m0.01342\u001b[0m\u001b[0m | time: 0.286s\n",
            "| Adam | epoch: 182 | loss: 0.01342 - acc: 0.9951 -- iter: 240/963\n",
            "Training Step: 21932  | total loss: \u001b[1m\u001b[32m0.01212\u001b[0m\u001b[0m | time: 0.290s\n",
            "| Adam | epoch: 182 | loss: 0.01212 - acc: 0.9956 -- iter: 248/963\n",
            "Training Step: 21933  | total loss: \u001b[1m\u001b[32m0.01093\u001b[0m\u001b[0m | time: 0.293s\n",
            "| Adam | epoch: 182 | loss: 0.01093 - acc: 0.9960 -- iter: 256/963\n",
            "Training Step: 21934  | total loss: \u001b[1m\u001b[32m0.00986\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 182 | loss: 0.00986 - acc: 0.9964 -- iter: 264/963\n",
            "Training Step: 21935  | total loss: \u001b[1m\u001b[32m0.00891\u001b[0m\u001b[0m | time: 0.299s\n",
            "| Adam | epoch: 182 | loss: 0.00891 - acc: 0.9968 -- iter: 272/963\n",
            "Training Step: 21936  | total loss: \u001b[1m\u001b[32m0.00804\u001b[0m\u001b[0m | time: 0.302s\n",
            "| Adam | epoch: 182 | loss: 0.00804 - acc: 0.9971 -- iter: 280/963\n",
            "Training Step: 21937  | total loss: \u001b[1m\u001b[32m0.00725\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 182 | loss: 0.00725 - acc: 0.9974 -- iter: 288/963\n",
            "Training Step: 21938  | total loss: \u001b[1m\u001b[32m0.00657\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 182 | loss: 0.00657 - acc: 0.9977 -- iter: 296/963\n",
            "Training Step: 21939  | total loss: \u001b[1m\u001b[32m0.01470\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 182 | loss: 0.01470 - acc: 0.9854 -- iter: 304/963\n",
            "Training Step: 21940  | total loss: \u001b[1m\u001b[32m0.01327\u001b[0m\u001b[0m | time: 0.316s\n",
            "| Adam | epoch: 182 | loss: 0.01327 - acc: 0.9869 -- iter: 312/963\n",
            "Training Step: 21941  | total loss: \u001b[1m\u001b[32m0.01195\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 182 | loss: 0.01195 - acc: 0.9882 -- iter: 320/963\n",
            "Training Step: 21942  | total loss: \u001b[1m\u001b[32m0.01077\u001b[0m\u001b[0m | time: 0.323s\n",
            "| Adam | epoch: 182 | loss: 0.01077 - acc: 0.9894 -- iter: 328/963\n",
            "Training Step: 21943  | total loss: \u001b[1m\u001b[32m0.00971\u001b[0m\u001b[0m | time: 0.327s\n",
            "| Adam | epoch: 182 | loss: 0.00971 - acc: 0.9904 -- iter: 336/963\n",
            "Training Step: 21944  | total loss: \u001b[1m\u001b[32m0.00885\u001b[0m\u001b[0m | time: 0.331s\n",
            "| Adam | epoch: 182 | loss: 0.00885 - acc: 0.9914 -- iter: 344/963\n",
            "Training Step: 21945  | total loss: \u001b[1m\u001b[32m0.00797\u001b[0m\u001b[0m | time: 0.336s\n",
            "| Adam | epoch: 182 | loss: 0.00797 - acc: 0.9922 -- iter: 352/963\n",
            "Training Step: 21946  | total loss: \u001b[1m\u001b[32m0.02451\u001b[0m\u001b[0m | time: 0.340s\n",
            "| Adam | epoch: 182 | loss: 0.02451 - acc: 0.9805 -- iter: 360/963\n",
            "Training Step: 21947  | total loss: \u001b[1m\u001b[32m0.02210\u001b[0m\u001b[0m | time: 0.343s\n",
            "| Adam | epoch: 182 | loss: 0.02210 - acc: 0.9825 -- iter: 368/963\n",
            "Training Step: 21948  | total loss: \u001b[1m\u001b[32m0.01991\u001b[0m\u001b[0m | time: 0.346s\n",
            "| Adam | epoch: 182 | loss: 0.01991 - acc: 0.9842 -- iter: 376/963\n",
            "Training Step: 21949  | total loss: \u001b[1m\u001b[32m0.01793\u001b[0m\u001b[0m | time: 0.349s\n",
            "| Adam | epoch: 182 | loss: 0.01793 - acc: 0.9858 -- iter: 384/963\n",
            "Training Step: 21950  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.352s\n",
            "| Adam | epoch: 182 | loss: 0.01731 - acc: 0.9872 -- iter: 392/963\n",
            "Training Step: 21951  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.355s\n",
            "| Adam | epoch: 182 | loss: 0.01559 - acc: 0.9885 -- iter: 400/963\n",
            "Training Step: 21952  | total loss: \u001b[1m\u001b[32m0.01411\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 182 | loss: 0.01411 - acc: 0.9896 -- iter: 408/963\n",
            "Training Step: 21953  | total loss: \u001b[1m\u001b[32m0.01271\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 182 | loss: 0.01271 - acc: 0.9907 -- iter: 416/963\n",
            "Training Step: 21954  | total loss: \u001b[1m\u001b[32m0.01146\u001b[0m\u001b[0m | time: 0.366s\n",
            "| Adam | epoch: 182 | loss: 0.01146 - acc: 0.9916 -- iter: 424/963\n",
            "Training Step: 21955  | total loss: \u001b[1m\u001b[32m0.01032\u001b[0m\u001b[0m | time: 0.369s\n",
            "| Adam | epoch: 182 | loss: 0.01032 - acc: 0.9925 -- iter: 432/963\n",
            "Training Step: 21956  | total loss: \u001b[1m\u001b[32m0.00932\u001b[0m\u001b[0m | time: 0.372s\n",
            "| Adam | epoch: 182 | loss: 0.00932 - acc: 0.9932 -- iter: 440/963\n",
            "Training Step: 21957  | total loss: \u001b[1m\u001b[32m0.00842\u001b[0m\u001b[0m | time: 0.376s\n",
            "| Adam | epoch: 182 | loss: 0.00842 - acc: 0.9939 -- iter: 448/963\n",
            "Training Step: 21958  | total loss: \u001b[1m\u001b[32m0.00798\u001b[0m\u001b[0m | time: 0.378s\n",
            "| Adam | epoch: 182 | loss: 0.00798 - acc: 0.9945 -- iter: 456/963\n",
            "Training Step: 21959  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.381s\n",
            "| Adam | epoch: 182 | loss: 0.01836 - acc: 0.9825 -- iter: 464/963\n",
            "Training Step: 21960  | total loss: \u001b[1m\u001b[32m0.01654\u001b[0m\u001b[0m | time: 0.384s\n",
            "| Adam | epoch: 182 | loss: 0.01654 - acc: 0.9843 -- iter: 472/963\n",
            "Training Step: 21961  | total loss: \u001b[1m\u001b[32m0.01490\u001b[0m\u001b[0m | time: 0.387s\n",
            "| Adam | epoch: 182 | loss: 0.01490 - acc: 0.9859 -- iter: 480/963\n",
            "Training Step: 21962  | total loss: \u001b[1m\u001b[32m0.01344\u001b[0m\u001b[0m | time: 0.391s\n",
            "| Adam | epoch: 182 | loss: 0.01344 - acc: 0.9873 -- iter: 488/963\n",
            "Training Step: 21963  | total loss: \u001b[1m\u001b[32m0.01211\u001b[0m\u001b[0m | time: 0.394s\n",
            "| Adam | epoch: 182 | loss: 0.01211 - acc: 0.9885 -- iter: 496/963\n",
            "Training Step: 21964  | total loss: \u001b[1m\u001b[32m0.01091\u001b[0m\u001b[0m | time: 0.399s\n",
            "| Adam | epoch: 182 | loss: 0.01091 - acc: 0.9897 -- iter: 504/963\n",
            "Training Step: 21965  | total loss: \u001b[1m\u001b[32m0.00984\u001b[0m\u001b[0m | time: 0.402s\n",
            "| Adam | epoch: 182 | loss: 0.00984 - acc: 0.9907 -- iter: 512/963\n",
            "Training Step: 21966  | total loss: \u001b[1m\u001b[32m0.00893\u001b[0m\u001b[0m | time: 0.404s\n",
            "| Adam | epoch: 182 | loss: 0.00893 - acc: 0.9917 -- iter: 520/963\n",
            "Training Step: 21967  | total loss: \u001b[1m\u001b[32m0.00806\u001b[0m\u001b[0m | time: 0.408s\n",
            "| Adam | epoch: 182 | loss: 0.00806 - acc: 0.9925 -- iter: 528/963\n",
            "Training Step: 21968  | total loss: \u001b[1m\u001b[32m0.00729\u001b[0m\u001b[0m | time: 0.411s\n",
            "| Adam | epoch: 182 | loss: 0.00729 - acc: 0.9932 -- iter: 536/963\n",
            "Training Step: 21969  | total loss: \u001b[1m\u001b[32m0.00661\u001b[0m\u001b[0m | time: 0.414s\n",
            "| Adam | epoch: 182 | loss: 0.00661 - acc: 0.9939 -- iter: 544/963\n",
            "Training Step: 21970  | total loss: \u001b[1m\u001b[32m0.00599\u001b[0m\u001b[0m | time: 0.417s\n",
            "| Adam | epoch: 182 | loss: 0.00599 - acc: 0.9945 -- iter: 552/963\n",
            "Training Step: 21971  | total loss: \u001b[1m\u001b[32m0.06092\u001b[0m\u001b[0m | time: 0.421s\n",
            "| Adam | epoch: 182 | loss: 0.06092 - acc: 0.9701 -- iter: 560/963\n",
            "Training Step: 21972  | total loss: \u001b[1m\u001b[32m0.05485\u001b[0m\u001b[0m | time: 0.424s\n",
            "| Adam | epoch: 182 | loss: 0.05485 - acc: 0.9731 -- iter: 568/963\n",
            "Training Step: 21973  | total loss: \u001b[1m\u001b[32m0.04939\u001b[0m\u001b[0m | time: 0.428s\n",
            "| Adam | epoch: 182 | loss: 0.04939 - acc: 0.9758 -- iter: 576/963\n",
            "Training Step: 21974  | total loss: \u001b[1m\u001b[32m0.04446\u001b[0m\u001b[0m | time: 0.431s\n",
            "| Adam | epoch: 182 | loss: 0.04446 - acc: 0.9782 -- iter: 584/963\n",
            "Training Step: 21975  | total loss: \u001b[1m\u001b[32m0.04004\u001b[0m\u001b[0m | time: 0.433s\n",
            "| Adam | epoch: 182 | loss: 0.04004 - acc: 0.9804 -- iter: 592/963\n",
            "Training Step: 21976  | total loss: \u001b[1m\u001b[32m0.08042\u001b[0m\u001b[0m | time: 0.437s\n",
            "| Adam | epoch: 182 | loss: 0.08042 - acc: 0.9573 -- iter: 600/963\n",
            "Training Step: 21977  | total loss: \u001b[1m\u001b[32m0.07240\u001b[0m\u001b[0m | time: 0.444s\n",
            "| Adam | epoch: 182 | loss: 0.07240 - acc: 0.9616 -- iter: 608/963\n",
            "Training Step: 21978  | total loss: \u001b[1m\u001b[32m0.06743\u001b[0m\u001b[0m | time: 0.448s\n",
            "| Adam | epoch: 182 | loss: 0.06743 - acc: 0.9654 -- iter: 616/963\n",
            "Training Step: 21979  | total loss: \u001b[1m\u001b[32m0.06078\u001b[0m\u001b[0m | time: 0.452s\n",
            "| Adam | epoch: 182 | loss: 0.06078 - acc: 0.9689 -- iter: 624/963\n",
            "Training Step: 21980  | total loss: \u001b[1m\u001b[32m0.05474\u001b[0m\u001b[0m | time: 0.456s\n",
            "| Adam | epoch: 182 | loss: 0.05474 - acc: 0.9720 -- iter: 632/963\n",
            "Training Step: 21981  | total loss: \u001b[1m\u001b[32m0.04928\u001b[0m\u001b[0m | time: 0.460s\n",
            "| Adam | epoch: 182 | loss: 0.04928 - acc: 0.9748 -- iter: 640/963\n",
            "Training Step: 21982  | total loss: \u001b[1m\u001b[32m0.04438\u001b[0m\u001b[0m | time: 0.464s\n",
            "| Adam | epoch: 182 | loss: 0.04438 - acc: 0.9773 -- iter: 648/963\n",
            "Training Step: 21983  | total loss: \u001b[1m\u001b[32m0.03996\u001b[0m\u001b[0m | time: 0.469s\n",
            "| Adam | epoch: 182 | loss: 0.03996 - acc: 0.9796 -- iter: 656/963\n",
            "Training Step: 21984  | total loss: \u001b[1m\u001b[32m0.03599\u001b[0m\u001b[0m | time: 0.472s\n",
            "| Adam | epoch: 182 | loss: 0.03599 - acc: 0.9816 -- iter: 664/963\n",
            "Training Step: 21985  | total loss: \u001b[1m\u001b[32m0.03240\u001b[0m\u001b[0m | time: 0.476s\n",
            "| Adam | epoch: 182 | loss: 0.03240 - acc: 0.9835 -- iter: 672/963\n",
            "Training Step: 21986  | total loss: \u001b[1m\u001b[32m0.04823\u001b[0m\u001b[0m | time: 0.481s\n",
            "| Adam | epoch: 182 | loss: 0.04823 - acc: 0.9726 -- iter: 680/963\n",
            "Training Step: 21987  | total loss: \u001b[1m\u001b[32m0.04342\u001b[0m\u001b[0m | time: 0.484s\n",
            "| Adam | epoch: 182 | loss: 0.04342 - acc: 0.9754 -- iter: 688/963\n",
            "Training Step: 21988  | total loss: \u001b[1m\u001b[32m0.03908\u001b[0m\u001b[0m | time: 0.489s\n",
            "| Adam | epoch: 182 | loss: 0.03908 - acc: 0.9778 -- iter: 696/963\n",
            "Training Step: 21989  | total loss: \u001b[1m\u001b[32m0.03521\u001b[0m\u001b[0m | time: 0.492s\n",
            "| Adam | epoch: 182 | loss: 0.03521 - acc: 0.9800 -- iter: 704/963\n",
            "Training Step: 21990  | total loss: \u001b[1m\u001b[32m0.03170\u001b[0m\u001b[0m | time: 0.496s\n",
            "| Adam | epoch: 182 | loss: 0.03170 - acc: 0.9820 -- iter: 712/963\n",
            "Training Step: 21991  | total loss: \u001b[1m\u001b[32m0.02855\u001b[0m\u001b[0m | time: 0.500s\n",
            "| Adam | epoch: 182 | loss: 0.02855 - acc: 0.9838 -- iter: 720/963\n",
            "Training Step: 21992  | total loss: \u001b[1m\u001b[32m0.02571\u001b[0m\u001b[0m | time: 0.503s\n",
            "| Adam | epoch: 182 | loss: 0.02571 - acc: 0.9854 -- iter: 728/963\n",
            "Training Step: 21993  | total loss: \u001b[1m\u001b[32m0.02315\u001b[0m\u001b[0m | time: 0.507s\n",
            "| Adam | epoch: 182 | loss: 0.02315 - acc: 0.9869 -- iter: 736/963\n",
            "Training Step: 21994  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.512s\n",
            "| Adam | epoch: 182 | loss: 0.02086 - acc: 0.9882 -- iter: 744/963\n",
            "Training Step: 21995  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.515s\n",
            "| Adam | epoch: 182 | loss: 0.01881 - acc: 0.9894 -- iter: 752/963\n",
            "Training Step: 21996  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.519s\n",
            "| Adam | epoch: 182 | loss: 0.01694 - acc: 0.9905 -- iter: 760/963\n",
            "Training Step: 21997  | total loss: \u001b[1m\u001b[32m0.02680\u001b[0m\u001b[0m | time: 0.524s\n",
            "| Adam | epoch: 182 | loss: 0.02680 - acc: 0.9789 -- iter: 768/963\n",
            "Training Step: 21998  | total loss: \u001b[1m\u001b[32m0.02418\u001b[0m\u001b[0m | time: 0.528s\n",
            "| Adam | epoch: 182 | loss: 0.02418 - acc: 0.9810 -- iter: 776/963\n",
            "Training Step: 21999  | total loss: \u001b[1m\u001b[32m0.02178\u001b[0m\u001b[0m | time: 0.533s\n",
            "| Adam | epoch: 182 | loss: 0.02178 - acc: 0.9829 -- iter: 784/963\n",
            "Training Step: 22000  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.539s\n",
            "| Adam | epoch: 182 | loss: 0.01964 - acc: 0.9846 -- iter: 792/963\n",
            "Training Step: 22001  | total loss: \u001b[1m\u001b[32m0.01768\u001b[0m\u001b[0m | time: 0.545s\n",
            "| Adam | epoch: 182 | loss: 0.01768 - acc: 0.9862 -- iter: 800/963\n",
            "Training Step: 22002  | total loss: \u001b[1m\u001b[32m0.04402\u001b[0m\u001b[0m | time: 0.554s\n",
            "| Adam | epoch: 182 | loss: 0.04402 - acc: 0.9750 -- iter: 808/963\n",
            "Training Step: 22003  | total loss: \u001b[1m\u001b[32m0.03978\u001b[0m\u001b[0m | time: 0.559s\n",
            "| Adam | epoch: 182 | loss: 0.03978 - acc: 0.9775 -- iter: 816/963\n",
            "Training Step: 22004  | total loss: \u001b[1m\u001b[32m0.03583\u001b[0m\u001b[0m | time: 0.562s\n",
            "| Adam | epoch: 182 | loss: 0.03583 - acc: 0.9798 -- iter: 824/963\n",
            "Training Step: 22005  | total loss: \u001b[1m\u001b[32m0.03227\u001b[0m\u001b[0m | time: 0.567s\n",
            "| Adam | epoch: 182 | loss: 0.03227 - acc: 0.9818 -- iter: 832/963\n",
            "Training Step: 22006  | total loss: \u001b[1m\u001b[32m0.02906\u001b[0m\u001b[0m | time: 0.571s\n",
            "| Adam | epoch: 182 | loss: 0.02906 - acc: 0.9836 -- iter: 840/963\n",
            "Training Step: 22007  | total loss: \u001b[1m\u001b[32m0.02618\u001b[0m\u001b[0m | time: 0.574s\n",
            "| Adam | epoch: 182 | loss: 0.02618 - acc: 0.9853 -- iter: 848/963\n",
            "Training Step: 22008  | total loss: \u001b[1m\u001b[32m0.02358\u001b[0m\u001b[0m | time: 0.578s\n",
            "| Adam | epoch: 182 | loss: 0.02358 - acc: 0.9867 -- iter: 856/963\n",
            "Training Step: 22009  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.582s\n",
            "| Adam | epoch: 182 | loss: 0.02125 - acc: 0.9881 -- iter: 864/963\n",
            "Training Step: 22010  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.586s\n",
            "| Adam | epoch: 182 | loss: 0.01923 - acc: 0.9893 -- iter: 872/963\n",
            "Training Step: 22011  | total loss: \u001b[1m\u001b[32m0.01733\u001b[0m\u001b[0m | time: 0.590s\n",
            "| Adam | epoch: 182 | loss: 0.01733 - acc: 0.9903 -- iter: 880/963\n",
            "Training Step: 22012  | total loss: \u001b[1m\u001b[32m0.01561\u001b[0m\u001b[0m | time: 0.594s\n",
            "| Adam | epoch: 182 | loss: 0.01561 - acc: 0.9913 -- iter: 888/963\n",
            "Training Step: 22013  | total loss: \u001b[1m\u001b[32m0.01407\u001b[0m\u001b[0m | time: 0.599s\n",
            "| Adam | epoch: 182 | loss: 0.01407 - acc: 0.9922 -- iter: 896/963\n",
            "Training Step: 22014  | total loss: \u001b[1m\u001b[32m0.01268\u001b[0m\u001b[0m | time: 0.604s\n",
            "| Adam | epoch: 182 | loss: 0.01268 - acc: 0.9930 -- iter: 904/963\n",
            "Training Step: 22015  | total loss: \u001b[1m\u001b[32m0.01145\u001b[0m\u001b[0m | time: 0.608s\n",
            "| Adam | epoch: 182 | loss: 0.01145 - acc: 0.9937 -- iter: 912/963\n",
            "Training Step: 22016  | total loss: \u001b[1m\u001b[32m0.01031\u001b[0m\u001b[0m | time: 0.613s\n",
            "| Adam | epoch: 182 | loss: 0.01031 - acc: 0.9943 -- iter: 920/963\n",
            "Training Step: 22017  | total loss: \u001b[1m\u001b[32m0.00929\u001b[0m\u001b[0m | time: 0.616s\n",
            "| Adam | epoch: 182 | loss: 0.00929 - acc: 0.9949 -- iter: 928/963\n",
            "Training Step: 22018  | total loss: \u001b[1m\u001b[32m0.00841\u001b[0m\u001b[0m | time: 0.622s\n",
            "| Adam | epoch: 182 | loss: 0.00841 - acc: 0.9954 -- iter: 936/963\n",
            "Training Step: 22019  | total loss: \u001b[1m\u001b[32m0.00759\u001b[0m\u001b[0m | time: 0.627s\n",
            "| Adam | epoch: 182 | loss: 0.00759 - acc: 0.9958 -- iter: 944/963\n",
            "Training Step: 22020  | total loss: \u001b[1m\u001b[32m0.00685\u001b[0m\u001b[0m | time: 0.631s\n",
            "| Adam | epoch: 182 | loss: 0.00685 - acc: 0.9963 -- iter: 952/963\n",
            "Training Step: 22021  | total loss: \u001b[1m\u001b[32m0.00619\u001b[0m\u001b[0m | time: 0.637s\n",
            "| Adam | epoch: 182 | loss: 0.00619 - acc: 0.9966 -- iter: 960/963\n",
            "Training Step: 22022  | total loss: \u001b[1m\u001b[32m0.03199\u001b[0m\u001b[0m | time: 0.640s\n",
            "| Adam | epoch: 182 | loss: 0.03199 - acc: 0.9845 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22023  | total loss: \u001b[1m\u001b[32m0.02880\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 183 | loss: 0.02880 - acc: 0.9860 -- iter: 008/963\n",
            "Training Step: 22024  | total loss: \u001b[1m\u001b[32m0.02594\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 183 | loss: 0.02594 - acc: 0.9874 -- iter: 016/963\n",
            "Training Step: 22025  | total loss: \u001b[1m\u001b[32m0.02338\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 183 | loss: 0.02338 - acc: 0.9887 -- iter: 024/963\n",
            "Training Step: 22026  | total loss: \u001b[1m\u001b[32m0.02107\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 183 | loss: 0.02107 - acc: 0.9898 -- iter: 032/963\n",
            "Training Step: 22027  | total loss: \u001b[1m\u001b[32m0.01899\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 183 | loss: 0.01899 - acc: 0.9908 -- iter: 040/963\n",
            "Training Step: 22028  | total loss: \u001b[1m\u001b[32m0.02651\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 183 | loss: 0.02651 - acc: 0.9792 -- iter: 048/963\n",
            "Training Step: 22029  | total loss: \u001b[1m\u001b[32m0.02389\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 183 | loss: 0.02389 - acc: 0.9813 -- iter: 056/963\n",
            "Training Step: 22030  | total loss: \u001b[1m\u001b[32m0.02153\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 183 | loss: 0.02153 - acc: 0.9832 -- iter: 064/963\n",
            "Training Step: 22031  | total loss: \u001b[1m\u001b[32m0.01941\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 183 | loss: 0.01941 - acc: 0.9849 -- iter: 072/963\n",
            "Training Step: 22032  | total loss: \u001b[1m\u001b[32m0.01748\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 183 | loss: 0.01748 - acc: 0.9864 -- iter: 080/963\n",
            "Training Step: 22033  | total loss: \u001b[1m\u001b[32m0.01578\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 183 | loss: 0.01578 - acc: 0.9877 -- iter: 088/963\n",
            "Training Step: 22034  | total loss: \u001b[1m\u001b[32m0.01431\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 183 | loss: 0.01431 - acc: 0.9890 -- iter: 096/963\n",
            "Training Step: 22035  | total loss: \u001b[1m\u001b[32m0.01288\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 183 | loss: 0.01288 - acc: 0.9901 -- iter: 104/963\n",
            "Training Step: 22036  | total loss: \u001b[1m\u001b[32m0.01163\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 183 | loss: 0.01163 - acc: 0.9911 -- iter: 112/963\n",
            "Training Step: 22037  | total loss: \u001b[1m\u001b[32m0.01055\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 183 | loss: 0.01055 - acc: 0.9920 -- iter: 120/963\n",
            "Training Step: 22038  | total loss: \u001b[1m\u001b[32m0.00952\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 183 | loss: 0.00952 - acc: 0.9928 -- iter: 128/963\n",
            "Training Step: 22039  | total loss: \u001b[1m\u001b[32m0.01340\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 183 | loss: 0.01340 - acc: 0.9935 -- iter: 136/963\n",
            "Training Step: 22040  | total loss: \u001b[1m\u001b[32m0.01219\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 183 | loss: 0.01219 - acc: 0.9941 -- iter: 144/963\n",
            "Training Step: 22041  | total loss: \u001b[1m\u001b[32m0.01103\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 183 | loss: 0.01103 - acc: 0.9947 -- iter: 152/963\n",
            "Training Step: 22042  | total loss: \u001b[1m\u001b[32m0.00994\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 183 | loss: 0.00994 - acc: 0.9953 -- iter: 160/963\n",
            "Training Step: 22043  | total loss: \u001b[1m\u001b[32m0.00895\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 183 | loss: 0.00895 - acc: 0.9957 -- iter: 168/963\n",
            "Training Step: 22044  | total loss: \u001b[1m\u001b[32m0.00807\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 183 | loss: 0.00807 - acc: 0.9962 -- iter: 176/963\n",
            "Training Step: 22045  | total loss: \u001b[1m\u001b[32m0.00730\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 183 | loss: 0.00730 - acc: 0.9965 -- iter: 184/963\n",
            "Training Step: 22046  | total loss: \u001b[1m\u001b[32m0.00659\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 183 | loss: 0.00659 - acc: 0.9969 -- iter: 192/963\n",
            "Training Step: 22047  | total loss: \u001b[1m\u001b[32m0.00600\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 183 | loss: 0.00600 - acc: 0.9972 -- iter: 200/963\n",
            "Training Step: 22048  | total loss: \u001b[1m\u001b[32m0.00788\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 183 | loss: 0.00788 - acc: 0.9975 -- iter: 208/963\n",
            "Training Step: 22049  | total loss: \u001b[1m\u001b[32m0.00710\u001b[0m\u001b[0m | time: 0.134s\n",
            "| Adam | epoch: 183 | loss: 0.00710 - acc: 0.9977 -- iter: 216/963\n",
            "Training Step: 22050  | total loss: \u001b[1m\u001b[32m0.00646\u001b[0m\u001b[0m | time: 0.140s\n",
            "| Adam | epoch: 183 | loss: 0.00646 - acc: 0.9980 -- iter: 224/963\n",
            "Training Step: 22051  | total loss: \u001b[1m\u001b[32m0.00583\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 183 | loss: 0.00583 - acc: 0.9982 -- iter: 232/963\n",
            "Training Step: 22052  | total loss: \u001b[1m\u001b[32m0.00527\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 183 | loss: 0.00527 - acc: 0.9983 -- iter: 240/963\n",
            "Training Step: 22053  | total loss: \u001b[1m\u001b[32m0.00475\u001b[0m\u001b[0m | time: 0.151s\n",
            "| Adam | epoch: 183 | loss: 0.00475 - acc: 0.9985 -- iter: 248/963\n",
            "Training Step: 22054  | total loss: \u001b[1m\u001b[32m0.00429\u001b[0m\u001b[0m | time: 0.157s\n",
            "| Adam | epoch: 183 | loss: 0.00429 - acc: 0.9987 -- iter: 256/963\n",
            "Training Step: 22055  | total loss: \u001b[1m\u001b[32m0.00388\u001b[0m\u001b[0m | time: 0.161s\n",
            "| Adam | epoch: 183 | loss: 0.00388 - acc: 0.9988 -- iter: 264/963\n",
            "Training Step: 22056  | total loss: \u001b[1m\u001b[32m0.00351\u001b[0m\u001b[0m | time: 0.167s\n",
            "| Adam | epoch: 183 | loss: 0.00351 - acc: 0.9989 -- iter: 272/963\n",
            "Training Step: 22057  | total loss: \u001b[1m\u001b[32m0.00318\u001b[0m\u001b[0m | time: 0.175s\n",
            "| Adam | epoch: 183 | loss: 0.00318 - acc: 0.9990 -- iter: 280/963\n",
            "Training Step: 22058  | total loss: \u001b[1m\u001b[32m0.00288\u001b[0m\u001b[0m | time: 0.179s\n",
            "| Adam | epoch: 183 | loss: 0.00288 - acc: 0.9991 -- iter: 288/963\n",
            "Training Step: 22059  | total loss: \u001b[1m\u001b[32m0.03099\u001b[0m\u001b[0m | time: 0.184s\n",
            "| Adam | epoch: 183 | loss: 0.03099 - acc: 0.9867 -- iter: 296/963\n",
            "Training Step: 22060  | total loss: \u001b[1m\u001b[32m0.02791\u001b[0m\u001b[0m | time: 0.188s\n",
            "| Adam | epoch: 183 | loss: 0.02791 - acc: 0.9880 -- iter: 304/963\n",
            "Training Step: 22061  | total loss: \u001b[1m\u001b[32m0.02513\u001b[0m\u001b[0m | time: 0.193s\n",
            "| Adam | epoch: 183 | loss: 0.02513 - acc: 0.9892 -- iter: 312/963\n",
            "Training Step: 22062  | total loss: \u001b[1m\u001b[32m0.02263\u001b[0m\u001b[0m | time: 0.196s\n",
            "| Adam | epoch: 183 | loss: 0.02263 - acc: 0.9903 -- iter: 320/963\n",
            "Training Step: 22063  | total loss: \u001b[1m\u001b[32m0.02039\u001b[0m\u001b[0m | time: 0.199s\n",
            "| Adam | epoch: 183 | loss: 0.02039 - acc: 0.9913 -- iter: 328/963\n",
            "Training Step: 22064  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.203s\n",
            "| Adam | epoch: 183 | loss: 0.01836 - acc: 0.9922 -- iter: 336/963\n",
            "Training Step: 22065  | total loss: \u001b[1m\u001b[32m0.01656\u001b[0m\u001b[0m | time: 0.207s\n",
            "| Adam | epoch: 183 | loss: 0.01656 - acc: 0.9929 -- iter: 344/963\n",
            "Training Step: 22066  | total loss: \u001b[1m\u001b[32m0.01491\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 183 | loss: 0.01491 - acc: 0.9936 -- iter: 352/963\n",
            "Training Step: 22067  | total loss: \u001b[1m\u001b[32m0.01345\u001b[0m\u001b[0m | time: 0.214s\n",
            "| Adam | epoch: 183 | loss: 0.01345 - acc: 0.9943 -- iter: 360/963\n",
            "Training Step: 22068  | total loss: \u001b[1m\u001b[32m0.01214\u001b[0m\u001b[0m | time: 0.217s\n",
            "| Adam | epoch: 183 | loss: 0.01214 - acc: 0.9949 -- iter: 368/963\n",
            "Training Step: 22069  | total loss: \u001b[1m\u001b[32m0.01093\u001b[0m\u001b[0m | time: 0.219s\n",
            "| Adam | epoch: 183 | loss: 0.01093 - acc: 0.9954 -- iter: 376/963\n",
            "Training Step: 22070  | total loss: \u001b[1m\u001b[32m0.00985\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 183 | loss: 0.00985 - acc: 0.9958 -- iter: 384/963\n",
            "Training Step: 22071  | total loss: \u001b[1m\u001b[32m0.00890\u001b[0m\u001b[0m | time: 0.227s\n",
            "| Adam | epoch: 183 | loss: 0.00890 - acc: 0.9962 -- iter: 392/963\n",
            "Training Step: 22072  | total loss: \u001b[1m\u001b[32m0.00802\u001b[0m\u001b[0m | time: 0.233s\n",
            "| Adam | epoch: 183 | loss: 0.00802 - acc: 0.9966 -- iter: 400/963\n",
            "Training Step: 22073  | total loss: \u001b[1m\u001b[32m0.00723\u001b[0m\u001b[0m | time: 0.239s\n",
            "| Adam | epoch: 183 | loss: 0.00723 - acc: 0.9970 -- iter: 408/963\n",
            "Training Step: 22074  | total loss: \u001b[1m\u001b[32m0.00653\u001b[0m\u001b[0m | time: 0.245s\n",
            "| Adam | epoch: 183 | loss: 0.00653 - acc: 0.9973 -- iter: 416/963\n",
            "Training Step: 22075  | total loss: \u001b[1m\u001b[32m0.00592\u001b[0m\u001b[0m | time: 0.248s\n",
            "| Adam | epoch: 183 | loss: 0.00592 - acc: 0.9975 -- iter: 424/963\n",
            "Training Step: 22076  | total loss: \u001b[1m\u001b[32m0.00823\u001b[0m\u001b[0m | time: 0.253s\n",
            "| Adam | epoch: 183 | loss: 0.00823 - acc: 0.9978 -- iter: 432/963\n",
            "Training Step: 22077  | total loss: \u001b[1m\u001b[32m0.00741\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 183 | loss: 0.00741 - acc: 0.9980 -- iter: 440/963\n",
            "Training Step: 22078  | total loss: \u001b[1m\u001b[32m0.00684\u001b[0m\u001b[0m | time: 0.261s\n",
            "| Adam | epoch: 183 | loss: 0.00684 - acc: 0.9984 -- iter: 448/963\n",
            "Training Step: 22079  | total loss: \u001b[1m\u001b[32m0.00618\u001b[0m\u001b[0m | time: 0.264s\n",
            "| Adam | epoch: 183 | loss: 0.00618 - acc: 0.9984 -- iter: 456/963\n",
            "Training Step: 22080  | total loss: \u001b[1m\u001b[32m0.00561\u001b[0m\u001b[0m | time: 0.268s\n",
            "| Adam | epoch: 183 | loss: 0.00561 - acc: 0.9985 -- iter: 464/963\n",
            "Training Step: 22081  | total loss: \u001b[1m\u001b[32m0.00505\u001b[0m\u001b[0m | time: 0.271s\n",
            "| Adam | epoch: 183 | loss: 0.00505 - acc: 0.9987 -- iter: 472/963\n",
            "Training Step: 22082  | total loss: \u001b[1m\u001b[32m0.00467\u001b[0m\u001b[0m | time: 0.275s\n",
            "| Adam | epoch: 183 | loss: 0.00467 - acc: 0.9988 -- iter: 480/963\n",
            "Training Step: 22083  | total loss: \u001b[1m\u001b[32m0.00432\u001b[0m\u001b[0m | time: 0.278s\n",
            "| Adam | epoch: 183 | loss: 0.00432 - acc: 0.9989 -- iter: 488/963\n",
            "Training Step: 22084  | total loss: \u001b[1m\u001b[32m0.03532\u001b[0m\u001b[0m | time: 0.281s\n",
            "| Adam | epoch: 183 | loss: 0.03532 - acc: 0.9865 -- iter: 496/963\n",
            "Training Step: 22085  | total loss: \u001b[1m\u001b[32m0.03180\u001b[0m\u001b[0m | time: 0.284s\n",
            "| Adam | epoch: 183 | loss: 0.03180 - acc: 0.9879 -- iter: 504/963\n",
            "Training Step: 22086  | total loss: \u001b[1m\u001b[32m0.02864\u001b[0m\u001b[0m | time: 0.289s\n",
            "| Adam | epoch: 183 | loss: 0.02864 - acc: 0.9891 -- iter: 512/963\n",
            "Training Step: 22087  | total loss: \u001b[1m\u001b[32m0.03503\u001b[0m\u001b[0m | time: 0.292s\n",
            "| Adam | epoch: 183 | loss: 0.03503 - acc: 0.9777 -- iter: 520/963\n",
            "Training Step: 22088  | total loss: \u001b[1m\u001b[32m0.03154\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 183 | loss: 0.03154 - acc: 0.9799 -- iter: 528/963\n",
            "Training Step: 22089  | total loss: \u001b[1m\u001b[32m0.02839\u001b[0m\u001b[0m | time: 0.300s\n",
            "| Adam | epoch: 183 | loss: 0.02839 - acc: 0.9819 -- iter: 536/963\n",
            "Training Step: 22090  | total loss: \u001b[1m\u001b[32m0.02865\u001b[0m\u001b[0m | time: 0.304s\n",
            "| Adam | epoch: 183 | loss: 0.02865 - acc: 0.9837 -- iter: 544/963\n",
            "Training Step: 22091  | total loss: \u001b[1m\u001b[32m0.02580\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 183 | loss: 0.02580 - acc: 0.9854 -- iter: 552/963\n",
            "Training Step: 22092  | total loss: \u001b[1m\u001b[32m0.02323\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 183 | loss: 0.02323 - acc: 0.9868 -- iter: 560/963\n",
            "Training Step: 22093  | total loss: \u001b[1m\u001b[32m0.02093\u001b[0m\u001b[0m | time: 0.312s\n",
            "| Adam | epoch: 183 | loss: 0.02093 - acc: 0.9881 -- iter: 568/963\n",
            "Training Step: 22094  | total loss: \u001b[1m\u001b[32m0.01905\u001b[0m\u001b[0m | time: 0.362s\n",
            "| Adam | epoch: 183 | loss: 0.01905 - acc: 0.9893 -- iter: 576/963\n",
            "Training Step: 22095  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.365s\n",
            "| Adam | epoch: 183 | loss: 0.01717 - acc: 0.9904 -- iter: 584/963\n",
            "Training Step: 22096  | total loss: \u001b[1m\u001b[32m0.01550\u001b[0m\u001b[0m | time: 0.370s\n",
            "| Adam | epoch: 183 | loss: 0.01550 - acc: 0.9914 -- iter: 592/963\n",
            "Training Step: 22097  | total loss: \u001b[1m\u001b[32m0.01397\u001b[0m\u001b[0m | time: 0.374s\n",
            "| Adam | epoch: 183 | loss: 0.01397 - acc: 0.9922 -- iter: 600/963\n",
            "Training Step: 22098  | total loss: \u001b[1m\u001b[32m0.01260\u001b[0m\u001b[0m | time: 0.377s\n",
            "| Adam | epoch: 183 | loss: 0.01260 - acc: 0.9930 -- iter: 608/963\n",
            "Training Step: 22099  | total loss: \u001b[1m\u001b[32m0.01134\u001b[0m\u001b[0m | time: 0.379s\n",
            "| Adam | epoch: 183 | loss: 0.01134 - acc: 0.9937 -- iter: 616/963\n",
            "Training Step: 22100  | total loss: \u001b[1m\u001b[32m0.01025\u001b[0m\u001b[0m | time: 0.384s\n",
            "| Adam | epoch: 183 | loss: 0.01025 - acc: 0.9943 -- iter: 624/963\n",
            "Training Step: 22101  | total loss: \u001b[1m\u001b[32m0.00923\u001b[0m\u001b[0m | time: 0.386s\n",
            "| Adam | epoch: 183 | loss: 0.00923 - acc: 0.9949 -- iter: 632/963\n",
            "Training Step: 22102  | total loss: \u001b[1m\u001b[32m0.00832\u001b[0m\u001b[0m | time: 0.389s\n",
            "| Adam | epoch: 183 | loss: 0.00832 - acc: 0.9954 -- iter: 640/963\n",
            "Training Step: 22103  | total loss: \u001b[1m\u001b[32m0.00750\u001b[0m\u001b[0m | time: 0.391s\n",
            "| Adam | epoch: 183 | loss: 0.00750 - acc: 0.9959 -- iter: 648/963\n",
            "Training Step: 22104  | total loss: \u001b[1m\u001b[32m0.01018\u001b[0m\u001b[0m | time: 0.394s\n",
            "| Adam | epoch: 183 | loss: 0.01018 - acc: 0.9963 -- iter: 656/963\n",
            "Training Step: 22105  | total loss: \u001b[1m\u001b[32m0.00917\u001b[0m\u001b[0m | time: 0.397s\n",
            "| Adam | epoch: 183 | loss: 0.00917 - acc: 0.9967 -- iter: 664/963\n",
            "Training Step: 22106  | total loss: \u001b[1m\u001b[32m0.00826\u001b[0m\u001b[0m | time: 0.399s\n",
            "| Adam | epoch: 183 | loss: 0.00826 - acc: 0.9970 -- iter: 672/963\n",
            "Training Step: 22107  | total loss: \u001b[1m\u001b[32m0.01822\u001b[0m\u001b[0m | time: 0.404s\n",
            "| Adam | epoch: 183 | loss: 0.01822 - acc: 0.9848 -- iter: 680/963\n",
            "Training Step: 22108  | total loss: \u001b[1m\u001b[32m0.04677\u001b[0m\u001b[0m | time: 0.409s\n",
            "| Adam | epoch: 183 | loss: 0.04677 - acc: 0.9738 -- iter: 688/963\n",
            "Training Step: 22109  | total loss: \u001b[1m\u001b[32m0.04210\u001b[0m\u001b[0m | time: 0.413s\n",
            "| Adam | epoch: 183 | loss: 0.04210 - acc: 0.9764 -- iter: 696/963\n",
            "Training Step: 22110  | total loss: \u001b[1m\u001b[32m0.03867\u001b[0m\u001b[0m | time: 0.416s\n",
            "| Adam | epoch: 183 | loss: 0.03867 - acc: 0.9788 -- iter: 704/963\n",
            "Training Step: 22111  | total loss: \u001b[1m\u001b[32m0.03492\u001b[0m\u001b[0m | time: 0.419s\n",
            "| Adam | epoch: 183 | loss: 0.03492 - acc: 0.9809 -- iter: 712/963\n",
            "Training Step: 22112  | total loss: \u001b[1m\u001b[32m0.03149\u001b[0m\u001b[0m | time: 0.422s\n",
            "| Adam | epoch: 183 | loss: 0.03149 - acc: 0.9828 -- iter: 720/963\n",
            "Training Step: 22113  | total loss: \u001b[1m\u001b[32m0.02840\u001b[0m\u001b[0m | time: 0.426s\n",
            "| Adam | epoch: 183 | loss: 0.02840 - acc: 0.9845 -- iter: 728/963\n",
            "Training Step: 22114  | total loss: \u001b[1m\u001b[32m0.02559\u001b[0m\u001b[0m | time: 0.429s\n",
            "| Adam | epoch: 183 | loss: 0.02559 - acc: 0.9861 -- iter: 736/963\n",
            "Training Step: 22115  | total loss: \u001b[1m\u001b[32m0.02306\u001b[0m\u001b[0m | time: 0.443s\n",
            "| Adam | epoch: 183 | loss: 0.02306 - acc: 0.9875 -- iter: 744/963\n",
            "Training Step: 22116  | total loss: \u001b[1m\u001b[32m0.02250\u001b[0m\u001b[0m | time: 0.446s\n",
            "| Adam | epoch: 183 | loss: 0.02250 - acc: 0.9887 -- iter: 752/963\n",
            "Training Step: 22117  | total loss: \u001b[1m\u001b[32m0.02521\u001b[0m\u001b[0m | time: 0.449s\n",
            "| Adam | epoch: 183 | loss: 0.02521 - acc: 0.9899 -- iter: 760/963\n",
            "Training Step: 22118  | total loss: \u001b[1m\u001b[32m0.02272\u001b[0m\u001b[0m | time: 0.452s\n",
            "| Adam | epoch: 183 | loss: 0.02272 - acc: 0.9909 -- iter: 768/963\n",
            "Training Step: 22119  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.455s\n",
            "| Adam | epoch: 183 | loss: 0.02048 - acc: 0.9918 -- iter: 776/963\n",
            "Training Step: 22120  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.457s\n",
            "| Adam | epoch: 183 | loss: 0.01846 - acc: 0.9926 -- iter: 784/963\n",
            "Training Step: 22121  | total loss: \u001b[1m\u001b[32m0.01665\u001b[0m\u001b[0m | time: 0.459s\n",
            "| Adam | epoch: 183 | loss: 0.01665 - acc: 0.9933 -- iter: 792/963\n",
            "Training Step: 22122  | total loss: \u001b[1m\u001b[32m0.01515\u001b[0m\u001b[0m | time: 0.462s\n",
            "| Adam | epoch: 183 | loss: 0.01515 - acc: 0.9940 -- iter: 800/963\n",
            "Training Step: 22123  | total loss: \u001b[1m\u001b[32m0.01366\u001b[0m\u001b[0m | time: 0.466s\n",
            "| Adam | epoch: 183 | loss: 0.01366 - acc: 0.9946 -- iter: 808/963\n",
            "Training Step: 22124  | total loss: \u001b[1m\u001b[32m0.01250\u001b[0m\u001b[0m | time: 0.469s\n",
            "| Adam | epoch: 183 | loss: 0.01250 - acc: 0.9951 -- iter: 816/963\n",
            "Training Step: 22125  | total loss: \u001b[1m\u001b[32m0.01147\u001b[0m\u001b[0m | time: 0.474s\n",
            "| Adam | epoch: 183 | loss: 0.01147 - acc: 0.9956 -- iter: 824/963\n",
            "Training Step: 22126  | total loss: \u001b[1m\u001b[32m0.01035\u001b[0m\u001b[0m | time: 0.478s\n",
            "| Adam | epoch: 183 | loss: 0.01035 - acc: 0.9961 -- iter: 832/963\n",
            "Training Step: 22127  | total loss: \u001b[1m\u001b[32m0.00937\u001b[0m\u001b[0m | time: 0.480s\n",
            "| Adam | epoch: 183 | loss: 0.00937 - acc: 0.9965 -- iter: 840/963\n",
            "Training Step: 22128  | total loss: \u001b[1m\u001b[32m0.00844\u001b[0m\u001b[0m | time: 0.484s\n",
            "| Adam | epoch: 183 | loss: 0.00844 - acc: 0.9968 -- iter: 848/963\n",
            "Training Step: 22129  | total loss: \u001b[1m\u001b[32m0.00784\u001b[0m\u001b[0m | time: 0.487s\n",
            "| Adam | epoch: 183 | loss: 0.00784 - acc: 0.9971 -- iter: 856/963\n",
            "Training Step: 22130  | total loss: \u001b[1m\u001b[32m0.00708\u001b[0m\u001b[0m | time: 0.490s\n",
            "| Adam | epoch: 183 | loss: 0.00708 - acc: 0.9974 -- iter: 864/963\n",
            "Training Step: 22131  | total loss: \u001b[1m\u001b[32m0.00639\u001b[0m\u001b[0m | time: 0.495s\n",
            "| Adam | epoch: 183 | loss: 0.00639 - acc: 0.9977 -- iter: 872/963\n",
            "Training Step: 22132  | total loss: \u001b[1m\u001b[32m0.00576\u001b[0m\u001b[0m | time: 0.498s\n",
            "| Adam | epoch: 183 | loss: 0.00576 - acc: 0.9979 -- iter: 880/963\n",
            "Training Step: 22133  | total loss: \u001b[1m\u001b[32m0.00652\u001b[0m\u001b[0m | time: 0.503s\n",
            "| Adam | epoch: 183 | loss: 0.00652 - acc: 0.9981 -- iter: 888/963\n",
            "Training Step: 22134  | total loss: \u001b[1m\u001b[32m0.00594\u001b[0m\u001b[0m | time: 0.507s\n",
            "| Adam | epoch: 183 | loss: 0.00594 - acc: 0.9983 -- iter: 896/963\n",
            "Training Step: 22135  | total loss: \u001b[1m\u001b[32m0.00537\u001b[0m\u001b[0m | time: 0.514s\n",
            "| Adam | epoch: 183 | loss: 0.00537 - acc: 0.9985 -- iter: 904/963\n",
            "Training Step: 22136  | total loss: \u001b[1m\u001b[32m0.00484\u001b[0m\u001b[0m | time: 0.517s\n",
            "| Adam | epoch: 183 | loss: 0.00484 - acc: 0.9986 -- iter: 912/963\n",
            "Training Step: 22137  | total loss: \u001b[1m\u001b[32m0.00437\u001b[0m\u001b[0m | time: 0.521s\n",
            "| Adam | epoch: 183 | loss: 0.00437 - acc: 0.9988 -- iter: 920/963\n",
            "Training Step: 22138  | total loss: \u001b[1m\u001b[32m0.00396\u001b[0m\u001b[0m | time: 0.523s\n",
            "| Adam | epoch: 183 | loss: 0.00396 - acc: 0.9989 -- iter: 928/963\n",
            "Training Step: 22139  | total loss: \u001b[1m\u001b[32m0.00360\u001b[0m\u001b[0m | time: 0.526s\n",
            "| Adam | epoch: 183 | loss: 0.00360 - acc: 0.9990 -- iter: 936/963\n",
            "Training Step: 22140  | total loss: \u001b[1m\u001b[32m0.00325\u001b[0m\u001b[0m | time: 0.530s\n",
            "| Adam | epoch: 183 | loss: 0.00325 - acc: 0.9991 -- iter: 944/963\n",
            "Training Step: 22141  | total loss: \u001b[1m\u001b[32m0.00296\u001b[0m\u001b[0m | time: 0.535s\n",
            "| Adam | epoch: 183 | loss: 0.00296 - acc: 0.9992 -- iter: 952/963\n",
            "Training Step: 22142  | total loss: \u001b[1m\u001b[32m0.00269\u001b[0m\u001b[0m | time: 0.539s\n",
            "| Adam | epoch: 183 | loss: 0.00269 - acc: 0.9993 -- iter: 960/963\n",
            "Training Step: 22143  | total loss: \u001b[1m\u001b[32m0.04013\u001b[0m\u001b[0m | time: 0.541s\n",
            "| Adam | epoch: 183 | loss: 0.04013 - acc: 0.9868 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22144  | total loss: \u001b[1m\u001b[32m0.03617\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 184 | loss: 0.03617 - acc: 0.9882 -- iter: 008/963\n",
            "Training Step: 22145  | total loss: \u001b[1m\u001b[32m0.03258\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 184 | loss: 0.03258 - acc: 0.9893 -- iter: 016/963\n",
            "Training Step: 22146  | total loss: \u001b[1m\u001b[32m0.02937\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 184 | loss: 0.02937 - acc: 0.9904 -- iter: 024/963\n",
            "Training Step: 22147  | total loss: \u001b[1m\u001b[32m0.02645\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 184 | loss: 0.02645 - acc: 0.9914 -- iter: 032/963\n",
            "Training Step: 22148  | total loss: \u001b[1m\u001b[32m0.02604\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 184 | loss: 0.02604 - acc: 0.9922 -- iter: 040/963\n",
            "Training Step: 22149  | total loss: \u001b[1m\u001b[32m0.05349\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 184 | loss: 0.05349 - acc: 0.9805 -- iter: 048/963\n",
            "Training Step: 22150  | total loss: \u001b[1m\u001b[32m0.04900\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 184 | loss: 0.04900 - acc: 0.9825 -- iter: 056/963\n",
            "Training Step: 22151  | total loss: \u001b[1m\u001b[32m0.04411\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 184 | loss: 0.04411 - acc: 0.9842 -- iter: 064/963\n",
            "Training Step: 22152  | total loss: \u001b[1m\u001b[32m0.03972\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 184 | loss: 0.03972 - acc: 0.9858 -- iter: 072/963\n",
            "Training Step: 22153  | total loss: \u001b[1m\u001b[32m0.03576\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 184 | loss: 0.03576 - acc: 0.9872 -- iter: 080/963\n",
            "Training Step: 22154  | total loss: \u001b[1m\u001b[32m0.03219\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 184 | loss: 0.03219 - acc: 0.9885 -- iter: 088/963\n",
            "Training Step: 22155  | total loss: \u001b[1m\u001b[32m0.02898\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 184 | loss: 0.02898 - acc: 0.9896 -- iter: 096/963\n",
            "Training Step: 22156  | total loss: \u001b[1m\u001b[32m0.02610\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 184 | loss: 0.02610 - acc: 0.9907 -- iter: 104/963\n",
            "Training Step: 22157  | total loss: \u001b[1m\u001b[32m0.03926\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 184 | loss: 0.03926 - acc: 0.9791 -- iter: 112/963\n",
            "Training Step: 22158  | total loss: \u001b[1m\u001b[32m0.03535\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 184 | loss: 0.03535 - acc: 0.9812 -- iter: 120/963\n",
            "Training Step: 22159  | total loss: \u001b[1m\u001b[32m0.03204\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 184 | loss: 0.03204 - acc: 0.9831 -- iter: 128/963\n",
            "Training Step: 22160  | total loss: \u001b[1m\u001b[32m0.02890\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 184 | loss: 0.02890 - acc: 0.9848 -- iter: 136/963\n",
            "Training Step: 22161  | total loss: \u001b[1m\u001b[32m0.04525\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 184 | loss: 0.04525 - acc: 0.9738 -- iter: 144/963\n",
            "Training Step: 22162  | total loss: \u001b[1m\u001b[32m0.04073\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 184 | loss: 0.04073 - acc: 0.9764 -- iter: 152/963\n",
            "Training Step: 22163  | total loss: \u001b[1m\u001b[32m0.03667\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 184 | loss: 0.03667 - acc: 0.9788 -- iter: 160/963\n",
            "Training Step: 22164  | total loss: \u001b[1m\u001b[32m0.03311\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 184 | loss: 0.03311 - acc: 0.9809 -- iter: 168/963\n",
            "Training Step: 22165  | total loss: \u001b[1m\u001b[32m0.02981\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 184 | loss: 0.02981 - acc: 0.9828 -- iter: 176/963\n",
            "Training Step: 22166  | total loss: \u001b[1m\u001b[32m0.03901\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 184 | loss: 0.03901 - acc: 0.9720 -- iter: 184/963\n",
            "Training Step: 22167  | total loss: \u001b[1m\u001b[32m0.03513\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 184 | loss: 0.03513 - acc: 0.9748 -- iter: 192/963\n",
            "Training Step: 22168  | total loss: \u001b[1m\u001b[32m0.03162\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 184 | loss: 0.03162 - acc: 0.9773 -- iter: 200/963\n",
            "Training Step: 22169  | total loss: \u001b[1m\u001b[32m0.02848\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 184 | loss: 0.02848 - acc: 0.9796 -- iter: 208/963\n",
            "Training Step: 22170  | total loss: \u001b[1m\u001b[32m0.02564\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 184 | loss: 0.02564 - acc: 0.9816 -- iter: 216/963\n",
            "Training Step: 22171  | total loss: \u001b[1m\u001b[32m0.02310\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 184 | loss: 0.02310 - acc: 0.9835 -- iter: 224/963\n",
            "Training Step: 22172  | total loss: \u001b[1m\u001b[32m0.02086\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 184 | loss: 0.02086 - acc: 0.9851 -- iter: 232/963\n",
            "Training Step: 22173  | total loss: \u001b[1m\u001b[32m0.01879\u001b[0m\u001b[0m | time: 0.134s\n",
            "| Adam | epoch: 184 | loss: 0.01879 - acc: 0.9866 -- iter: 240/963\n",
            "Training Step: 22174  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 184 | loss: 0.01694 - acc: 0.9880 -- iter: 248/963\n",
            "Training Step: 22175  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 184 | loss: 0.01526 - acc: 0.9892 -- iter: 256/963\n",
            "Training Step: 22176  | total loss: \u001b[1m\u001b[32m0.01375\u001b[0m\u001b[0m | time: 0.149s\n",
            "| Adam | epoch: 184 | loss: 0.01375 - acc: 0.9902 -- iter: 264/963\n",
            "Training Step: 22177  | total loss: \u001b[1m\u001b[32m0.01404\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 184 | loss: 0.01404 - acc: 0.9912 -- iter: 272/963\n",
            "Training Step: 22178  | total loss: \u001b[1m\u001b[32m0.01265\u001b[0m\u001b[0m | time: 0.160s\n",
            "| Adam | epoch: 184 | loss: 0.01265 - acc: 0.9921 -- iter: 280/963\n",
            "Training Step: 22179  | total loss: \u001b[1m\u001b[32m0.01139\u001b[0m\u001b[0m | time: 0.165s\n",
            "| Adam | epoch: 184 | loss: 0.01139 - acc: 0.9929 -- iter: 288/963\n",
            "Training Step: 22180  | total loss: \u001b[1m\u001b[32m0.01161\u001b[0m\u001b[0m | time: 0.172s\n",
            "| Adam | epoch: 184 | loss: 0.01161 - acc: 0.9936 -- iter: 296/963\n",
            "Training Step: 22181  | total loss: \u001b[1m\u001b[32m0.01046\u001b[0m\u001b[0m | time: 0.178s\n",
            "| Adam | epoch: 184 | loss: 0.01046 - acc: 0.9942 -- iter: 304/963\n",
            "Training Step: 22182  | total loss: \u001b[1m\u001b[32m0.00947\u001b[0m\u001b[0m | time: 0.183s\n",
            "| Adam | epoch: 184 | loss: 0.00947 - acc: 0.9948 -- iter: 312/963\n",
            "Training Step: 22183  | total loss: \u001b[1m\u001b[32m0.00854\u001b[0m\u001b[0m | time: 0.190s\n",
            "| Adam | epoch: 184 | loss: 0.00854 - acc: 0.9953 -- iter: 320/963\n",
            "Training Step: 22184  | total loss: \u001b[1m\u001b[32m0.00770\u001b[0m\u001b[0m | time: 0.198s\n",
            "| Adam | epoch: 184 | loss: 0.00770 - acc: 0.9958 -- iter: 328/963\n",
            "Training Step: 22185  | total loss: \u001b[1m\u001b[32m0.00695\u001b[0m\u001b[0m | time: 0.205s\n",
            "| Adam | epoch: 184 | loss: 0.00695 - acc: 0.9962 -- iter: 336/963\n",
            "Training Step: 22186  | total loss: \u001b[1m\u001b[32m0.00627\u001b[0m\u001b[0m | time: 0.214s\n",
            "| Adam | epoch: 184 | loss: 0.00627 - acc: 0.9966 -- iter: 344/963\n",
            "Training Step: 22187  | total loss: \u001b[1m\u001b[32m0.00566\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 184 | loss: 0.00566 - acc: 0.9969 -- iter: 352/963\n",
            "Training Step: 22188  | total loss: \u001b[1m\u001b[32m0.00513\u001b[0m\u001b[0m | time: 0.225s\n",
            "| Adam | epoch: 184 | loss: 0.00513 - acc: 0.9972 -- iter: 360/963\n",
            "Training Step: 22189  | total loss: \u001b[1m\u001b[32m0.00463\u001b[0m\u001b[0m | time: 0.232s\n",
            "| Adam | epoch: 184 | loss: 0.00463 - acc: 0.9975 -- iter: 368/963\n",
            "Training Step: 22190  | total loss: \u001b[1m\u001b[32m0.00419\u001b[0m\u001b[0m | time: 0.238s\n",
            "| Adam | epoch: 184 | loss: 0.00419 - acc: 0.9978 -- iter: 376/963\n",
            "Training Step: 22191  | total loss: \u001b[1m\u001b[32m0.00378\u001b[0m\u001b[0m | time: 0.245s\n",
            "| Adam | epoch: 184 | loss: 0.00378 - acc: 0.9980 -- iter: 384/963\n",
            "Training Step: 22192  | total loss: \u001b[1m\u001b[32m0.00340\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 184 | loss: 0.00340 - acc: 0.9982 -- iter: 392/963\n",
            "Training Step: 22193  | total loss: \u001b[1m\u001b[32m0.00316\u001b[0m\u001b[0m | time: 0.254s\n",
            "| Adam | epoch: 184 | loss: 0.00316 - acc: 0.9984 -- iter: 400/963\n",
            "Training Step: 22194  | total loss: \u001b[1m\u001b[32m0.00287\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 184 | loss: 0.00287 - acc: 0.9985 -- iter: 408/963\n",
            "Training Step: 22195  | total loss: \u001b[1m\u001b[32m0.00262\u001b[0m\u001b[0m | time: 0.261s\n",
            "| Adam | epoch: 184 | loss: 0.00262 - acc: 0.9987 -- iter: 416/963\n",
            "Training Step: 22196  | total loss: \u001b[1m\u001b[32m0.00237\u001b[0m\u001b[0m | time: 0.267s\n",
            "| Adam | epoch: 184 | loss: 0.00237 - acc: 0.9988 -- iter: 424/963\n",
            "Training Step: 22197  | total loss: \u001b[1m\u001b[32m0.00240\u001b[0m\u001b[0m | time: 0.271s\n",
            "| Adam | epoch: 184 | loss: 0.00240 - acc: 0.9989 -- iter: 432/963\n",
            "Training Step: 22198  | total loss: \u001b[1m\u001b[32m0.00471\u001b[0m\u001b[0m | time: 0.274s\n",
            "| Adam | epoch: 184 | loss: 0.00471 - acc: 0.9990 -- iter: 440/963\n",
            "Training Step: 22199  | total loss: \u001b[1m\u001b[32m0.00426\u001b[0m\u001b[0m | time: 0.278s\n",
            "| Adam | epoch: 184 | loss: 0.00426 - acc: 0.9991 -- iter: 448/963\n",
            "Training Step: 22200  | total loss: \u001b[1m\u001b[32m0.00385\u001b[0m\u001b[0m | time: 0.281s\n",
            "| Adam | epoch: 184 | loss: 0.00385 - acc: 0.9992 -- iter: 456/963\n",
            "Training Step: 22201  | total loss: \u001b[1m\u001b[32m0.00349\u001b[0m\u001b[0m | time: 0.284s\n",
            "| Adam | epoch: 184 | loss: 0.00349 - acc: 0.9993 -- iter: 464/963\n",
            "Training Step: 22202  | total loss: \u001b[1m\u001b[32m0.00326\u001b[0m\u001b[0m | time: 0.288s\n",
            "| Adam | epoch: 184 | loss: 0.00326 - acc: 0.9994 -- iter: 472/963\n",
            "Training Step: 22203  | total loss: \u001b[1m\u001b[32m0.00294\u001b[0m\u001b[0m | time: 0.291s\n",
            "| Adam | epoch: 184 | loss: 0.00294 - acc: 0.9994 -- iter: 480/963\n",
            "Training Step: 22204  | total loss: \u001b[1m\u001b[32m0.00266\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 184 | loss: 0.00266 - acc: 0.9995 -- iter: 488/963\n",
            "Training Step: 22205  | total loss: \u001b[1m\u001b[32m0.00241\u001b[0m\u001b[0m | time: 0.301s\n",
            "| Adam | epoch: 184 | loss: 0.00241 - acc: 0.9995 -- iter: 496/963\n",
            "Training Step: 22206  | total loss: \u001b[1m\u001b[32m0.00219\u001b[0m\u001b[0m | time: 0.304s\n",
            "| Adam | epoch: 184 | loss: 0.00219 - acc: 0.9996 -- iter: 504/963\n",
            "Training Step: 22207  | total loss: \u001b[1m\u001b[32m0.00203\u001b[0m\u001b[0m | time: 0.308s\n",
            "| Adam | epoch: 184 | loss: 0.00203 - acc: 0.9996 -- iter: 512/963\n",
            "Training Step: 22208  | total loss: \u001b[1m\u001b[32m0.00189\u001b[0m\u001b[0m | time: 0.315s\n",
            "| Adam | epoch: 184 | loss: 0.00189 - acc: 0.9997 -- iter: 520/963\n",
            "Training Step: 22209  | total loss: \u001b[1m\u001b[32m0.00172\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 184 | loss: 0.00172 - acc: 0.9997 -- iter: 528/963\n",
            "Training Step: 22210  | total loss: \u001b[1m\u001b[32m0.03832\u001b[0m\u001b[0m | time: 0.328s\n",
            "| Adam | epoch: 184 | loss: 0.03832 - acc: 0.9872 -- iter: 536/963\n",
            "Training Step: 22211  | total loss: \u001b[1m\u001b[32m0.03449\u001b[0m\u001b[0m | time: 0.334s\n",
            "| Adam | epoch: 184 | loss: 0.03449 - acc: 0.9885 -- iter: 544/963\n",
            "Training Step: 22212  | total loss: \u001b[1m\u001b[32m0.03106\u001b[0m\u001b[0m | time: 0.341s\n",
            "| Adam | epoch: 184 | loss: 0.03106 - acc: 0.9897 -- iter: 552/963\n",
            "Training Step: 22213  | total loss: \u001b[1m\u001b[32m0.02797\u001b[0m\u001b[0m | time: 0.346s\n",
            "| Adam | epoch: 184 | loss: 0.02797 - acc: 0.9907 -- iter: 560/963\n",
            "Training Step: 22214  | total loss: \u001b[1m\u001b[32m0.02518\u001b[0m\u001b[0m | time: 0.353s\n",
            "| Adam | epoch: 184 | loss: 0.02518 - acc: 0.9916 -- iter: 568/963\n",
            "Training Step: 22215  | total loss: \u001b[1m\u001b[32m0.02270\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 184 | loss: 0.02270 - acc: 0.9925 -- iter: 576/963\n",
            "Training Step: 22216  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.365s\n",
            "| Adam | epoch: 184 | loss: 0.02044 - acc: 0.9932 -- iter: 584/963\n",
            "Training Step: 22217  | total loss: \u001b[1m\u001b[32m0.01842\u001b[0m\u001b[0m | time: 0.368s\n",
            "| Adam | epoch: 184 | loss: 0.01842 - acc: 0.9939 -- iter: 592/963\n",
            "Training Step: 22218  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.372s\n",
            "| Adam | epoch: 184 | loss: 0.01669 - acc: 0.9945 -- iter: 600/963\n",
            "Training Step: 22219  | total loss: \u001b[1m\u001b[32m0.01510\u001b[0m\u001b[0m | time: 0.376s\n",
            "| Adam | epoch: 184 | loss: 0.01510 - acc: 0.9951 -- iter: 608/963\n",
            "Training Step: 22220  | total loss: \u001b[1m\u001b[32m0.01361\u001b[0m\u001b[0m | time: 0.381s\n",
            "| Adam | epoch: 184 | loss: 0.01361 - acc: 0.9955 -- iter: 616/963\n",
            "Training Step: 22221  | total loss: \u001b[1m\u001b[32m0.01229\u001b[0m\u001b[0m | time: 0.387s\n",
            "| Adam | epoch: 184 | loss: 0.01229 - acc: 0.9960 -- iter: 624/963\n",
            "Training Step: 22222  | total loss: \u001b[1m\u001b[32m0.04842\u001b[0m\u001b[0m | time: 0.392s\n",
            "| Adam | epoch: 184 | loss: 0.04842 - acc: 0.9839 -- iter: 632/963\n",
            "Training Step: 22223  | total loss: \u001b[1m\u001b[32m0.04950\u001b[0m\u001b[0m | time: 0.397s\n",
            "| Adam | epoch: 184 | loss: 0.04950 - acc: 0.9855 -- iter: 640/963\n",
            "Training Step: 22224  | total loss: \u001b[1m\u001b[32m0.04457\u001b[0m\u001b[0m | time: 0.401s\n",
            "| Adam | epoch: 184 | loss: 0.04457 - acc: 0.9870 -- iter: 648/963\n",
            "Training Step: 22225  | total loss: \u001b[1m\u001b[32m0.04014\u001b[0m\u001b[0m | time: 0.410s\n",
            "| Adam | epoch: 184 | loss: 0.04014 - acc: 0.9883 -- iter: 656/963\n",
            "Training Step: 22226  | total loss: \u001b[1m\u001b[32m0.03617\u001b[0m\u001b[0m | time: 0.418s\n",
            "| Adam | epoch: 184 | loss: 0.03617 - acc: 0.9894 -- iter: 664/963\n",
            "Training Step: 22227  | total loss: \u001b[1m\u001b[32m0.03256\u001b[0m\u001b[0m | time: 0.423s\n",
            "| Adam | epoch: 184 | loss: 0.03256 - acc: 0.9905 -- iter: 672/963\n",
            "Training Step: 22228  | total loss: \u001b[1m\u001b[32m0.02932\u001b[0m\u001b[0m | time: 0.430s\n",
            "| Adam | epoch: 184 | loss: 0.02932 - acc: 0.9914 -- iter: 680/963\n",
            "Training Step: 22229  | total loss: \u001b[1m\u001b[32m0.02640\u001b[0m\u001b[0m | time: 0.435s\n",
            "| Adam | epoch: 184 | loss: 0.02640 - acc: 0.9923 -- iter: 688/963\n",
            "Training Step: 22230  | total loss: \u001b[1m\u001b[32m0.02378\u001b[0m\u001b[0m | time: 0.441s\n",
            "| Adam | epoch: 184 | loss: 0.02378 - acc: 0.9931 -- iter: 696/963\n",
            "Training Step: 22231  | total loss: \u001b[1m\u001b[32m0.02165\u001b[0m\u001b[0m | time: 0.445s\n",
            "| Adam | epoch: 184 | loss: 0.02165 - acc: 0.9938 -- iter: 704/963\n",
            "Training Step: 22232  | total loss: \u001b[1m\u001b[32m0.01950\u001b[0m\u001b[0m | time: 0.450s\n",
            "| Adam | epoch: 184 | loss: 0.01950 - acc: 0.9944 -- iter: 712/963\n",
            "Training Step: 22233  | total loss: \u001b[1m\u001b[32m0.05019\u001b[0m\u001b[0m | time: 0.456s\n",
            "| Adam | epoch: 184 | loss: 0.05019 - acc: 0.9824 -- iter: 720/963\n",
            "Training Step: 22234  | total loss: \u001b[1m\u001b[32m0.07768\u001b[0m\u001b[0m | time: 0.460s\n",
            "| Adam | epoch: 184 | loss: 0.07768 - acc: 0.9717 -- iter: 728/963\n",
            "Training Step: 22235  | total loss: \u001b[1m\u001b[32m0.07341\u001b[0m\u001b[0m | time: 0.464s\n",
            "| Adam | epoch: 184 | loss: 0.07341 - acc: 0.9745 -- iter: 736/963\n",
            "Training Step: 22236  | total loss: \u001b[1m\u001b[32m0.06608\u001b[0m\u001b[0m | time: 0.467s\n",
            "| Adam | epoch: 184 | loss: 0.06608 - acc: 0.9771 -- iter: 744/963\n",
            "Training Step: 22237  | total loss: \u001b[1m\u001b[32m0.05950\u001b[0m\u001b[0m | time: 0.472s\n",
            "| Adam | epoch: 184 | loss: 0.05950 - acc: 0.9794 -- iter: 752/963\n",
            "Training Step: 22238  | total loss: \u001b[1m\u001b[32m0.05491\u001b[0m\u001b[0m | time: 0.475s\n",
            "| Adam | epoch: 184 | loss: 0.05491 - acc: 0.9814 -- iter: 760/963\n",
            "Training Step: 22239  | total loss: \u001b[1m\u001b[32m0.04945\u001b[0m\u001b[0m | time: 0.479s\n",
            "| Adam | epoch: 184 | loss: 0.04945 - acc: 0.9833 -- iter: 768/963\n",
            "Training Step: 22240  | total loss: \u001b[1m\u001b[32m0.04452\u001b[0m\u001b[0m | time: 0.482s\n",
            "| Adam | epoch: 184 | loss: 0.04452 - acc: 0.9850 -- iter: 776/963\n",
            "Training Step: 22241  | total loss: \u001b[1m\u001b[32m0.04011\u001b[0m\u001b[0m | time: 0.486s\n",
            "| Adam | epoch: 184 | loss: 0.04011 - acc: 0.9865 -- iter: 784/963\n",
            "Training Step: 22242  | total loss: \u001b[1m\u001b[32m0.03611\u001b[0m\u001b[0m | time: 0.490s\n",
            "| Adam | epoch: 184 | loss: 0.03611 - acc: 0.9878 -- iter: 792/963\n",
            "Training Step: 22243  | total loss: \u001b[1m\u001b[32m0.03601\u001b[0m\u001b[0m | time: 0.495s\n",
            "| Adam | epoch: 184 | loss: 0.03601 - acc: 0.9890 -- iter: 800/963\n",
            "Training Step: 22244  | total loss: \u001b[1m\u001b[32m0.03980\u001b[0m\u001b[0m | time: 0.499s\n",
            "| Adam | epoch: 184 | loss: 0.03980 - acc: 0.9901 -- iter: 808/963\n",
            "Training Step: 22245  | total loss: \u001b[1m\u001b[32m0.03585\u001b[0m\u001b[0m | time: 0.502s\n",
            "| Adam | epoch: 184 | loss: 0.03585 - acc: 0.9911 -- iter: 816/963\n",
            "Training Step: 22246  | total loss: \u001b[1m\u001b[32m0.03227\u001b[0m\u001b[0m | time: 0.505s\n",
            "| Adam | epoch: 184 | loss: 0.03227 - acc: 0.9920 -- iter: 824/963\n",
            "Training Step: 22247  | total loss: \u001b[1m\u001b[32m0.03007\u001b[0m\u001b[0m | time: 0.508s\n",
            "| Adam | epoch: 184 | loss: 0.03007 - acc: 0.9928 -- iter: 832/963\n",
            "Training Step: 22248  | total loss: \u001b[1m\u001b[32m0.02708\u001b[0m\u001b[0m | time: 0.511s\n",
            "| Adam | epoch: 184 | loss: 0.02708 - acc: 0.9935 -- iter: 840/963\n",
            "Training Step: 22249  | total loss: \u001b[1m\u001b[32m0.04910\u001b[0m\u001b[0m | time: 0.515s\n",
            "| Adam | epoch: 184 | loss: 0.04910 - acc: 0.9817 -- iter: 848/963\n",
            "Training Step: 22250  | total loss: \u001b[1m\u001b[32m0.04421\u001b[0m\u001b[0m | time: 0.517s\n",
            "| Adam | epoch: 184 | loss: 0.04421 - acc: 0.9835 -- iter: 856/963\n",
            "Training Step: 22251  | total loss: \u001b[1m\u001b[32m0.03980\u001b[0m\u001b[0m | time: 0.521s\n",
            "| Adam | epoch: 184 | loss: 0.03980 - acc: 0.9852 -- iter: 864/963\n",
            "Training Step: 22252  | total loss: \u001b[1m\u001b[32m0.04164\u001b[0m\u001b[0m | time: 0.524s\n",
            "| Adam | epoch: 184 | loss: 0.04164 - acc: 0.9866 -- iter: 872/963\n",
            "Training Step: 22253  | total loss: \u001b[1m\u001b[32m0.03750\u001b[0m\u001b[0m | time: 0.527s\n",
            "| Adam | epoch: 184 | loss: 0.03750 - acc: 0.9880 -- iter: 880/963\n",
            "Training Step: 22254  | total loss: \u001b[1m\u001b[32m0.03376\u001b[0m\u001b[0m | time: 0.530s\n",
            "| Adam | epoch: 184 | loss: 0.03376 - acc: 0.9892 -- iter: 888/963\n",
            "Training Step: 22255  | total loss: \u001b[1m\u001b[32m0.03039\u001b[0m\u001b[0m | time: 0.534s\n",
            "| Adam | epoch: 184 | loss: 0.03039 - acc: 0.9903 -- iter: 896/963\n",
            "Training Step: 22256  | total loss: \u001b[1m\u001b[32m0.02736\u001b[0m\u001b[0m | time: 0.537s\n",
            "| Adam | epoch: 184 | loss: 0.02736 - acc: 0.9912 -- iter: 904/963\n",
            "Training Step: 22257  | total loss: \u001b[1m\u001b[32m0.02464\u001b[0m\u001b[0m | time: 0.540s\n",
            "| Adam | epoch: 184 | loss: 0.02464 - acc: 0.9921 -- iter: 912/963\n",
            "Training Step: 22258  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.543s\n",
            "| Adam | epoch: 184 | loss: 0.02219 - acc: 0.9929 -- iter: 920/963\n",
            "Training Step: 22259  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.547s\n",
            "| Adam | epoch: 184 | loss: 0.02000 - acc: 0.9936 -- iter: 928/963\n",
            "Training Step: 22260  | total loss: \u001b[1m\u001b[32m0.01806\u001b[0m\u001b[0m | time: 0.550s\n",
            "| Adam | epoch: 184 | loss: 0.01806 - acc: 0.9942 -- iter: 936/963\n",
            "Training Step: 22261  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.552s\n",
            "| Adam | epoch: 184 | loss: 0.01627 - acc: 0.9948 -- iter: 944/963\n",
            "Training Step: 22262  | total loss: \u001b[1m\u001b[32m0.01465\u001b[0m\u001b[0m | time: 0.555s\n",
            "| Adam | epoch: 184 | loss: 0.01465 - acc: 0.9953 -- iter: 952/963\n",
            "Training Step: 22263  | total loss: \u001b[1m\u001b[32m0.01321\u001b[0m\u001b[0m | time: 0.558s\n",
            "| Adam | epoch: 184 | loss: 0.01321 - acc: 0.9958 -- iter: 960/963\n",
            "Training Step: 22264  | total loss: \u001b[1m\u001b[32m0.01271\u001b[0m\u001b[0m | time: 0.562s\n",
            "| Adam | epoch: 184 | loss: 0.01271 - acc: 0.9962 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22265  | total loss: \u001b[1m\u001b[32m0.01147\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 185 | loss: 0.01147 - acc: 0.9966 -- iter: 008/963\n",
            "Training Step: 22266  | total loss: \u001b[1m\u001b[32m0.01034\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 185 | loss: 0.01034 - acc: 0.9969 -- iter: 016/963\n",
            "Training Step: 22267  | total loss: \u001b[1m\u001b[32m0.00931\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 185 | loss: 0.00931 - acc: 0.9972 -- iter: 024/963\n",
            "Training Step: 22268  | total loss: \u001b[1m\u001b[32m0.00876\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 185 | loss: 0.00876 - acc: 0.9975 -- iter: 032/963\n",
            "Training Step: 22269  | total loss: \u001b[1m\u001b[32m0.00792\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 185 | loss: 0.00792 - acc: 0.9978 -- iter: 040/963\n",
            "Training Step: 22270  | total loss: \u001b[1m\u001b[32m0.04573\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 185 | loss: 0.04573 - acc: 0.9855 -- iter: 048/963\n",
            "Training Step: 22271  | total loss: \u001b[1m\u001b[32m0.06119\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 185 | loss: 0.06119 - acc: 0.9744 -- iter: 056/963\n",
            "Training Step: 22272  | total loss: \u001b[1m\u001b[32m0.05513\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 185 | loss: 0.05513 - acc: 0.9770 -- iter: 064/963\n",
            "Training Step: 22273  | total loss: \u001b[1m\u001b[32m0.04967\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 185 | loss: 0.04967 - acc: 0.9793 -- iter: 072/963\n",
            "Training Step: 22274  | total loss: \u001b[1m\u001b[32m0.04471\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 185 | loss: 0.04471 - acc: 0.9814 -- iter: 080/963\n",
            "Training Step: 22275  | total loss: \u001b[1m\u001b[32m0.04028\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 185 | loss: 0.04028 - acc: 0.9832 -- iter: 088/963\n",
            "Training Step: 22276  | total loss: \u001b[1m\u001b[32m0.03629\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 185 | loss: 0.03629 - acc: 0.9849 -- iter: 096/963\n",
            "Training Step: 22277  | total loss: \u001b[1m\u001b[32m0.03268\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 185 | loss: 0.03268 - acc: 0.9864 -- iter: 104/963\n",
            "Training Step: 22278  | total loss: \u001b[1m\u001b[32m0.02943\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 185 | loss: 0.02943 - acc: 0.9878 -- iter: 112/963\n",
            "Training Step: 22279  | total loss: \u001b[1m\u001b[32m0.02782\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 185 | loss: 0.02782 - acc: 0.9890 -- iter: 120/963\n",
            "Training Step: 22280  | total loss: \u001b[1m\u001b[32m0.02504\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 185 | loss: 0.02504 - acc: 0.9901 -- iter: 128/963\n",
            "Training Step: 22281  | total loss: \u001b[1m\u001b[32m0.02256\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 185 | loss: 0.02256 - acc: 0.9911 -- iter: 136/963\n",
            "Training Step: 22282  | total loss: \u001b[1m\u001b[32m0.02031\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 185 | loss: 0.02031 - acc: 0.9920 -- iter: 144/963\n",
            "Training Step: 22283  | total loss: \u001b[1m\u001b[32m0.01832\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 185 | loss: 0.01832 - acc: 0.9928 -- iter: 152/963\n",
            "Training Step: 22284  | total loss: \u001b[1m\u001b[32m0.01653\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 185 | loss: 0.01653 - acc: 0.9935 -- iter: 160/963\n",
            "Training Step: 22285  | total loss: \u001b[1m\u001b[32m0.01491\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 185 | loss: 0.01491 - acc: 0.9942 -- iter: 168/963\n",
            "Training Step: 22286  | total loss: \u001b[1m\u001b[32m0.01344\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 185 | loss: 0.01344 - acc: 0.9947 -- iter: 176/963\n",
            "Training Step: 22287  | total loss: \u001b[1m\u001b[32m0.01210\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 185 | loss: 0.01210 - acc: 0.9953 -- iter: 184/963\n",
            "Training Step: 22288  | total loss: \u001b[1m\u001b[32m0.01095\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 185 | loss: 0.01095 - acc: 0.9957 -- iter: 192/963\n",
            "Training Step: 22289  | total loss: \u001b[1m\u001b[32m0.01007\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 185 | loss: 0.01007 - acc: 0.9962 -- iter: 200/963\n",
            "Training Step: 22290  | total loss: \u001b[1m\u001b[32m0.00910\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 185 | loss: 0.00910 - acc: 0.9965 -- iter: 208/963\n",
            "Training Step: 22291  | total loss: \u001b[1m\u001b[32m0.00821\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 185 | loss: 0.00821 - acc: 0.9969 -- iter: 216/963\n",
            "Training Step: 22292  | total loss: \u001b[1m\u001b[32m0.00744\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 185 | loss: 0.00744 - acc: 0.9972 -- iter: 224/963\n",
            "Training Step: 22293  | total loss: \u001b[1m\u001b[32m0.00671\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 185 | loss: 0.00671 - acc: 0.9975 -- iter: 232/963\n",
            "Training Step: 22294  | total loss: \u001b[1m\u001b[32m0.00609\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 185 | loss: 0.00609 - acc: 0.9977 -- iter: 240/963\n",
            "Training Step: 22295  | total loss: \u001b[1m\u001b[32m0.00551\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 185 | loss: 0.00551 - acc: 0.9980 -- iter: 248/963\n",
            "Training Step: 22296  | total loss: \u001b[1m\u001b[32m0.00499\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 185 | loss: 0.00499 - acc: 0.9982 -- iter: 256/963\n",
            "Training Step: 22297  | total loss: \u001b[1m\u001b[32m0.01652\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 185 | loss: 0.01652 - acc: 0.9858 -- iter: 264/963\n",
            "Training Step: 22298  | total loss: \u001b[1m\u001b[32m0.01490\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 185 | loss: 0.01490 - acc: 0.9873 -- iter: 272/963\n",
            "Training Step: 22299  | total loss: \u001b[1m\u001b[32m0.01366\u001b[0m\u001b[0m | time: 0.127s\n",
            "| Adam | epoch: 185 | loss: 0.01366 - acc: 0.9885 -- iter: 280/963\n",
            "Training Step: 22300  | total loss: \u001b[1m\u001b[32m0.02721\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 185 | loss: 0.02721 - acc: 0.9772 -- iter: 288/963\n",
            "Training Step: 22301  | total loss: \u001b[1m\u001b[32m0.02451\u001b[0m\u001b[0m | time: 0.133s\n",
            "| Adam | epoch: 185 | loss: 0.02451 - acc: 0.9795 -- iter: 296/963\n",
            "Training Step: 22302  | total loss: \u001b[1m\u001b[32m0.02207\u001b[0m\u001b[0m | time: 0.136s\n",
            "| Adam | epoch: 185 | loss: 0.02207 - acc: 0.9815 -- iter: 304/963\n",
            "Training Step: 22303  | total loss: \u001b[1m\u001b[32m0.01990\u001b[0m\u001b[0m | time: 0.139s\n",
            "| Adam | epoch: 185 | loss: 0.01990 - acc: 0.9834 -- iter: 312/963\n",
            "Training Step: 22304  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.143s\n",
            "| Adam | epoch: 185 | loss: 0.01795 - acc: 0.9850 -- iter: 320/963\n",
            "Training Step: 22305  | total loss: \u001b[1m\u001b[32m0.02540\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 185 | loss: 0.02540 - acc: 0.9740 -- iter: 328/963\n",
            "Training Step: 22306  | total loss: \u001b[1m\u001b[32m0.02306\u001b[0m\u001b[0m | time: 0.150s\n",
            "| Adam | epoch: 185 | loss: 0.02306 - acc: 0.9766 -- iter: 336/963\n",
            "Training Step: 22307  | total loss: \u001b[1m\u001b[32m0.02902\u001b[0m\u001b[0m | time: 0.158s\n",
            "| Adam | epoch: 185 | loss: 0.02902 - acc: 0.9790 -- iter: 344/963\n",
            "Training Step: 22308  | total loss: \u001b[1m\u001b[32m0.02613\u001b[0m\u001b[0m | time: 0.164s\n",
            "| Adam | epoch: 185 | loss: 0.02613 - acc: 0.9811 -- iter: 352/963\n",
            "Training Step: 22309  | total loss: \u001b[1m\u001b[32m0.02355\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 185 | loss: 0.02355 - acc: 0.9830 -- iter: 360/963\n",
            "Training Step: 22310  | total loss: \u001b[1m\u001b[32m0.05623\u001b[0m\u001b[0m | time: 0.172s\n",
            "| Adam | epoch: 185 | loss: 0.05623 - acc: 0.9722 -- iter: 368/963\n",
            "Training Step: 22311  | total loss: \u001b[1m\u001b[32m0.05063\u001b[0m\u001b[0m | time: 0.175s\n",
            "| Adam | epoch: 185 | loss: 0.05063 - acc: 0.9749 -- iter: 376/963\n",
            "Training Step: 22312  | total loss: \u001b[1m\u001b[32m0.06542\u001b[0m\u001b[0m | time: 0.178s\n",
            "| Adam | epoch: 185 | loss: 0.06542 - acc: 0.9650 -- iter: 384/963\n",
            "Training Step: 22313  | total loss: \u001b[1m\u001b[32m0.05890\u001b[0m\u001b[0m | time: 0.181s\n",
            "| Adam | epoch: 185 | loss: 0.05890 - acc: 0.9685 -- iter: 392/963\n",
            "Training Step: 22314  | total loss: \u001b[1m\u001b[32m0.05303\u001b[0m\u001b[0m | time: 0.184s\n",
            "| Adam | epoch: 185 | loss: 0.05303 - acc: 0.9716 -- iter: 400/963\n",
            "Training Step: 22315  | total loss: \u001b[1m\u001b[32m0.04773\u001b[0m\u001b[0m | time: 0.190s\n",
            "| Adam | epoch: 185 | loss: 0.04773 - acc: 0.9745 -- iter: 408/963\n",
            "Training Step: 22316  | total loss: \u001b[1m\u001b[32m0.04298\u001b[0m\u001b[0m | time: 0.193s\n",
            "| Adam | epoch: 185 | loss: 0.04298 - acc: 0.9770 -- iter: 416/963\n",
            "Training Step: 22317  | total loss: \u001b[1m\u001b[32m0.03869\u001b[0m\u001b[0m | time: 0.197s\n",
            "| Adam | epoch: 185 | loss: 0.03869 - acc: 0.9793 -- iter: 424/963\n",
            "Training Step: 22318  | total loss: \u001b[1m\u001b[32m0.03483\u001b[0m\u001b[0m | time: 0.205s\n",
            "| Adam | epoch: 185 | loss: 0.03483 - acc: 0.9814 -- iter: 432/963\n",
            "Training Step: 22319  | total loss: \u001b[1m\u001b[32m0.03136\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 185 | loss: 0.03136 - acc: 0.9832 -- iter: 440/963\n",
            "Training Step: 22320  | total loss: \u001b[1m\u001b[32m0.02543\u001b[0m\u001b[0m | time: 0.218s\n",
            "| Adam | epoch: 185 | loss: 0.02543 - acc: 0.9849 -- iter: 448/963\n",
            "Training Step: 22321  | total loss: \u001b[1m\u001b[32m0.02543\u001b[0m\u001b[0m | time: 0.224s\n",
            "| Adam | epoch: 185 | loss: 0.02543 - acc: 0.9864 -- iter: 456/963\n",
            "Training Step: 22322  | total loss: \u001b[1m\u001b[32m0.02062\u001b[0m\u001b[0m | time: 0.229s\n",
            "| Adam | epoch: 185 | loss: 0.02062 - acc: 0.9878 -- iter: 464/963\n",
            "Training Step: 22323  | total loss: \u001b[1m\u001b[32m0.01858\u001b[0m\u001b[0m | time: 0.234s\n",
            "| Adam | epoch: 185 | loss: 0.01858 - acc: 0.9890 -- iter: 472/963\n",
            "Training Step: 22324  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 185 | loss: 0.01673 - acc: 0.9901 -- iter: 480/963\n",
            "Training Step: 22325  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 185 | loss: 0.01673 - acc: 0.9911 -- iter: 488/963\n",
            "Training Step: 22326  | total loss: \u001b[1m\u001b[32m0.01356\u001b[0m\u001b[0m | time: 0.254s\n",
            "| Adam | epoch: 185 | loss: 0.01356 - acc: 0.9920 -- iter: 496/963\n",
            "Training Step: 22327  | total loss: \u001b[1m\u001b[32m0.01230\u001b[0m\u001b[0m | time: 0.260s\n",
            "| Adam | epoch: 185 | loss: 0.01230 - acc: 0.9928 -- iter: 504/963\n",
            "Training Step: 22328  | total loss: \u001b[1m\u001b[32m0.01230\u001b[0m\u001b[0m | time: 0.267s\n",
            "| Adam | epoch: 185 | loss: 0.01230 - acc: 0.9935 -- iter: 512/963\n",
            "Training Step: 22329  | total loss: \u001b[1m\u001b[32m0.01109\u001b[0m\u001b[0m | time: 0.273s\n",
            "| Adam | epoch: 185 | loss: 0.01109 - acc: 0.9942 -- iter: 520/963\n",
            "Training Step: 22330  | total loss: \u001b[1m\u001b[32m0.01003\u001b[0m\u001b[0m | time: 0.280s\n",
            "| Adam | epoch: 185 | loss: 0.01003 - acc: 0.9947 -- iter: 528/963\n",
            "Training Step: 22331  | total loss: \u001b[1m\u001b[32m0.00903\u001b[0m\u001b[0m | time: 0.290s\n",
            "| Adam | epoch: 185 | loss: 0.00903 - acc: 0.9953 -- iter: 536/963\n",
            "Training Step: 22332  | total loss: \u001b[1m\u001b[32m0.00814\u001b[0m\u001b[0m | time: 0.294s\n",
            "| Adam | epoch: 185 | loss: 0.00814 - acc: 0.9957 -- iter: 544/963\n",
            "Training Step: 22333  | total loss: \u001b[1m\u001b[32m0.01520\u001b[0m\u001b[0m | time: 0.297s\n",
            "| Adam | epoch: 185 | loss: 0.01520 - acc: 0.9962 -- iter: 552/963\n",
            "Training Step: 22334  | total loss: \u001b[1m\u001b[32m0.01370\u001b[0m\u001b[0m | time: 0.304s\n",
            "| Adam | epoch: 185 | loss: 0.01370 - acc: 0.9965 -- iter: 560/963\n",
            "Training Step: 22335  | total loss: \u001b[1m\u001b[32m0.01585\u001b[0m\u001b[0m | time: 0.307s\n",
            "| Adam | epoch: 185 | loss: 0.01585 - acc: 0.9969 -- iter: 568/963\n",
            "Training Step: 22336  | total loss: \u001b[1m\u001b[32m0.02907\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 185 | loss: 0.02907 - acc: 0.9847 -- iter: 576/963\n",
            "Training Step: 22337  | total loss: \u001b[1m\u001b[32m0.02750\u001b[0m\u001b[0m | time: 0.317s\n",
            "| Adam | epoch: 185 | loss: 0.02750 - acc: 0.9876 -- iter: 584/963\n",
            "Training Step: 22338  | total loss: \u001b[1m\u001b[32m0.02475\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 185 | loss: 0.02475 - acc: 0.9876 -- iter: 592/963\n",
            "Training Step: 22339  | total loss: \u001b[1m\u001b[32m0.02229\u001b[0m\u001b[0m | time: 0.326s\n",
            "| Adam | epoch: 185 | loss: 0.02229 - acc: 0.9888 -- iter: 600/963\n",
            "Training Step: 22340  | total loss: \u001b[1m\u001b[32m0.02009\u001b[0m\u001b[0m | time: 0.332s\n",
            "| Adam | epoch: 185 | loss: 0.02009 - acc: 0.9900 -- iter: 608/963\n",
            "Training Step: 22341  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.337s\n",
            "| Adam | epoch: 185 | loss: 0.01811 - acc: 0.9910 -- iter: 616/963\n",
            "Training Step: 22342  | total loss: \u001b[1m\u001b[32m0.01631\u001b[0m\u001b[0m | time: 0.341s\n",
            "| Adam | epoch: 185 | loss: 0.01631 - acc: 0.9919 -- iter: 624/963\n",
            "Training Step: 22343  | total loss: \u001b[1m\u001b[32m0.01471\u001b[0m\u001b[0m | time: 0.347s\n",
            "| Adam | epoch: 185 | loss: 0.01471 - acc: 0.9927 -- iter: 632/963\n",
            "Training Step: 22344  | total loss: \u001b[1m\u001b[32m0.04357\u001b[0m\u001b[0m | time: 0.355s\n",
            "| Adam | epoch: 185 | loss: 0.04357 - acc: 0.9934 -- iter: 640/963\n",
            "Training Step: 22345  | total loss: \u001b[1m\u001b[32m0.04357\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 185 | loss: 0.04357 - acc: 0.9816 -- iter: 648/963\n",
            "Training Step: 22346  | total loss: \u001b[1m\u001b[32m0.03923\u001b[0m\u001b[0m | time: 0.367s\n",
            "| Adam | epoch: 185 | loss: 0.03923 - acc: 0.9834 -- iter: 656/963\n",
            "Training Step: 22347  | total loss: \u001b[1m\u001b[32m0.03182\u001b[0m\u001b[0m | time: 0.371s\n",
            "| Adam | epoch: 185 | loss: 0.03182 - acc: 0.9851 -- iter: 664/963\n",
            "Training Step: 22348  | total loss: \u001b[1m\u001b[32m0.03182\u001b[0m\u001b[0m | time: 0.377s\n",
            "| Adam | epoch: 185 | loss: 0.03182 - acc: 0.9866 -- iter: 672/963\n",
            "Training Step: 22349  | total loss: \u001b[1m\u001b[32m0.02868\u001b[0m\u001b[0m | time: 0.383s\n",
            "| Adam | epoch: 185 | loss: 0.02868 - acc: 0.9879 -- iter: 680/963\n",
            "Training Step: 22350  | total loss: \u001b[1m\u001b[32m0.02584\u001b[0m\u001b[0m | time: 0.388s\n",
            "| Adam | epoch: 185 | loss: 0.02584 - acc: 0.9891 -- iter: 688/963\n",
            "Training Step: 22351  | total loss: \u001b[1m\u001b[32m0.02328\u001b[0m\u001b[0m | time: 0.394s\n",
            "| Adam | epoch: 185 | loss: 0.02328 - acc: 0.9902 -- iter: 696/963\n",
            "Training Step: 22352  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.400s\n",
            "| Adam | epoch: 185 | loss: 0.01888 - acc: 0.9912 -- iter: 704/963\n",
            "Training Step: 22353  | total loss: \u001b[1m\u001b[32m0.01888\u001b[0m\u001b[0m | time: 0.406s\n",
            "| Adam | epoch: 185 | loss: 0.01888 - acc: 0.9921 -- iter: 712/963\n",
            "Training Step: 22354  | total loss: \u001b[1m\u001b[32m0.01701\u001b[0m\u001b[0m | time: 0.411s\n",
            "| Adam | epoch: 185 | loss: 0.01701 - acc: 0.9929 -- iter: 720/963\n",
            "Training Step: 22355  | total loss: \u001b[1m\u001b[32m0.01386\u001b[0m\u001b[0m | time: 0.416s\n",
            "| Adam | epoch: 185 | loss: 0.01386 - acc: 0.9936 -- iter: 728/963\n",
            "Training Step: 22356  | total loss: \u001b[1m\u001b[32m0.01248\u001b[0m\u001b[0m | time: 0.422s\n",
            "| Adam | epoch: 185 | loss: 0.01248 - acc: 0.9942 -- iter: 736/963\n",
            "Training Step: 22357  | total loss: \u001b[1m\u001b[32m0.01248\u001b[0m\u001b[0m | time: 0.427s\n",
            "| Adam | epoch: 185 | loss: 0.01248 - acc: 0.9948 -- iter: 744/963\n",
            "Training Step: 22358  | total loss: \u001b[1m\u001b[32m0.01127\u001b[0m\u001b[0m | time: 0.433s\n",
            "| Adam | epoch: 185 | loss: 0.01127 - acc: 0.9953 -- iter: 752/963\n",
            "Training Step: 22359  | total loss: \u001b[1m\u001b[32m0.01024\u001b[0m\u001b[0m | time: 0.438s\n",
            "| Adam | epoch: 185 | loss: 0.01024 - acc: 0.9958 -- iter: 760/963\n",
            "Training Step: 22360  | total loss: \u001b[1m\u001b[32m0.01024\u001b[0m\u001b[0m | time: 0.444s\n",
            "| Adam | epoch: 185 | loss: 0.01024 - acc: 0.9962 -- iter: 768/963\n",
            "Training Step: 22361  | total loss: \u001b[1m\u001b[32m0.00835\u001b[0m\u001b[0m | time: 0.450s\n",
            "| Adam | epoch: 185 | loss: 0.00835 - acc: 0.9966 -- iter: 776/963\n",
            "Training Step: 22362  | total loss: \u001b[1m\u001b[32m0.00756\u001b[0m\u001b[0m | time: 0.455s\n",
            "| Adam | epoch: 185 | loss: 0.00756 - acc: 0.9969 -- iter: 784/963\n",
            "Training Step: 22363  | total loss: \u001b[1m\u001b[32m0.00756\u001b[0m\u001b[0m | time: 0.461s\n",
            "| Adam | epoch: 185 | loss: 0.00756 - acc: 0.9972 -- iter: 792/963\n",
            "Training Step: 22364  | total loss: \u001b[1m\u001b[32m0.00616\u001b[0m\u001b[0m | time: 0.466s\n",
            "| Adam | epoch: 185 | loss: 0.00616 - acc: 0.9975 -- iter: 800/963\n",
            "Training Step: 22365  | total loss: \u001b[1m\u001b[32m0.00616\u001b[0m\u001b[0m | time: 0.471s\n",
            "| Adam | epoch: 185 | loss: 0.00616 - acc: 0.9978 -- iter: 808/963\n",
            "Training Step: 22366  | total loss: \u001b[1m\u001b[32m0.00505\u001b[0m\u001b[0m | time: 0.476s\n",
            "| Adam | epoch: 185 | loss: 0.00505 - acc: 0.9980 -- iter: 816/963\n",
            "Training Step: 22367  | total loss: \u001b[1m\u001b[32m0.00505\u001b[0m\u001b[0m | time: 0.481s\n",
            "| Adam | epoch: 185 | loss: 0.00505 - acc: 0.9982 -- iter: 824/963\n",
            "Training Step: 22368  | total loss: \u001b[1m\u001b[32m0.00456\u001b[0m\u001b[0m | time: 0.486s\n",
            "| Adam | epoch: 185 | loss: 0.00456 - acc: 0.9984 -- iter: 832/963\n",
            "Training Step: 22369  | total loss: \u001b[1m\u001b[32m0.00381\u001b[0m\u001b[0m | time: 0.492s\n",
            "| Adam | epoch: 185 | loss: 0.00381 - acc: 0.9985 -- iter: 840/963\n",
            "Training Step: 22370  | total loss: \u001b[1m\u001b[32m0.00346\u001b[0m\u001b[0m | time: 0.497s\n",
            "| Adam | epoch: 185 | loss: 0.00346 - acc: 0.9987 -- iter: 848/963\n",
            "Training Step: 22371  | total loss: \u001b[1m\u001b[32m0.00346\u001b[0m\u001b[0m | time: 0.503s\n",
            "| Adam | epoch: 185 | loss: 0.00346 - acc: 0.9988 -- iter: 856/963\n",
            "Training Step: 22372  | total loss: \u001b[1m\u001b[32m0.00860\u001b[0m\u001b[0m | time: 0.507s\n",
            "| Adam | epoch: 185 | loss: 0.00860 - acc: 0.9989 -- iter: 864/963\n",
            "Training Step: 22373  | total loss: \u001b[1m\u001b[32m0.00702\u001b[0m\u001b[0m | time: 0.512s\n",
            "| Adam | epoch: 185 | loss: 0.00702 - acc: 0.9990 -- iter: 872/963\n",
            "Training Step: 22374  | total loss: \u001b[1m\u001b[32m0.00635\u001b[0m\u001b[0m | time: 0.517s\n",
            "| Adam | epoch: 185 | loss: 0.00635 - acc: 0.9991 -- iter: 880/963\n",
            "Training Step: 22375  | total loss: \u001b[1m\u001b[32m0.00574\u001b[0m\u001b[0m | time: 0.520s\n",
            "| Adam | epoch: 185 | loss: 0.00574 - acc: 0.9992 -- iter: 888/963\n",
            "Training Step: 22376  | total loss: \u001b[1m\u001b[32m0.00574\u001b[0m\u001b[0m | time: 0.521s\n",
            "| Adam | epoch: 185 | loss: 0.00574 - acc: 0.9993 -- iter: 896/963\n",
            "Training Step: 22377  | total loss: \u001b[1m\u001b[32m0.04221\u001b[0m\u001b[0m | time: 0.523s\n",
            "| Adam | epoch: 185 | loss: 0.04221 - acc: 0.9869 -- iter: 904/963\n",
            "Training Step: 22378  | total loss: \u001b[1m\u001b[32m0.06551\u001b[0m\u001b[0m | time: 0.525s\n",
            "| Adam | epoch: 185 | loss: 0.06551 - acc: 0.9757 -- iter: 912/963\n",
            "Training Step: 22379  | total loss: \u001b[1m\u001b[32m0.05897\u001b[0m\u001b[0m | time: 0.528s\n",
            "| Adam | epoch: 185 | loss: 0.05897 - acc: 0.9781 -- iter: 920/963\n",
            "Training Step: 22380  | total loss: \u001b[1m\u001b[32m0.05308\u001b[0m\u001b[0m | time: 0.530s\n",
            "| Adam | epoch: 185 | loss: 0.05308 - acc: 0.9803 -- iter: 928/963\n",
            "Training Step: 22381  | total loss: \u001b[1m\u001b[32m0.04779\u001b[0m\u001b[0m | time: 0.532s\n",
            "| Adam | epoch: 185 | loss: 0.04779 - acc: 0.9823 -- iter: 936/963\n",
            "Training Step: 22382  | total loss: \u001b[1m\u001b[32m0.04318\u001b[0m\u001b[0m | time: 0.534s\n",
            "| Adam | epoch: 185 | loss: 0.04318 - acc: 0.9840 -- iter: 944/963\n",
            "Training Step: 22383  | total loss: \u001b[1m\u001b[32m0.03890\u001b[0m\u001b[0m | time: 0.536s\n",
            "| Adam | epoch: 185 | loss: 0.03890 - acc: 0.9856 -- iter: 952/963\n",
            "Training Step: 22384  | total loss: \u001b[1m\u001b[32m0.03505\u001b[0m\u001b[0m | time: 0.538s\n",
            "| Adam | epoch: 185 | loss: 0.03505 - acc: 0.9871 -- iter: 960/963\n",
            "Training Step: 22385  | total loss: \u001b[1m\u001b[32m0.03155\u001b[0m\u001b[0m | time: 0.541s\n",
            "| Adam | epoch: 185 | loss: 0.03155 - acc: 0.9884 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22386  | total loss: \u001b[1m\u001b[32m0.04736\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 186 | loss: 0.04736 - acc: 0.9770 -- iter: 008/963\n",
            "Training Step: 22387  | total loss: \u001b[1m\u001b[32m0.04563\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 186 | loss: 0.04563 - acc: 0.9793 -- iter: 016/963\n",
            "Training Step: 22388  | total loss: \u001b[1m\u001b[32m0.04107\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 186 | loss: 0.04107 - acc: 0.9814 -- iter: 024/963\n",
            "Training Step: 22389  | total loss: \u001b[1m\u001b[32m0.04511\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 186 | loss: 0.04511 - acc: 0.9833 -- iter: 032/963\n",
            "Training Step: 22390  | total loss: \u001b[1m\u001b[32m0.04062\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 186 | loss: 0.04062 - acc: 0.9849 -- iter: 040/963\n",
            "Training Step: 22391  | total loss: \u001b[1m\u001b[32m0.03658\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 186 | loss: 0.03658 - acc: 0.9864 -- iter: 048/963\n",
            "Training Step: 22392  | total loss: \u001b[1m\u001b[32m0.03295\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 186 | loss: 0.03295 - acc: 0.9878 -- iter: 056/963\n",
            "Training Step: 22393  | total loss: \u001b[1m\u001b[32m0.02968\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 186 | loss: 0.02968 - acc: 0.9890 -- iter: 064/963\n",
            "Training Step: 22394  | total loss: \u001b[1m\u001b[32m0.02672\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 186 | loss: 0.02672 - acc: 0.9901 -- iter: 072/963\n",
            "Training Step: 22395  | total loss: \u001b[1m\u001b[32m0.02408\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 186 | loss: 0.02408 - acc: 0.9911 -- iter: 080/963\n",
            "Training Step: 22396  | total loss: \u001b[1m\u001b[32m0.02174\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 186 | loss: 0.02174 - acc: 0.9920 -- iter: 088/963\n",
            "Training Step: 22397  | total loss: \u001b[1m\u001b[32m0.01966\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 186 | loss: 0.01966 - acc: 0.9928 -- iter: 096/963\n",
            "Training Step: 22398  | total loss: \u001b[1m\u001b[32m0.01770\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 186 | loss: 0.01770 - acc: 0.9935 -- iter: 104/963\n",
            "Training Step: 22399  | total loss: \u001b[1m\u001b[32m0.01596\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 186 | loss: 0.01596 - acc: 0.9942 -- iter: 112/963\n",
            "Training Step: 22400  | total loss: \u001b[1m\u001b[32m0.01439\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 186 | loss: 0.01439 - acc: 0.9947 -- iter: 120/963\n",
            "Training Step: 22401  | total loss: \u001b[1m\u001b[32m0.01296\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 186 | loss: 0.01296 - acc: 0.9953 -- iter: 128/963\n",
            "Training Step: 22402  | total loss: \u001b[1m\u001b[32m0.01169\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 186 | loss: 0.01169 - acc: 0.9957 -- iter: 136/963\n",
            "Training Step: 22403  | total loss: \u001b[1m\u001b[32m0.01053\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 186 | loss: 0.01053 - acc: 0.9962 -- iter: 144/963\n",
            "Training Step: 22404  | total loss: \u001b[1m\u001b[32m0.00949\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 186 | loss: 0.00949 - acc: 0.9966 -- iter: 152/963\n",
            "Training Step: 22405  | total loss: \u001b[1m\u001b[32m0.00856\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 186 | loss: 0.00856 - acc: 0.9969 -- iter: 160/963\n",
            "Training Step: 22406  | total loss: \u001b[1m\u001b[32m0.00772\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 186 | loss: 0.00772 - acc: 0.9972 -- iter: 168/963\n",
            "Training Step: 22407  | total loss: \u001b[1m\u001b[32m0.00699\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 186 | loss: 0.00699 - acc: 0.9975 -- iter: 176/963\n",
            "Training Step: 22408  | total loss: \u001b[1m\u001b[32m0.00631\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 186 | loss: 0.00631 - acc: 0.9977 -- iter: 184/963\n",
            "Training Step: 22409  | total loss: \u001b[1m\u001b[32m0.00570\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 186 | loss: 0.00570 - acc: 0.9980 -- iter: 192/963\n",
            "Training Step: 22410  | total loss: \u001b[1m\u001b[32m0.00657\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 186 | loss: 0.00657 - acc: 0.9982 -- iter: 200/963\n",
            "Training Step: 22411  | total loss: \u001b[1m\u001b[32m0.00595\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 186 | loss: 0.00595 - acc: 0.9984 -- iter: 208/963\n",
            "Training Step: 22412  | total loss: \u001b[1m\u001b[32m0.00540\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 186 | loss: 0.00540 - acc: 0.9985 -- iter: 216/963\n",
            "Training Step: 22413  | total loss: \u001b[1m\u001b[32m0.00489\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 186 | loss: 0.00489 - acc: 0.9987 -- iter: 224/963\n",
            "Training Step: 22414  | total loss: \u001b[1m\u001b[32m0.00443\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 186 | loss: 0.00443 - acc: 0.9988 -- iter: 232/963\n",
            "Training Step: 22415  | total loss: \u001b[1m\u001b[32m0.00400\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 186 | loss: 0.00400 - acc: 0.9989 -- iter: 240/963\n",
            "Training Step: 22416  | total loss: \u001b[1m\u001b[32m2.30618\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 186 | loss: 2.30618 - acc: 0.8990 -- iter: 248/963\n",
            "Training Step: 22417  | total loss: \u001b[1m\u001b[32m2.07569\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 186 | loss: 2.07569 - acc: 0.9091 -- iter: 256/963\n",
            "Training Step: 22418  | total loss: \u001b[1m\u001b[32m1.89829\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 186 | loss: 1.89829 - acc: 0.9057 -- iter: 264/963\n",
            "Training Step: 22419  | total loss: \u001b[1m\u001b[32m1.70847\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 186 | loss: 1.70847 - acc: 0.9151 -- iter: 272/963\n",
            "Training Step: 22420  | total loss: \u001b[1m\u001b[32m1.53764\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 186 | loss: 1.53764 - acc: 0.9236 -- iter: 280/963\n",
            "Training Step: 22421  | total loss: \u001b[1m\u001b[32m1.38389\u001b[0m\u001b[0m | time: 0.150s\n",
            "| Adam | epoch: 186 | loss: 1.38389 - acc: 0.9313 -- iter: 288/963\n",
            "Training Step: 22422  | total loss: \u001b[1m\u001b[32m1.24552\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 186 | loss: 1.24552 - acc: 0.9381 -- iter: 296/963\n",
            "Training Step: 22423  | total loss: \u001b[1m\u001b[32m1.13852\u001b[0m\u001b[0m | time: 0.157s\n",
            "| Adam | epoch: 186 | loss: 1.13852 - acc: 0.9318 -- iter: 304/963\n",
            "Training Step: 22424  | total loss: \u001b[1m\u001b[32m1.02472\u001b[0m\u001b[0m | time: 0.160s\n",
            "| Adam | epoch: 186 | loss: 1.02472 - acc: 0.9386 -- iter: 312/963\n",
            "Training Step: 22425  | total loss: \u001b[1m\u001b[32m0.92227\u001b[0m\u001b[0m | time: 0.164s\n",
            "| Adam | epoch: 186 | loss: 0.92227 - acc: 0.9448 -- iter: 320/963\n",
            "Training Step: 22426  | total loss: \u001b[1m\u001b[32m0.83006\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 186 | loss: 0.83006 - acc: 0.9503 -- iter: 328/963\n",
            "Training Step: 22427  | total loss: \u001b[1m\u001b[32m0.76085\u001b[0m\u001b[0m | time: 0.175s\n",
            "| Adam | epoch: 186 | loss: 0.76085 - acc: 0.9485 -- iter: 336/963\n",
            "Training Step: 22428  | total loss: \u001b[1m\u001b[32m0.68495\u001b[0m\u001b[0m | time: 0.181s\n",
            "| Adam | epoch: 186 | loss: 0.68495 - acc: 0.9485 -- iter: 344/963\n",
            "Training Step: 22429  | total loss: \u001b[1m\u001b[32m0.61647\u001b[0m\u001b[0m | time: 0.187s\n",
            "| Adam | epoch: 186 | loss: 0.61647 - acc: 0.9536 -- iter: 352/963\n",
            "Training Step: 22430  | total loss: \u001b[1m\u001b[32m0.55484\u001b[0m\u001b[0m | time: 0.192s\n",
            "| Adam | epoch: 186 | loss: 0.55484 - acc: 0.9583 -- iter: 360/963\n",
            "Training Step: 22431  | total loss: \u001b[1m\u001b[32m0.49937\u001b[0m\u001b[0m | time: 0.198s\n",
            "| Adam | epoch: 186 | loss: 0.49937 - acc: 0.9625 -- iter: 368/963\n",
            "Training Step: 22432  | total loss: \u001b[1m\u001b[32m0.44945\u001b[0m\u001b[0m | time: 0.204s\n",
            "| Adam | epoch: 186 | loss: 0.44945 - acc: 0.9662 -- iter: 376/963\n",
            "Training Step: 22433  | total loss: \u001b[1m\u001b[32m0.40451\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 186 | loss: 0.40451 - acc: 0.9696 -- iter: 384/963\n",
            "Training Step: 22434  | total loss: \u001b[1m\u001b[32m0.36414\u001b[0m\u001b[0m | time: 0.216s\n",
            "| Adam | epoch: 186 | loss: 0.36414 - acc: 0.9726 -- iter: 392/963\n",
            "Training Step: 22435  | total loss: \u001b[1m\u001b[32m0.32774\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 186 | loss: 0.32774 - acc: 0.9754 -- iter: 400/963\n",
            "Training Step: 22436  | total loss: \u001b[1m\u001b[32m0.29500\u001b[0m\u001b[0m | time: 0.230s\n",
            "| Adam | epoch: 186 | loss: 0.29500 - acc: 0.9778 -- iter: 408/963\n",
            "Training Step: 22437  | total loss: \u001b[1m\u001b[32m0.26551\u001b[0m\u001b[0m | time: 0.238s\n",
            "| Adam | epoch: 186 | loss: 0.26551 - acc: 0.9800 -- iter: 416/963\n",
            "Training Step: 22438  | total loss: \u001b[1m\u001b[32m0.23897\u001b[0m\u001b[0m | time: 0.241s\n",
            "| Adam | epoch: 186 | loss: 0.23897 - acc: 0.9820 -- iter: 424/963\n",
            "Training Step: 22439  | total loss: \u001b[1m\u001b[32m0.21510\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 186 | loss: 0.21510 - acc: 0.9838 -- iter: 432/963\n",
            "Training Step: 22440  | total loss: \u001b[1m\u001b[32m0.19361\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 186 | loss: 0.19361 - acc: 0.9855 -- iter: 440/963\n",
            "Training Step: 22441  | total loss: \u001b[1m\u001b[32m0.17554\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 186 | loss: 0.17554 - acc: 0.9869 -- iter: 448/963\n",
            "Training Step: 22442  | total loss: \u001b[1m\u001b[32m0.15817\u001b[0m\u001b[0m | time: 0.267s\n",
            "| Adam | epoch: 186 | loss: 0.15817 - acc: 0.9882 -- iter: 456/963\n",
            "Training Step: 22443  | total loss: \u001b[1m\u001b[32m0.14240\u001b[0m\u001b[0m | time: 0.273s\n",
            "| Adam | epoch: 186 | loss: 0.14240 - acc: 0.9894 -- iter: 464/963\n",
            "Training Step: 22444  | total loss: \u001b[1m\u001b[32m0.13645\u001b[0m\u001b[0m | time: 0.281s\n",
            "| Adam | epoch: 186 | loss: 0.13645 - acc: 0.9905 -- iter: 472/963\n",
            "Training Step: 22445  | total loss: \u001b[1m\u001b[32m0.12282\u001b[0m\u001b[0m | time: 0.286s\n",
            "| Adam | epoch: 186 | loss: 0.12282 - acc: 0.9914 -- iter: 480/963\n",
            "Training Step: 22446  | total loss: \u001b[1m\u001b[32m0.11056\u001b[0m\u001b[0m | time: 0.294s\n",
            "| Adam | epoch: 186 | loss: 0.11056 - acc: 0.9923 -- iter: 488/963\n",
            "Training Step: 22447  | total loss: \u001b[1m\u001b[32m0.09952\u001b[0m\u001b[0m | time: 0.301s\n",
            "| Adam | epoch: 186 | loss: 0.09952 - acc: 0.9930 -- iter: 496/963\n",
            "Training Step: 22448  | total loss: \u001b[1m\u001b[32m0.08063\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 186 | loss: 0.08063 - acc: 0.9937 -- iter: 504/963\n",
            "Training Step: 22449  | total loss: \u001b[1m\u001b[32m0.07258\u001b[0m\u001b[0m | time: 0.310s\n",
            "| Adam | epoch: 186 | loss: 0.07258 - acc: 0.9944 -- iter: 512/963\n",
            "Training Step: 22450  | total loss: \u001b[1m\u001b[32m0.06533\u001b[0m\u001b[0m | time: 0.317s\n",
            "| Adam | epoch: 186 | loss: 0.06533 - acc: 0.9949 -- iter: 520/963\n",
            "Training Step: 22451  | total loss: \u001b[1m\u001b[32m0.05882\u001b[0m\u001b[0m | time: 0.323s\n",
            "| Adam | epoch: 186 | loss: 0.05882 - acc: 0.9954 -- iter: 528/963\n",
            "Training Step: 22452  | total loss: \u001b[1m\u001b[32m0.05882\u001b[0m\u001b[0m | time: 0.328s\n",
            "| Adam | epoch: 186 | loss: 0.05882 - acc: 0.9959 -- iter: 536/963\n",
            "Training Step: 22453  | total loss: \u001b[1m\u001b[32m0.05296\u001b[0m\u001b[0m | time: 0.333s\n",
            "| Adam | epoch: 186 | loss: 0.05296 - acc: 0.9963 -- iter: 544/963\n",
            "Training Step: 22454  | total loss: \u001b[1m\u001b[32m0.04768\u001b[0m\u001b[0m | time: 0.338s\n",
            "| Adam | epoch: 186 | loss: 0.04768 - acc: 0.9967 -- iter: 552/963\n",
            "Training Step: 22455  | total loss: \u001b[1m\u001b[32m0.05026\u001b[0m\u001b[0m | time: 0.343s\n",
            "| Adam | epoch: 186 | loss: 0.05026 - acc: 0.9845 -- iter: 560/963\n",
            "Training Step: 22456  | total loss: \u001b[1m\u001b[32m0.04524\u001b[0m\u001b[0m | time: 0.347s\n",
            "| Adam | epoch: 186 | loss: 0.04524 - acc: 0.9861 -- iter: 568/963\n",
            "Training Step: 22457  | total loss: \u001b[1m\u001b[32m0.04075\u001b[0m\u001b[0m | time: 0.352s\n",
            "| Adam | epoch: 186 | loss: 0.04075 - acc: 0.9874 -- iter: 576/963\n",
            "Training Step: 22458  | total loss: \u001b[1m\u001b[32m0.03669\u001b[0m\u001b[0m | time: 0.359s\n",
            "| Adam | epoch: 186 | loss: 0.03669 - acc: 0.9887 -- iter: 584/963\n",
            "Training Step: 22459  | total loss: \u001b[1m\u001b[32m0.03669\u001b[0m\u001b[0m | time: 0.364s\n",
            "| Adam | epoch: 186 | loss: 0.03669 - acc: 0.9898 -- iter: 592/963\n",
            "Training Step: 22460  | total loss: \u001b[1m\u001b[32m0.02974\u001b[0m\u001b[0m | time: 0.370s\n",
            "| Adam | epoch: 186 | loss: 0.02974 - acc: 0.9909 -- iter: 600/963\n",
            "Training Step: 22461  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.376s\n",
            "| Adam | epoch: 186 | loss: 0.02693 - acc: 0.9918 -- iter: 608/963\n",
            "Training Step: 22462  | total loss: \u001b[1m\u001b[32m0.02693\u001b[0m\u001b[0m | time: 0.381s\n",
            "| Adam | epoch: 186 | loss: 0.02693 - acc: 0.9926 -- iter: 616/963\n",
            "Training Step: 22463  | total loss: \u001b[1m\u001b[32m0.02653\u001b[0m\u001b[0m | time: 0.386s\n",
            "| Adam | epoch: 186 | loss: 0.02653 - acc: 0.9933 -- iter: 624/963\n",
            "Training Step: 22464  | total loss: \u001b[1m\u001b[32m0.02213\u001b[0m\u001b[0m | time: 0.391s\n",
            "| Adam | epoch: 186 | loss: 0.02213 - acc: 0.9940 -- iter: 632/963\n",
            "Training Step: 22465  | total loss: \u001b[1m\u001b[32m0.01997\u001b[0m\u001b[0m | time: 0.397s\n",
            "| Adam | epoch: 186 | loss: 0.01997 - acc: 0.9946 -- iter: 640/963\n",
            "Training Step: 22466  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.403s\n",
            "| Adam | epoch: 186 | loss: 0.01801 - acc: 0.9951 -- iter: 648/963\n",
            "Training Step: 22467  | total loss: \u001b[1m\u001b[32m0.01622\u001b[0m\u001b[0m | time: 0.408s\n",
            "| Adam | epoch: 186 | loss: 0.01622 - acc: 0.9956 -- iter: 656/963\n",
            "Training Step: 22468  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.414s\n",
            "| Adam | epoch: 186 | loss: 0.01462 - acc: 0.9961 -- iter: 664/963\n",
            "Training Step: 22469  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.420s\n",
            "| Adam | epoch: 186 | loss: 0.01462 - acc: 0.9965 -- iter: 672/963\n",
            "Training Step: 22470  | total loss: \u001b[1m\u001b[32m0.01317\u001b[0m\u001b[0m | time: 0.426s\n",
            "| Adam | epoch: 186 | loss: 0.01317 - acc: 0.9968 -- iter: 680/963\n",
            "Training Step: 22471  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.430s\n",
            "| Adam | epoch: 186 | loss: 0.02105 - acc: 0.9846 -- iter: 688/963\n",
            "Training Step: 22472  | total loss: \u001b[1m\u001b[32m0.01903\u001b[0m\u001b[0m | time: 0.437s\n",
            "| Adam | epoch: 186 | loss: 0.01903 - acc: 0.9862 -- iter: 696/963\n",
            "Training Step: 22473  | total loss: \u001b[1m\u001b[32m0.01544\u001b[0m\u001b[0m | time: 0.441s\n",
            "| Adam | epoch: 186 | loss: 0.01544 - acc: 0.9875 -- iter: 704/963\n",
            "Training Step: 22474  | total loss: \u001b[1m\u001b[32m0.01392\u001b[0m\u001b[0m | time: 0.448s\n",
            "| Adam | epoch: 186 | loss: 0.01392 - acc: 0.9888 -- iter: 712/963\n",
            "Training Step: 22475  | total loss: \u001b[1m\u001b[32m0.01392\u001b[0m\u001b[0m | time: 0.453s\n",
            "| Adam | epoch: 186 | loss: 0.01392 - acc: 0.9899 -- iter: 720/963\n",
            "Training Step: 22476  | total loss: \u001b[1m\u001b[32m0.01254\u001b[0m\u001b[0m | time: 0.459s\n",
            "| Adam | epoch: 186 | loss: 0.01254 - acc: 0.9909 -- iter: 728/963\n",
            "Training Step: 22477  | total loss: \u001b[1m\u001b[32m0.01133\u001b[0m\u001b[0m | time: 0.465s\n",
            "| Adam | epoch: 186 | loss: 0.01133 - acc: 0.9918 -- iter: 736/963\n",
            "Training Step: 22478  | total loss: \u001b[1m\u001b[32m0.01020\u001b[0m\u001b[0m | time: 0.470s\n",
            "| Adam | epoch: 186 | loss: 0.01020 - acc: 0.9926 -- iter: 744/963\n",
            "Training Step: 22479  | total loss: \u001b[1m\u001b[32m0.00830\u001b[0m\u001b[0m | time: 0.475s\n",
            "| Adam | epoch: 186 | loss: 0.00830 - acc: 0.9934 -- iter: 752/963\n",
            "Training Step: 22480  | total loss: \u001b[1m\u001b[32m0.00830\u001b[0m\u001b[0m | time: 0.480s\n",
            "| Adam | epoch: 186 | loss: 0.00830 - acc: 0.9940 -- iter: 760/963\n",
            "Training Step: 22481  | total loss: \u001b[1m\u001b[32m0.00676\u001b[0m\u001b[0m | time: 0.486s\n",
            "| Adam | epoch: 186 | loss: 0.00676 - acc: 0.9946 -- iter: 768/963\n",
            "Training Step: 22482  | total loss: \u001b[1m\u001b[32m0.00676\u001b[0m\u001b[0m | time: 0.492s\n",
            "| Adam | epoch: 186 | loss: 0.00676 - acc: 0.9952 -- iter: 776/963\n",
            "Training Step: 22483  | total loss: \u001b[1m\u001b[32m0.00552\u001b[0m\u001b[0m | time: 0.498s\n",
            "| Adam | epoch: 186 | loss: 0.00552 - acc: 0.9957 -- iter: 784/963\n",
            "Training Step: 22484  | total loss: \u001b[1m\u001b[32m0.00498\u001b[0m\u001b[0m | time: 0.501s\n",
            "| Adam | epoch: 186 | loss: 0.00498 - acc: 0.9961 -- iter: 792/963\n",
            "Training Step: 22485  | total loss: \u001b[1m\u001b[32m0.00498\u001b[0m\u001b[0m | time: 0.504s\n",
            "| Adam | epoch: 186 | loss: 0.00498 - acc: 0.9965 -- iter: 800/963\n",
            "Training Step: 22486  | total loss: \u001b[1m\u001b[32m0.00451\u001b[0m\u001b[0m | time: 0.506s\n",
            "| Adam | epoch: 186 | loss: 0.00451 - acc: 0.9968 -- iter: 808/963\n",
            "Training Step: 22487  | total loss: \u001b[1m\u001b[32m0.00410\u001b[0m\u001b[0m | time: 0.508s\n",
            "| Adam | epoch: 186 | loss: 0.00410 - acc: 0.9972 -- iter: 816/963\n",
            "Training Step: 22488  | total loss: \u001b[1m\u001b[32m0.00425\u001b[0m\u001b[0m | time: 0.510s\n",
            "| Adam | epoch: 186 | loss: 0.00425 - acc: 0.9974 -- iter: 824/963\n",
            "Training Step: 22489  | total loss: \u001b[1m\u001b[32m0.00386\u001b[0m\u001b[0m | time: 0.513s\n",
            "| Adam | epoch: 186 | loss: 0.00386 - acc: 0.9977 -- iter: 832/963\n",
            "Training Step: 22490  | total loss: \u001b[1m\u001b[32m0.00348\u001b[0m\u001b[0m | time: 0.516s\n",
            "| Adam | epoch: 186 | loss: 0.00348 - acc: 0.9979 -- iter: 840/963\n",
            "Training Step: 22491  | total loss: \u001b[1m\u001b[32m0.00315\u001b[0m\u001b[0m | time: 0.518s\n",
            "| Adam | epoch: 186 | loss: 0.00315 - acc: 0.9981 -- iter: 848/963\n",
            "Training Step: 22492  | total loss: \u001b[1m\u001b[32m0.00286\u001b[0m\u001b[0m | time: 0.521s\n",
            "| Adam | epoch: 186 | loss: 0.00286 - acc: 0.9983 -- iter: 856/963\n",
            "Training Step: 22493  | total loss: \u001b[1m\u001b[32m0.00269\u001b[0m\u001b[0m | time: 0.524s\n",
            "| Adam | epoch: 186 | loss: 0.00269 - acc: 0.9985 -- iter: 864/963\n",
            "Training Step: 22494  | total loss: \u001b[1m\u001b[32m0.00245\u001b[0m\u001b[0m | time: 0.526s\n",
            "| Adam | epoch: 186 | loss: 0.00245 - acc: 0.9986 -- iter: 872/963\n",
            "Training Step: 22495  | total loss: \u001b[1m\u001b[32m0.00222\u001b[0m\u001b[0m | time: 0.528s\n",
            "| Adam | epoch: 186 | loss: 0.00222 - acc: 0.9988 -- iter: 880/963\n",
            "Training Step: 22496  | total loss: \u001b[1m\u001b[32m0.00201\u001b[0m\u001b[0m | time: 0.531s\n",
            "| Adam | epoch: 186 | loss: 0.00201 - acc: 0.9989 -- iter: 888/963\n",
            "Training Step: 22497  | total loss: \u001b[1m\u001b[32m0.00185\u001b[0m\u001b[0m | time: 0.534s\n",
            "| Adam | epoch: 186 | loss: 0.00185 - acc: 0.9990 -- iter: 896/963\n",
            "Training Step: 22498  | total loss: \u001b[1m\u001b[32m0.00167\u001b[0m\u001b[0m | time: 0.537s\n",
            "| Adam | epoch: 186 | loss: 0.00167 - acc: 0.9991 -- iter: 904/963\n",
            "Training Step: 22499  | total loss: \u001b[1m\u001b[32m0.00164\u001b[0m\u001b[0m | time: 0.539s\n",
            "| Adam | epoch: 186 | loss: 0.00164 - acc: 0.9992 -- iter: 912/963\n",
            "Training Step: 22500  | total loss: \u001b[1m\u001b[32m0.03494\u001b[0m\u001b[0m | time: 0.541s\n",
            "| Adam | epoch: 186 | loss: 0.03494 - acc: 0.9868 -- iter: 920/963\n",
            "Training Step: 22501  | total loss: \u001b[1m\u001b[32m0.03145\u001b[0m\u001b[0m | time: 0.544s\n",
            "| Adam | epoch: 186 | loss: 0.03145 - acc: 0.9881 -- iter: 928/963\n",
            "Training Step: 22502  | total loss: \u001b[1m\u001b[32m0.02837\u001b[0m\u001b[0m | time: 0.548s\n",
            "| Adam | epoch: 186 | loss: 0.02837 - acc: 0.9893 -- iter: 936/963\n",
            "Training Step: 22503  | total loss: \u001b[1m\u001b[32m0.02556\u001b[0m\u001b[0m | time: 0.550s\n",
            "| Adam | epoch: 186 | loss: 0.02556 - acc: 0.9904 -- iter: 944/963\n",
            "Training Step: 22504  | total loss: \u001b[1m\u001b[32m0.02304\u001b[0m\u001b[0m | time: 0.553s\n",
            "| Adam | epoch: 186 | loss: 0.02304 - acc: 0.9913 -- iter: 952/963\n",
            "Training Step: 22505  | total loss: \u001b[1m\u001b[32m0.02074\u001b[0m\u001b[0m | time: 0.556s\n",
            "| Adam | epoch: 186 | loss: 0.02074 - acc: 0.9922 -- iter: 960/963\n",
            "Training Step: 22506  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.558s\n",
            "| Adam | epoch: 186 | loss: 0.01876 - acc: 0.9930 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22507  | total loss: \u001b[1m\u001b[32m0.01691\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 187 | loss: 0.01691 - acc: 0.9937 -- iter: 008/963\n",
            "Training Step: 22508  | total loss: \u001b[1m\u001b[32m0.01523\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 187 | loss: 0.01523 - acc: 0.9943 -- iter: 016/963\n",
            "Training Step: 22509  | total loss: \u001b[1m\u001b[32m0.01374\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 187 | loss: 0.01374 - acc: 0.9949 -- iter: 024/963\n",
            "Training Step: 22510  | total loss: \u001b[1m\u001b[32m0.01238\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 187 | loss: 0.01238 - acc: 0.9954 -- iter: 032/963\n",
            "Training Step: 22511  | total loss: \u001b[1m\u001b[32m0.01126\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 187 | loss: 0.01126 - acc: 0.9959 -- iter: 040/963\n",
            "Training Step: 22512  | total loss: \u001b[1m\u001b[32m0.01018\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 187 | loss: 0.01018 - acc: 0.9963 -- iter: 048/963\n",
            "Training Step: 22513  | total loss: \u001b[1m\u001b[32m0.02550\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 187 | loss: 0.02550 - acc: 0.9841 -- iter: 056/963\n",
            "Training Step: 22514  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 187 | loss: 0.02299 - acc: 0.9857 -- iter: 064/963\n",
            "Training Step: 22515  | total loss: \u001b[1m\u001b[32m0.05307\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 187 | loss: 0.05307 - acc: 0.9747 -- iter: 072/963\n",
            "Training Step: 22516  | total loss: \u001b[1m\u001b[32m0.04777\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 187 | loss: 0.04777 - acc: 0.9772 -- iter: 080/963\n",
            "Training Step: 22517  | total loss: \u001b[1m\u001b[32m0.04303\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 187 | loss: 0.04303 - acc: 0.9795 -- iter: 088/963\n",
            "Training Step: 22518  | total loss: \u001b[1m\u001b[32m0.03875\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 187 | loss: 0.03875 - acc: 0.9815 -- iter: 096/963\n",
            "Training Step: 22519  | total loss: \u001b[1m\u001b[32m0.03498\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 187 | loss: 0.03498 - acc: 0.9834 -- iter: 104/963\n",
            "Training Step: 22520  | total loss: \u001b[1m\u001b[32m0.03240\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 187 | loss: 0.03240 - acc: 0.9850 -- iter: 112/963\n",
            "Training Step: 22521  | total loss: \u001b[1m\u001b[32m0.02919\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 187 | loss: 0.02919 - acc: 0.9865 -- iter: 120/963\n",
            "Training Step: 22522  | total loss: \u001b[1m\u001b[32m0.02631\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 187 | loss: 0.02631 - acc: 0.9879 -- iter: 128/963\n",
            "Training Step: 22523  | total loss: \u001b[1m\u001b[32m0.02369\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 187 | loss: 0.02369 - acc: 0.9891 -- iter: 136/963\n",
            "Training Step: 22524  | total loss: \u001b[1m\u001b[32m0.02134\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 187 | loss: 0.02134 - acc: 0.9902 -- iter: 144/963\n",
            "Training Step: 22525  | total loss: \u001b[1m\u001b[32m0.01923\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 187 | loss: 0.01923 - acc: 0.9912 -- iter: 152/963\n",
            "Training Step: 22526  | total loss: \u001b[1m\u001b[32m0.01737\u001b[0m\u001b[0m | time: 0.054s\n",
            "| Adam | epoch: 187 | loss: 0.01737 - acc: 0.9920 -- iter: 160/963\n",
            "Training Step: 22527  | total loss: \u001b[1m\u001b[32m0.05793\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 187 | loss: 0.05793 - acc: 0.9803 -- iter: 168/963\n",
            "Training Step: 22528  | total loss: \u001b[1m\u001b[32m0.05216\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 187 | loss: 0.05216 - acc: 0.9823 -- iter: 176/963\n",
            "Training Step: 22529  | total loss: \u001b[1m\u001b[32m0.08161\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 187 | loss: 0.08161 - acc: 0.9716 -- iter: 184/963\n",
            "Training Step: 22530  | total loss: \u001b[1m\u001b[32m0.07346\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 187 | loss: 0.07346 - acc: 0.9744 -- iter: 192/963\n",
            "Training Step: 22531  | total loss: \u001b[1m\u001b[32m0.06611\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 187 | loss: 0.06611 - acc: 0.9770 -- iter: 200/963\n",
            "Training Step: 22532  | total loss: \u001b[1m\u001b[32m0.05951\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 187 | loss: 0.05951 - acc: 0.9793 -- iter: 208/963\n",
            "Training Step: 22533  | total loss: \u001b[1m\u001b[32m0.05357\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 187 | loss: 0.05357 - acc: 0.9814 -- iter: 216/963\n",
            "Training Step: 22534  | total loss: \u001b[1m\u001b[32m0.04831\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 187 | loss: 0.04831 - acc: 0.9832 -- iter: 224/963\n",
            "Training Step: 22535  | total loss: \u001b[1m\u001b[32m0.04350\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 187 | loss: 0.04350 - acc: 0.9849 -- iter: 232/963\n",
            "Training Step: 22536  | total loss: \u001b[1m\u001b[32m0.03916\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 187 | loss: 0.03916 - acc: 0.9864 -- iter: 240/963\n",
            "Training Step: 22537  | total loss: \u001b[1m\u001b[32m0.03526\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 187 | loss: 0.03526 - acc: 0.9878 -- iter: 248/963\n",
            "Training Step: 22538  | total loss: \u001b[1m\u001b[32m0.04053\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 187 | loss: 0.04053 - acc: 0.9765 -- iter: 256/963\n",
            "Training Step: 22539  | total loss: \u001b[1m\u001b[32m0.04509\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 187 | loss: 0.04509 - acc: 0.9788 -- iter: 264/963\n",
            "Training Step: 22540  | total loss: \u001b[1m\u001b[32m0.04064\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 187 | loss: 0.04064 - acc: 0.9810 -- iter: 272/963\n",
            "Training Step: 22541  | total loss: \u001b[1m\u001b[32m0.03660\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 187 | loss: 0.03660 - acc: 0.9829 -- iter: 280/963\n",
            "Training Step: 22542  | total loss: \u001b[1m\u001b[32m0.03303\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 187 | loss: 0.03303 - acc: 0.9846 -- iter: 288/963\n",
            "Training Step: 22543  | total loss: \u001b[1m\u001b[32m0.02974\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 187 | loss: 0.02974 - acc: 0.9861 -- iter: 296/963\n",
            "Training Step: 22544  | total loss: \u001b[1m\u001b[32m0.02677\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 187 | loss: 0.02677 - acc: 0.9875 -- iter: 304/963\n",
            "Training Step: 22545  | total loss: \u001b[1m\u001b[32m0.02411\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 187 | loss: 0.02411 - acc: 0.9888 -- iter: 312/963\n",
            "Training Step: 22546  | total loss: \u001b[1m\u001b[32m0.02173\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 187 | loss: 0.02173 - acc: 0.9899 -- iter: 320/963\n",
            "Training Step: 22547  | total loss: \u001b[1m\u001b[32m0.01958\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 187 | loss: 0.01958 - acc: 0.9909 -- iter: 328/963\n",
            "Training Step: 22548  | total loss: \u001b[1m\u001b[32m0.01763\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 187 | loss: 0.01763 - acc: 0.9918 -- iter: 336/963\n",
            "Training Step: 22549  | total loss: \u001b[1m\u001b[32m0.01588\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 187 | loss: 0.01588 - acc: 0.9926 -- iter: 344/963\n",
            "Training Step: 22550  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.130s\n",
            "| Adam | epoch: 187 | loss: 0.01883 - acc: 0.9934 -- iter: 352/963\n",
            "Training Step: 22551  | total loss: \u001b[1m\u001b[32m0.01697\u001b[0m\u001b[0m | time: 0.135s\n",
            "| Adam | epoch: 187 | loss: 0.01697 - acc: 0.9940 -- iter: 360/963\n",
            "Training Step: 22552  | total loss: \u001b[1m\u001b[32m0.01530\u001b[0m\u001b[0m | time: 0.137s\n",
            "| Adam | epoch: 187 | loss: 0.01530 - acc: 0.9946 -- iter: 368/963\n",
            "Training Step: 22553  | total loss: \u001b[1m\u001b[32m0.01378\u001b[0m\u001b[0m | time: 0.143s\n",
            "| Adam | epoch: 187 | loss: 0.01378 - acc: 0.9952 -- iter: 376/963\n",
            "Training Step: 22554  | total loss: \u001b[1m\u001b[32m0.01242\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 187 | loss: 0.01242 - acc: 0.9956 -- iter: 384/963\n",
            "Training Step: 22555  | total loss: \u001b[1m\u001b[32m0.01119\u001b[0m\u001b[0m | time: 0.149s\n",
            "| Adam | epoch: 187 | loss: 0.01119 - acc: 0.9961 -- iter: 392/963\n",
            "Training Step: 22556  | total loss: \u001b[1m\u001b[32m0.03205\u001b[0m\u001b[0m | time: 0.152s\n",
            "| Adam | epoch: 187 | loss: 0.03205 - acc: 0.9840 -- iter: 400/963\n",
            "Training Step: 22557  | total loss: \u001b[1m\u001b[32m0.02886\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 187 | loss: 0.02886 - acc: 0.9856 -- iter: 408/963\n",
            "Training Step: 22558  | total loss: \u001b[1m\u001b[32m0.02722\u001b[0m\u001b[0m | time: 0.156s\n",
            "| Adam | epoch: 187 | loss: 0.02722 - acc: 0.9870 -- iter: 416/963\n",
            "Training Step: 22559  | total loss: \u001b[1m\u001b[32m0.02452\u001b[0m\u001b[0m | time: 0.160s\n",
            "| Adam | epoch: 187 | loss: 0.02452 - acc: 0.9883 -- iter: 424/963\n",
            "Training Step: 22560  | total loss: \u001b[1m\u001b[32m0.02247\u001b[0m\u001b[0m | time: 0.166s\n",
            "| Adam | epoch: 187 | loss: 0.02247 - acc: 0.9895 -- iter: 432/963\n",
            "Training Step: 22561  | total loss: \u001b[1m\u001b[32m0.02028\u001b[0m\u001b[0m | time: 0.168s\n",
            "| Adam | epoch: 187 | loss: 0.02028 - acc: 0.9905 -- iter: 440/963\n",
            "Training Step: 22562  | total loss: \u001b[1m\u001b[32m0.01826\u001b[0m\u001b[0m | time: 0.171s\n",
            "| Adam | epoch: 187 | loss: 0.01826 - acc: 0.9915 -- iter: 448/963\n",
            "Training Step: 22563  | total loss: \u001b[1m\u001b[32m0.01645\u001b[0m\u001b[0m | time: 0.175s\n",
            "| Adam | epoch: 187 | loss: 0.01645 - acc: 0.9923 -- iter: 456/963\n",
            "Training Step: 22564  | total loss: \u001b[1m\u001b[32m0.01482\u001b[0m\u001b[0m | time: 0.181s\n",
            "| Adam | epoch: 187 | loss: 0.01482 - acc: 0.9931 -- iter: 464/963\n",
            "Training Step: 22565  | total loss: \u001b[1m\u001b[32m0.01335\u001b[0m\u001b[0m | time: 0.185s\n",
            "| Adam | epoch: 187 | loss: 0.01335 - acc: 0.9938 -- iter: 472/963\n",
            "Training Step: 22566  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.188s\n",
            "| Adam | epoch: 187 | loss: 0.02325 - acc: 0.9819 -- iter: 480/963\n",
            "Training Step: 22567  | total loss: \u001b[1m\u001b[32m0.02098\u001b[0m\u001b[0m | time: 0.192s\n",
            "| Adam | epoch: 187 | loss: 0.02098 - acc: 0.9837 -- iter: 488/963\n",
            "Training Step: 22568  | total loss: \u001b[1m\u001b[32m0.01891\u001b[0m\u001b[0m | time: 0.196s\n",
            "| Adam | epoch: 187 | loss: 0.01891 - acc: 0.9853 -- iter: 496/963\n",
            "Training Step: 22569  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.200s\n",
            "| Adam | epoch: 187 | loss: 0.01721 - acc: 0.9868 -- iter: 504/963\n",
            "Training Step: 22570  | total loss: \u001b[1m\u001b[32m0.01559\u001b[0m\u001b[0m | time: 0.207s\n",
            "| Adam | epoch: 187 | loss: 0.01559 - acc: 0.9881 -- iter: 512/963\n",
            "Training Step: 22571  | total loss: \u001b[1m\u001b[32m0.01410\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 187 | loss: 0.01410 - acc: 0.9893 -- iter: 520/963\n",
            "Training Step: 22572  | total loss: \u001b[1m\u001b[32m0.01270\u001b[0m\u001b[0m | time: 0.217s\n",
            "| Adam | epoch: 187 | loss: 0.01270 - acc: 0.9904 -- iter: 528/963\n",
            "Training Step: 22573  | total loss: \u001b[1m\u001b[32m0.01147\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 187 | loss: 0.01147 - acc: 0.9913 -- iter: 536/963\n",
            "Training Step: 22574  | total loss: \u001b[1m\u001b[32m0.01033\u001b[0m\u001b[0m | time: 0.226s\n",
            "| Adam | epoch: 187 | loss: 0.01033 - acc: 0.9922 -- iter: 544/963\n",
            "Training Step: 22575  | total loss: \u001b[1m\u001b[32m0.00933\u001b[0m\u001b[0m | time: 0.230s\n",
            "| Adam | epoch: 187 | loss: 0.00933 - acc: 0.9930 -- iter: 552/963\n",
            "Training Step: 22576  | total loss: \u001b[1m\u001b[32m0.00841\u001b[0m\u001b[0m | time: 0.235s\n",
            "| Adam | epoch: 187 | loss: 0.00841 - acc: 0.9937 -- iter: 560/963\n",
            "Training Step: 22577  | total loss: \u001b[1m\u001b[32m0.00760\u001b[0m\u001b[0m | time: 0.238s\n",
            "| Adam | epoch: 187 | loss: 0.00760 - acc: 0.9943 -- iter: 568/963\n",
            "Training Step: 22578  | total loss: \u001b[1m\u001b[32m0.00702\u001b[0m\u001b[0m | time: 0.243s\n",
            "| Adam | epoch: 187 | loss: 0.00702 - acc: 0.9949 -- iter: 576/963\n",
            "Training Step: 22579  | total loss: \u001b[1m\u001b[32m0.00635\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 187 | loss: 0.00635 - acc: 0.9954 -- iter: 584/963\n",
            "Training Step: 22580  | total loss: \u001b[1m\u001b[32m0.00572\u001b[0m\u001b[0m | time: 0.254s\n",
            "| Adam | epoch: 187 | loss: 0.00572 - acc: 0.9959 -- iter: 592/963\n",
            "Training Step: 22581  | total loss: \u001b[1m\u001b[32m0.00516\u001b[0m\u001b[0m | time: 0.258s\n",
            "| Adam | epoch: 187 | loss: 0.00516 - acc: 0.9963 -- iter: 600/963\n",
            "Training Step: 22582  | total loss: \u001b[1m\u001b[32m0.04357\u001b[0m\u001b[0m | time: 0.264s\n",
            "| Adam | epoch: 187 | loss: 0.04357 - acc: 0.9841 -- iter: 608/963\n",
            "Training Step: 22583  | total loss: \u001b[1m\u001b[32m0.07247\u001b[0m\u001b[0m | time: 0.269s\n",
            "| Adam | epoch: 187 | loss: 0.07247 - acc: 0.9732 -- iter: 616/963\n",
            "Training Step: 22584  | total loss: \u001b[1m\u001b[32m0.06524\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 187 | loss: 0.06524 - acc: 0.9759 -- iter: 624/963\n",
            "Training Step: 22585  | total loss: \u001b[1m\u001b[32m0.05876\u001b[0m\u001b[0m | time: 0.276s\n",
            "| Adam | epoch: 187 | loss: 0.05876 - acc: 0.9783 -- iter: 632/963\n",
            "Training Step: 22586  | total loss: \u001b[1m\u001b[32m0.05290\u001b[0m\u001b[0m | time: 0.279s\n",
            "| Adam | epoch: 187 | loss: 0.05290 - acc: 0.9805 -- iter: 640/963\n",
            "Training Step: 22587  | total loss: \u001b[1m\u001b[32m0.04762\u001b[0m\u001b[0m | time: 0.282s\n",
            "| Adam | epoch: 187 | loss: 0.04762 - acc: 0.9824 -- iter: 648/963\n",
            "Training Step: 22588  | total loss: \u001b[1m\u001b[32m0.04288\u001b[0m\u001b[0m | time: 0.285s\n",
            "| Adam | epoch: 187 | loss: 0.04288 - acc: 0.9842 -- iter: 656/963\n",
            "Training Step: 22589  | total loss: \u001b[1m\u001b[32m0.03861\u001b[0m\u001b[0m | time: 0.290s\n",
            "| Adam | epoch: 187 | loss: 0.03861 - acc: 0.9858 -- iter: 664/963\n",
            "Training Step: 22590  | total loss: \u001b[1m\u001b[32m0.03478\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 187 | loss: 0.03478 - acc: 0.9872 -- iter: 672/963\n",
            "Training Step: 22591  | total loss: \u001b[1m\u001b[32m0.03209\u001b[0m\u001b[0m | time: 0.302s\n",
            "| Adam | epoch: 187 | loss: 0.03209 - acc: 0.9885 -- iter: 680/963\n",
            "Training Step: 22592  | total loss: \u001b[1m\u001b[32m0.05658\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 187 | loss: 0.05658 - acc: 0.9771 -- iter: 688/963\n",
            "Training Step: 22593  | total loss: \u001b[1m\u001b[32m0.07075\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 187 | loss: 0.07075 - acc: 0.9669 -- iter: 696/963\n",
            "Training Step: 22594  | total loss: \u001b[1m\u001b[32m0.07913\u001b[0m\u001b[0m | time: 0.317s\n",
            "| Adam | epoch: 187 | loss: 0.07913 - acc: 0.9577 -- iter: 704/963\n",
            "Training Step: 22595  | total loss: \u001b[1m\u001b[32m0.07122\u001b[0m\u001b[0m | time: 0.321s\n",
            "| Adam | epoch: 187 | loss: 0.07122 - acc: 0.9620 -- iter: 712/963\n",
            "Training Step: 22596  | total loss: \u001b[1m\u001b[32m0.06411\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 187 | loss: 0.06411 - acc: 0.9658 -- iter: 720/963\n",
            "Training Step: 22597  | total loss: \u001b[1m\u001b[32m0.05772\u001b[0m\u001b[0m | time: 0.328s\n",
            "| Adam | epoch: 187 | loss: 0.05772 - acc: 0.9692 -- iter: 728/963\n",
            "Training Step: 22598  | total loss: \u001b[1m\u001b[32m0.05197\u001b[0m\u001b[0m | time: 0.332s\n",
            "| Adam | epoch: 187 | loss: 0.05197 - acc: 0.9723 -- iter: 736/963\n",
            "Training Step: 22599  | total loss: \u001b[1m\u001b[32m0.04679\u001b[0m\u001b[0m | time: 0.336s\n",
            "| Adam | epoch: 187 | loss: 0.04679 - acc: 0.9750 -- iter: 744/963\n",
            "Training Step: 22600  | total loss: \u001b[1m\u001b[32m0.04212\u001b[0m\u001b[0m | time: 0.339s\n",
            "| Adam | epoch: 187 | loss: 0.04212 - acc: 0.9775 -- iter: 752/963\n",
            "Training Step: 22601  | total loss: \u001b[1m\u001b[32m0.03792\u001b[0m\u001b[0m | time: 0.343s\n",
            "| Adam | epoch: 187 | loss: 0.03792 - acc: 0.9798 -- iter: 760/963\n",
            "Training Step: 22602  | total loss: \u001b[1m\u001b[32m0.03419\u001b[0m\u001b[0m | time: 0.346s\n",
            "| Adam | epoch: 187 | loss: 0.03419 - acc: 0.9818 -- iter: 768/963\n",
            "Training Step: 22603  | total loss: \u001b[1m\u001b[32m0.03080\u001b[0m\u001b[0m | time: 0.351s\n",
            "| Adam | epoch: 187 | loss: 0.03080 - acc: 0.9836 -- iter: 776/963\n",
            "Training Step: 22604  | total loss: \u001b[1m\u001b[32m0.02776\u001b[0m\u001b[0m | time: 0.355s\n",
            "| Adam | epoch: 187 | loss: 0.02776 - acc: 0.9853 -- iter: 784/963\n",
            "Training Step: 22605  | total loss: \u001b[1m\u001b[32m0.02500\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 187 | loss: 0.02500 - acc: 0.9867 -- iter: 792/963\n",
            "Training Step: 22606  | total loss: \u001b[1m\u001b[32m0.02250\u001b[0m\u001b[0m | time: 0.363s\n",
            "| Adam | epoch: 187 | loss: 0.02250 - acc: 0.9893 -- iter: 800/963\n",
            "Training Step: 22607  | total loss: \u001b[1m\u001b[32m0.02026\u001b[0m\u001b[0m | time: 0.367s\n",
            "| Adam | epoch: 187 | loss: 0.02026 - acc: 0.9893 -- iter: 808/963\n",
            "Training Step: 22608  | total loss: \u001b[1m\u001b[32m0.01827\u001b[0m\u001b[0m | time: 0.370s\n",
            "| Adam | epoch: 187 | loss: 0.01827 - acc: 0.9903 -- iter: 816/963\n",
            "Training Step: 22609  | total loss: \u001b[1m\u001b[32m0.01646\u001b[0m\u001b[0m | time: 0.373s\n",
            "| Adam | epoch: 187 | loss: 0.01646 - acc: 0.9913 -- iter: 824/963\n",
            "Training Step: 22610  | total loss: \u001b[1m\u001b[32m0.02468\u001b[0m\u001b[0m | time: 0.379s\n",
            "| Adam | epoch: 187 | loss: 0.02468 - acc: 0.9797 -- iter: 832/963\n",
            "Training Step: 22611  | total loss: \u001b[1m\u001b[32m0.02234\u001b[0m\u001b[0m | time: 0.385s\n",
            "| Adam | epoch: 187 | loss: 0.02234 - acc: 0.9817 -- iter: 840/963\n",
            "Training Step: 22612  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.389s\n",
            "| Adam | epoch: 187 | loss: 0.02015 - acc: 0.9835 -- iter: 848/963\n",
            "Training Step: 22613  | total loss: \u001b[1m\u001b[32m0.01817\u001b[0m\u001b[0m | time: 0.394s\n",
            "| Adam | epoch: 187 | loss: 0.01817 - acc: 0.9852 -- iter: 856/963\n",
            "Training Step: 22614  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.401s\n",
            "| Adam | epoch: 187 | loss: 0.01637 - acc: 0.9867 -- iter: 864/963\n",
            "Training Step: 22615  | total loss: \u001b[1m\u001b[32m0.01476\u001b[0m\u001b[0m | time: 0.407s\n",
            "| Adam | epoch: 187 | loss: 0.01476 - acc: 0.9880 -- iter: 872/963\n",
            "Training Step: 22616  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.410s\n",
            "| Adam | epoch: 187 | loss: 0.02219 - acc: 0.9767 -- iter: 880/963\n",
            "Training Step: 22617  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.417s\n",
            "| Adam | epoch: 187 | loss: 0.02000 - acc: 0.9790 -- iter: 888/963\n",
            "Training Step: 22618  | total loss: \u001b[1m\u001b[32m0.01804\u001b[0m\u001b[0m | time: 0.421s\n",
            "| Adam | epoch: 187 | loss: 0.01804 - acc: 0.9811 -- iter: 896/963\n",
            "Training Step: 22619  | total loss: \u001b[1m\u001b[32m0.01627\u001b[0m\u001b[0m | time: 0.424s\n",
            "| Adam | epoch: 187 | loss: 0.01627 - acc: 0.9830 -- iter: 904/963\n",
            "Training Step: 22620  | total loss: \u001b[1m\u001b[32m0.01466\u001b[0m\u001b[0m | time: 0.428s\n",
            "| Adam | epoch: 187 | loss: 0.01466 - acc: 0.9847 -- iter: 912/963\n",
            "Training Step: 22621  | total loss: \u001b[1m\u001b[32m0.01323\u001b[0m\u001b[0m | time: 0.431s\n",
            "| Adam | epoch: 187 | loss: 0.01323 - acc: 0.9862 -- iter: 920/963\n",
            "Training Step: 22622  | total loss: \u001b[1m\u001b[32m0.01192\u001b[0m\u001b[0m | time: 0.433s\n",
            "| Adam | epoch: 187 | loss: 0.01192 - acc: 0.9876 -- iter: 928/963\n",
            "Training Step: 22623  | total loss: \u001b[1m\u001b[32m0.01442\u001b[0m\u001b[0m | time: 0.435s\n",
            "| Adam | epoch: 187 | loss: 0.01442 - acc: 0.9889 -- iter: 936/963\n",
            "Training Step: 22624  | total loss: \u001b[1m\u001b[32m0.01305\u001b[0m\u001b[0m | time: 0.437s\n",
            "| Adam | epoch: 187 | loss: 0.01305 - acc: 0.9900 -- iter: 944/963\n",
            "Training Step: 22625  | total loss: \u001b[1m\u001b[32m0.01178\u001b[0m\u001b[0m | time: 0.441s\n",
            "| Adam | epoch: 187 | loss: 0.01178 - acc: 0.9910 -- iter: 952/963\n",
            "Training Step: 22626  | total loss: \u001b[1m\u001b[32m0.01061\u001b[0m\u001b[0m | time: 0.445s\n",
            "| Adam | epoch: 187 | loss: 0.01061 - acc: 0.9919 -- iter: 960/963\n",
            "Training Step: 22627  | total loss: \u001b[1m\u001b[32m0.00956\u001b[0m\u001b[0m | time: 0.450s\n",
            "| Adam | epoch: 187 | loss: 0.00956 - acc: 0.9927 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22628  | total loss: \u001b[1m\u001b[32m0.00865\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 188 | loss: 0.00865 - acc: 0.9934 -- iter: 008/963\n",
            "Training Step: 22629  | total loss: \u001b[1m\u001b[32m0.00781\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 188 | loss: 0.00781 - acc: 0.9941 -- iter: 016/963\n",
            "Training Step: 22630  | total loss: \u001b[1m\u001b[32m0.00705\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 188 | loss: 0.00705 - acc: 0.9947 -- iter: 024/963\n",
            "Training Step: 22631  | total loss: \u001b[1m\u001b[32m0.04318\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 188 | loss: 0.04318 - acc: 0.9952 -- iter: 032/963\n",
            "Training Step: 22632  | total loss: \u001b[1m\u001b[32m0.04318\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 188 | loss: 0.04318 - acc: 0.9832 -- iter: 040/963\n",
            "Training Step: 22633  | total loss: \u001b[1m\u001b[32m0.03890\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 188 | loss: 0.03890 - acc: 0.9849 -- iter: 048/963\n",
            "Training Step: 22634  | total loss: \u001b[1m\u001b[32m0.04887\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 188 | loss: 0.04887 - acc: 0.9739 -- iter: 056/963\n",
            "Training Step: 22635  | total loss: \u001b[1m\u001b[32m0.05582\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 188 | loss: 0.05582 - acc: 0.9640 -- iter: 064/963\n",
            "Training Step: 22636  | total loss: \u001b[1m\u001b[32m0.05025\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 188 | loss: 0.05025 - acc: 0.9676 -- iter: 072/963\n",
            "Training Step: 22637  | total loss: \u001b[1m\u001b[32m0.04525\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 188 | loss: 0.04525 - acc: 0.9708 -- iter: 080/963\n",
            "Training Step: 22638  | total loss: \u001b[1m\u001b[32m0.04074\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 188 | loss: 0.04074 - acc: 0.9737 -- iter: 088/963\n",
            "Training Step: 22639  | total loss: \u001b[1m\u001b[32m0.03667\u001b[0m\u001b[0m | time: 0.202s\n",
            "| Adam | epoch: 188 | loss: 0.03667 - acc: 0.9764 -- iter: 096/963\n",
            "Training Step: 22640  | total loss: \u001b[1m\u001b[32m0.03301\u001b[0m\u001b[0m | time: 0.206s\n",
            "| Adam | epoch: 188 | loss: 0.03301 - acc: 0.9787 -- iter: 104/963\n",
            "Training Step: 22641  | total loss: \u001b[1m\u001b[32m0.02978\u001b[0m\u001b[0m | time: 0.208s\n",
            "| Adam | epoch: 188 | loss: 0.02978 - acc: 0.9809 -- iter: 112/963\n",
            "Training Step: 22642  | total loss: \u001b[1m\u001b[32m0.02683\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 188 | loss: 0.02683 - acc: 0.9828 -- iter: 120/963\n",
            "Training Step: 22643  | total loss: \u001b[1m\u001b[32m0.02419\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 188 | loss: 0.02419 - acc: 0.9845 -- iter: 128/963\n",
            "Training Step: 22644  | total loss: \u001b[1m\u001b[32m0.02180\u001b[0m\u001b[0m | time: 0.219s\n",
            "| Adam | epoch: 188 | loss: 0.02180 - acc: 0.9860 -- iter: 136/963\n",
            "Training Step: 22645  | total loss: \u001b[1m\u001b[32m0.01964\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 188 | loss: 0.01964 - acc: 0.9874 -- iter: 144/963\n",
            "Training Step: 22646  | total loss: \u001b[1m\u001b[32m0.01795\u001b[0m\u001b[0m | time: 0.228s\n",
            "| Adam | epoch: 188 | loss: 0.01795 - acc: 0.9887 -- iter: 152/963\n",
            "Training Step: 22647  | total loss: \u001b[1m\u001b[32m0.01617\u001b[0m\u001b[0m | time: 0.230s\n",
            "| Adam | epoch: 188 | loss: 0.01617 - acc: 0.9898 -- iter: 160/963\n",
            "Training Step: 22648  | total loss: \u001b[1m\u001b[32m0.01457\u001b[0m\u001b[0m | time: 0.235s\n",
            "| Adam | epoch: 188 | loss: 0.01457 - acc: 0.9908 -- iter: 168/963\n",
            "Training Step: 22649  | total loss: \u001b[1m\u001b[32m0.01185\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 188 | loss: 0.01185 - acc: 0.9918 -- iter: 176/963\n",
            "Training Step: 22650  | total loss: \u001b[1m\u001b[32m0.01455\u001b[0m\u001b[0m | time: 0.245s\n",
            "| Adam | epoch: 188 | loss: 0.01455 - acc: 0.9926 -- iter: 184/963\n",
            "Training Step: 22651  | total loss: \u001b[1m\u001b[32m0.01311\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 188 | loss: 0.01311 - acc: 0.9933 -- iter: 192/963\n",
            "Training Step: 22652  | total loss: \u001b[1m\u001b[32m0.01181\u001b[0m\u001b[0m | time: 0.254s\n",
            "| Adam | epoch: 188 | loss: 0.01181 - acc: 0.9940 -- iter: 200/963\n",
            "Training Step: 22653  | total loss: \u001b[1m\u001b[32m0.01068\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 188 | loss: 0.01068 - acc: 0.9946 -- iter: 208/963\n",
            "Training Step: 22654  | total loss: \u001b[1m\u001b[32m0.00964\u001b[0m\u001b[0m | time: 0.260s\n",
            "| Adam | epoch: 188 | loss: 0.00964 - acc: 0.9951 -- iter: 216/963\n",
            "Training Step: 22655  | total loss: \u001b[1m\u001b[32m0.00913\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 188 | loss: 0.00913 - acc: 0.9956 -- iter: 224/963\n",
            "Training Step: 22656  | total loss: \u001b[1m\u001b[32m0.00913\u001b[0m\u001b[0m | time: 0.269s\n",
            "| Adam | epoch: 188 | loss: 0.00913 - acc: 0.9961 -- iter: 232/963\n",
            "Training Step: 22657  | total loss: \u001b[1m\u001b[32m0.00948\u001b[0m\u001b[0m | time: 0.273s\n",
            "| Adam | epoch: 188 | loss: 0.00948 - acc: 0.9965 -- iter: 240/963\n",
            "Training Step: 22658  | total loss: \u001b[1m\u001b[32m0.00857\u001b[0m\u001b[0m | time: 0.278s\n",
            "| Adam | epoch: 188 | loss: 0.00857 - acc: 0.9968 -- iter: 248/963\n",
            "Training Step: 22659  | total loss: \u001b[1m\u001b[32m0.00772\u001b[0m\u001b[0m | time: 0.285s\n",
            "| Adam | epoch: 188 | loss: 0.00772 - acc: 0.9971 -- iter: 256/963\n",
            "Training Step: 22660  | total loss: \u001b[1m\u001b[32m2.30954\u001b[0m\u001b[0m | time: 0.292s\n",
            "| Adam | epoch: 188 | loss: 2.30954 - acc: 0.8974 -- iter: 264/963\n",
            "Training Step: 22661  | total loss: \u001b[1m\u001b[32m1.87075\u001b[0m\u001b[0m | time: 0.297s\n",
            "| Adam | epoch: 188 | loss: 1.87075 - acc: 0.9077 -- iter: 272/963\n",
            "Training Step: 22662  | total loss: \u001b[1m\u001b[32m1.87075\u001b[0m\u001b[0m | time: 0.300s\n",
            "| Adam | epoch: 188 | loss: 1.87075 - acc: 0.9169 -- iter: 280/963\n",
            "Training Step: 22663  | total loss: \u001b[1m\u001b[32m1.70926\u001b[0m\u001b[0m | time: 0.305s\n",
            "| Adam | epoch: 188 | loss: 1.70926 - acc: 0.9127 -- iter: 288/963\n",
            "Training Step: 22664  | total loss: \u001b[1m\u001b[32m1.38487\u001b[0m\u001b[0m | time: 0.314s\n",
            "| Adam | epoch: 188 | loss: 1.38487 - acc: 0.9214 -- iter: 296/963\n",
            "Training Step: 22665  | total loss: \u001b[1m\u001b[32m1.38487\u001b[0m\u001b[0m | time: 0.318s\n",
            "| Adam | epoch: 188 | loss: 1.38487 - acc: 0.9293 -- iter: 304/963\n",
            "Training Step: 22666  | total loss: \u001b[1m\u001b[32m1.24639\u001b[0m\u001b[0m | time: 0.321s\n",
            "| Adam | epoch: 188 | loss: 1.24639 - acc: 0.9364 -- iter: 312/963\n",
            "Training Step: 22667  | total loss: \u001b[1m\u001b[32m1.12176\u001b[0m\u001b[0m | time: 0.323s\n",
            "| Adam | epoch: 188 | loss: 1.12176 - acc: 0.9427 -- iter: 320/963\n",
            "Training Step: 22668  | total loss: \u001b[1m\u001b[32m1.00964\u001b[0m\u001b[0m | time: 0.328s\n",
            "| Adam | epoch: 188 | loss: 1.00964 - acc: 0.9485 -- iter: 328/963\n",
            "Training Step: 22669  | total loss: \u001b[1m\u001b[32m0.90869\u001b[0m\u001b[0m | time: 0.333s\n",
            "| Adam | epoch: 188 | loss: 0.90869 - acc: 0.9536 -- iter: 336/963\n",
            "Training Step: 22670  | total loss: \u001b[1m\u001b[32m0.81785\u001b[0m\u001b[0m | time: 0.338s\n",
            "| Adam | epoch: 188 | loss: 0.81785 - acc: 0.9583 -- iter: 344/963\n",
            "Training Step: 22671  | total loss: \u001b[1m\u001b[32m0.73608\u001b[0m\u001b[0m | time: 0.343s\n",
            "| Adam | epoch: 188 | loss: 0.73608 - acc: 0.9624 -- iter: 352/963\n",
            "Training Step: 22672  | total loss: \u001b[1m\u001b[32m0.66251\u001b[0m\u001b[0m | time: 0.345s\n",
            "| Adam | epoch: 188 | loss: 0.66251 - acc: 0.9662 -- iter: 360/963\n",
            "Training Step: 22673  | total loss: \u001b[1m\u001b[32m0.59629\u001b[0m\u001b[0m | time: 0.346s\n",
            "| Adam | epoch: 188 | loss: 0.59629 - acc: 0.9696 -- iter: 368/963\n",
            "Training Step: 22674  | total loss: \u001b[1m\u001b[32m0.53667\u001b[0m\u001b[0m | time: 0.350s\n",
            "| Adam | epoch: 188 | loss: 0.53667 - acc: 0.9726 -- iter: 376/963\n",
            "Training Step: 22675  | total loss: \u001b[1m\u001b[32m0.48323\u001b[0m\u001b[0m | time: 0.352s\n",
            "| Adam | epoch: 188 | loss: 0.48323 - acc: 0.9753 -- iter: 384/963\n",
            "Training Step: 22676  | total loss: \u001b[1m\u001b[32m0.44208\u001b[0m\u001b[0m | time: 0.354s\n",
            "| Adam | epoch: 188 | loss: 0.44208 - acc: 0.9778 -- iter: 392/963\n",
            "Training Step: 22677  | total loss: \u001b[1m\u001b[32m0.40437\u001b[0m\u001b[0m | time: 0.356s\n",
            "| Adam | epoch: 188 | loss: 0.40437 - acc: 0.9800 -- iter: 400/963\n",
            "Training Step: 22678  | total loss: \u001b[1m\u001b[32m0.36398\u001b[0m\u001b[0m | time: 0.360s\n",
            "| Adam | epoch: 188 | loss: 0.36398 - acc: 0.9820 -- iter: 408/963\n",
            "Training Step: 22679  | total loss: \u001b[1m\u001b[32m0.32759\u001b[0m\u001b[0m | time: 0.362s\n",
            "| Adam | epoch: 188 | loss: 0.32759 - acc: 0.9838 -- iter: 416/963\n",
            "Training Step: 22680  | total loss: \u001b[1m\u001b[32m0.29485\u001b[0m\u001b[0m | time: 0.365s\n",
            "| Adam | epoch: 188 | loss: 0.29485 - acc: 0.9854 -- iter: 424/963\n",
            "Training Step: 22681  | total loss: \u001b[1m\u001b[32m0.26541\u001b[0m\u001b[0m | time: 0.367s\n",
            "| Adam | epoch: 188 | loss: 0.26541 - acc: 0.9869 -- iter: 432/963\n",
            "Training Step: 22682  | total loss: \u001b[1m\u001b[32m0.23888\u001b[0m\u001b[0m | time: 0.372s\n",
            "| Adam | epoch: 188 | loss: 0.23888 - acc: 0.9882 -- iter: 440/963\n",
            "Training Step: 22683  | total loss: \u001b[1m\u001b[32m0.21502\u001b[0m\u001b[0m | time: 0.378s\n",
            "| Adam | epoch: 188 | loss: 0.21502 - acc: 0.9894 -- iter: 448/963\n",
            "Training Step: 22684  | total loss: \u001b[1m\u001b[32m0.19353\u001b[0m\u001b[0m | time: 0.387s\n",
            "| Adam | epoch: 188 | loss: 0.19353 - acc: 0.9904 -- iter: 456/963\n",
            "Training Step: 22685  | total loss: \u001b[1m\u001b[32m0.17421\u001b[0m\u001b[0m | time: 0.395s\n",
            "| Adam | epoch: 188 | loss: 0.17421 - acc: 0.9914 -- iter: 464/963\n",
            "Training Step: 22686  | total loss: \u001b[1m\u001b[32m0.15680\u001b[0m\u001b[0m | time: 0.404s\n",
            "| Adam | epoch: 188 | loss: 0.15680 - acc: 0.9923 -- iter: 472/963\n",
            "Training Step: 22687  | total loss: \u001b[1m\u001b[32m0.14113\u001b[0m\u001b[0m | time: 0.413s\n",
            "| Adam | epoch: 188 | loss: 0.14113 - acc: 0.9930 -- iter: 480/963\n",
            "Training Step: 22688  | total loss: \u001b[1m\u001b[32m0.13968\u001b[0m\u001b[0m | time: 0.418s\n",
            "| Adam | epoch: 188 | loss: 0.13968 - acc: 0.9937 -- iter: 488/963\n",
            "Training Step: 22689  | total loss: \u001b[1m\u001b[32m0.13968\u001b[0m\u001b[0m | time: 0.423s\n",
            "| Adam | epoch: 188 | loss: 0.13968 - acc: 0.9819 -- iter: 496/963\n",
            "Training Step: 22690  | total loss: \u001b[1m\u001b[32m0.12572\u001b[0m\u001b[0m | time: 0.429s\n",
            "| Adam | epoch: 188 | loss: 0.12572 - acc: 0.9837 -- iter: 504/963\n",
            "Training Step: 22691  | total loss: \u001b[1m\u001b[32m0.11317\u001b[0m\u001b[0m | time: 0.436s\n",
            "| Adam | epoch: 188 | loss: 0.11317 - acc: 0.9853 -- iter: 512/963\n",
            "Training Step: 22692  | total loss: \u001b[1m\u001b[32m0.10189\u001b[0m\u001b[0m | time: 0.443s\n",
            "| Adam | epoch: 188 | loss: 0.10189 - acc: 0.9868 -- iter: 520/963\n",
            "Training Step: 22693  | total loss: \u001b[1m\u001b[32m0.09174\u001b[0m\u001b[0m | time: 0.451s\n",
            "| Adam | epoch: 188 | loss: 0.09174 - acc: 0.9881 -- iter: 528/963\n",
            "Training Step: 22694  | total loss: \u001b[1m\u001b[32m0.08260\u001b[0m\u001b[0m | time: 0.459s\n",
            "| Adam | epoch: 188 | loss: 0.08260 - acc: 0.9893 -- iter: 536/963\n",
            "Training Step: 22695  | total loss: \u001b[1m\u001b[32m0.06694\u001b[0m\u001b[0m | time: 0.466s\n",
            "| Adam | epoch: 188 | loss: 0.06694 - acc: 0.9904 -- iter: 544/963\n",
            "Training Step: 22696  | total loss: \u001b[1m\u001b[32m0.06027\u001b[0m\u001b[0m | time: 0.470s\n",
            "| Adam | epoch: 188 | loss: 0.06027 - acc: 0.9913 -- iter: 552/963\n",
            "Training Step: 22697  | total loss: \u001b[1m\u001b[32m0.06027\u001b[0m\u001b[0m | time: 0.475s\n",
            "| Adam | epoch: 188 | loss: 0.06027 - acc: 0.9922 -- iter: 560/963\n",
            "Training Step: 22698  | total loss: \u001b[1m\u001b[32m0.05428\u001b[0m\u001b[0m | time: 0.481s\n",
            "| Adam | epoch: 188 | loss: 0.05428 - acc: 0.9930 -- iter: 568/963\n",
            "Training Step: 22699  | total loss: \u001b[1m\u001b[32m0.04887\u001b[0m\u001b[0m | time: 0.487s\n",
            "| Adam | epoch: 188 | loss: 0.04887 - acc: 0.9937 -- iter: 576/963\n",
            "Training Step: 22700  | total loss: \u001b[1m\u001b[32m0.03962\u001b[0m\u001b[0m | time: 0.493s\n",
            "| Adam | epoch: 188 | loss: 0.03962 - acc: 0.9943 -- iter: 584/963\n",
            "Training Step: 22701  | total loss: \u001b[1m\u001b[32m0.03962\u001b[0m\u001b[0m | time: 0.499s\n",
            "| Adam | epoch: 188 | loss: 0.03962 - acc: 0.9949 -- iter: 592/963\n",
            "Training Step: 22702  | total loss: \u001b[1m\u001b[32m0.06858\u001b[0m\u001b[0m | time: 0.505s\n",
            "| Adam | epoch: 188 | loss: 0.06858 - acc: 0.9829 -- iter: 600/963\n",
            "Training Step: 22703  | total loss: \u001b[1m\u001b[32m0.06858\u001b[0m\u001b[0m | time: 0.512s\n",
            "| Adam | epoch: 188 | loss: 0.06858 - acc: 0.9846 -- iter: 608/963\n",
            "Training Step: 22704  | total loss: \u001b[1m\u001b[32m0.06177\u001b[0m\u001b[0m | time: 0.517s\n",
            "| Adam | epoch: 188 | loss: 0.06177 - acc: 0.9861 -- iter: 616/963\n",
            "Training Step: 22705  | total loss: \u001b[1m\u001b[32m0.05561\u001b[0m\u001b[0m | time: 0.524s\n",
            "| Adam | epoch: 188 | loss: 0.05561 - acc: 0.9875 -- iter: 624/963\n",
            "Training Step: 22706  | total loss: \u001b[1m\u001b[32m0.05008\u001b[0m\u001b[0m | time: 0.529s\n",
            "| Adam | epoch: 188 | loss: 0.05008 - acc: 0.9888 -- iter: 632/963\n",
            "Training Step: 22707  | total loss: \u001b[1m\u001b[32m0.04509\u001b[0m\u001b[0m | time: 0.535s\n",
            "| Adam | epoch: 188 | loss: 0.04509 - acc: 0.9899 -- iter: 640/963\n",
            "Training Step: 22708  | total loss: \u001b[1m\u001b[32m0.04059\u001b[0m\u001b[0m | time: 0.539s\n",
            "| Adam | epoch: 188 | loss: 0.04059 - acc: 0.9909 -- iter: 648/963\n",
            "Training Step: 22709  | total loss: \u001b[1m\u001b[32m0.03655\u001b[0m\u001b[0m | time: 0.544s\n",
            "| Adam | epoch: 188 | loss: 0.03655 - acc: 0.9918 -- iter: 656/963\n",
            "Training Step: 22710  | total loss: \u001b[1m\u001b[32m0.03293\u001b[0m\u001b[0m | time: 0.550s\n",
            "| Adam | epoch: 188 | loss: 0.03293 - acc: 0.9926 -- iter: 664/963\n",
            "Training Step: 22711  | total loss: \u001b[1m\u001b[32m0.07002\u001b[0m\u001b[0m | time: 0.555s\n",
            "| Adam | epoch: 188 | loss: 0.07002 - acc: 0.9809 -- iter: 672/963\n",
            "Training Step: 22712  | total loss: \u001b[1m\u001b[32m0.07002\u001b[0m\u001b[0m | time: 0.560s\n",
            "| Adam | epoch: 188 | loss: 0.07002 - acc: 0.9828 -- iter: 680/963\n",
            "Training Step: 22713  | total loss: \u001b[1m\u001b[32m0.10066\u001b[0m\u001b[0m | time: 0.565s\n",
            "| Adam | epoch: 188 | loss: 0.10066 - acc: 0.9720 -- iter: 688/963\n",
            "Training Step: 22714  | total loss: \u001b[1m\u001b[32m0.08160\u001b[0m\u001b[0m | time: 0.572s\n",
            "| Adam | epoch: 188 | loss: 0.08160 - acc: 0.9748 -- iter: 696/963\n",
            "Training Step: 22715  | total loss: \u001b[1m\u001b[32m0.08160\u001b[0m\u001b[0m | time: 0.578s\n",
            "| Adam | epoch: 188 | loss: 0.08160 - acc: 0.9773 -- iter: 704/963\n",
            "Training Step: 22716  | total loss: \u001b[1m\u001b[32m0.07346\u001b[0m\u001b[0m | time: 0.585s\n",
            "| Adam | epoch: 188 | loss: 0.07346 - acc: 0.9796 -- iter: 712/963\n",
            "Training Step: 22717  | total loss: \u001b[1m\u001b[32m0.06638\u001b[0m\u001b[0m | time: 0.591s\n",
            "| Adam | epoch: 188 | loss: 0.06638 - acc: 0.9816 -- iter: 720/963\n",
            "Training Step: 22718  | total loss: \u001b[1m\u001b[32m0.05974\u001b[0m\u001b[0m | time: 0.595s\n",
            "| Adam | epoch: 188 | loss: 0.05974 - acc: 0.9835 -- iter: 728/963\n",
            "Training Step: 22719  | total loss: \u001b[1m\u001b[32m0.05380\u001b[0m\u001b[0m | time: 0.601s\n",
            "| Adam | epoch: 188 | loss: 0.05380 - acc: 0.9851 -- iter: 736/963\n",
            "Training Step: 22720  | total loss: \u001b[1m\u001b[32m0.04362\u001b[0m\u001b[0m | time: 0.607s\n",
            "| Adam | epoch: 188 | loss: 0.04362 - acc: 0.9866 -- iter: 744/963\n",
            "Training Step: 22721  | total loss: \u001b[1m\u001b[32m0.03926\u001b[0m\u001b[0m | time: 0.612s\n",
            "| Adam | epoch: 188 | loss: 0.03926 - acc: 0.9879 -- iter: 752/963\n",
            "Training Step: 22722  | total loss: \u001b[1m\u001b[32m0.03535\u001b[0m\u001b[0m | time: 0.618s\n",
            "| Adam | epoch: 188 | loss: 0.03535 - acc: 0.9892 -- iter: 760/963\n",
            "Training Step: 22723  | total loss: \u001b[1m\u001b[32m0.03184\u001b[0m\u001b[0m | time: 0.624s\n",
            "| Adam | epoch: 188 | loss: 0.03184 - acc: 0.9902 -- iter: 768/963\n",
            "Training Step: 22724  | total loss: \u001b[1m\u001b[32m0.03184\u001b[0m\u001b[0m | time: 0.631s\n",
            "| Adam | epoch: 188 | loss: 0.03184 - acc: 0.9912 -- iter: 776/963\n",
            "Training Step: 22725  | total loss: \u001b[1m\u001b[32m0.02867\u001b[0m\u001b[0m | time: 0.639s\n",
            "| Adam | epoch: 188 | loss: 0.02867 - acc: 0.9921 -- iter: 784/963\n",
            "Training Step: 22726  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 0.645s\n",
            "| Adam | epoch: 188 | loss: 0.02334 - acc: 0.9929 -- iter: 792/963\n",
            "Training Step: 22727  | total loss: \u001b[1m\u001b[32m0.02334\u001b[0m\u001b[0m | time: 0.649s\n",
            "| Adam | epoch: 188 | loss: 0.02334 - acc: 0.9936 -- iter: 800/963\n",
            "Training Step: 22728  | total loss: \u001b[1m\u001b[32m0.02417\u001b[0m\u001b[0m | time: 0.654s\n",
            "| Adam | epoch: 188 | loss: 0.02417 - acc: 0.9942 -- iter: 808/963\n",
            "Training Step: 22729  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.659s\n",
            "| Adam | epoch: 188 | loss: 0.02177 - acc: 0.9948 -- iter: 816/963\n",
            "Training Step: 22730  | total loss: \u001b[1m\u001b[32m0.03288\u001b[0m\u001b[0m | time: 0.664s\n",
            "| Adam | epoch: 188 | loss: 0.03288 - acc: 0.9953 -- iter: 824/963\n",
            "Training Step: 22731  | total loss: \u001b[1m\u001b[32m0.03288\u001b[0m\u001b[0m | time: 0.670s\n",
            "| Adam | epoch: 188 | loss: 0.03288 - acc: 0.9833 -- iter: 832/963\n",
            "Training Step: 22732  | total loss: \u001b[1m\u001b[32m0.02960\u001b[0m\u001b[0m | time: 0.678s\n",
            "| Adam | epoch: 188 | loss: 0.02960 - acc: 0.9850 -- iter: 840/963\n",
            "Training Step: 22733  | total loss: \u001b[1m\u001b[32m0.02401\u001b[0m\u001b[0m | time: 0.682s\n",
            "| Adam | epoch: 188 | loss: 0.02401 - acc: 0.9865 -- iter: 848/963\n",
            "Training Step: 22734  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.688s\n",
            "| Adam | epoch: 188 | loss: 0.02162 - acc: 0.9878 -- iter: 856/963\n",
            "Training Step: 22735  | total loss: \u001b[1m\u001b[32m0.02162\u001b[0m\u001b[0m | time: 0.690s\n",
            "| Adam | epoch: 188 | loss: 0.02162 - acc: 0.9890 -- iter: 864/963\n",
            "Training Step: 22736  | total loss: \u001b[1m\u001b[32m0.04744\u001b[0m\u001b[0m | time: 0.695s\n",
            "| Adam | epoch: 188 | loss: 0.04744 - acc: 0.9776 -- iter: 872/963\n",
            "Training Step: 22737  | total loss: \u001b[1m\u001b[32m0.03846\u001b[0m\u001b[0m | time: 0.702s\n",
            "| Adam | epoch: 188 | loss: 0.03846 - acc: 0.9799 -- iter: 880/963\n",
            "Training Step: 22738  | total loss: \u001b[1m\u001b[32m0.03846\u001b[0m\u001b[0m | time: 0.706s\n",
            "| Adam | epoch: 188 | loss: 0.03846 - acc: 0.9819 -- iter: 888/963\n",
            "Training Step: 22739  | total loss: \u001b[1m\u001b[32m0.03465\u001b[0m\u001b[0m | time: 0.711s\n",
            "| Adam | epoch: 188 | loss: 0.03465 - acc: 0.9837 -- iter: 896/963\n",
            "Training Step: 22740  | total loss: \u001b[1m\u001b[32m0.02819\u001b[0m\u001b[0m | time: 0.715s\n",
            "| Adam | epoch: 188 | loss: 0.02819 - acc: 0.9853 -- iter: 904/963\n",
            "Training Step: 22741  | total loss: \u001b[1m\u001b[32m0.02541\u001b[0m\u001b[0m | time: 0.722s\n",
            "| Adam | epoch: 188 | loss: 0.02541 - acc: 0.9868 -- iter: 912/963\n",
            "Training Step: 22742  | total loss: \u001b[1m\u001b[32m0.02288\u001b[0m\u001b[0m | time: 0.728s\n",
            "| Adam | epoch: 188 | loss: 0.02288 - acc: 0.9881 -- iter: 920/963\n",
            "Training Step: 22743  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.731s\n",
            "| Adam | epoch: 188 | loss: 0.02060 - acc: 0.9893 -- iter: 928/963\n",
            "Training Step: 22744  | total loss: \u001b[1m\u001b[32m0.01857\u001b[0m\u001b[0m | time: 0.737s\n",
            "| Adam | epoch: 188 | loss: 0.01857 - acc: 0.9904 -- iter: 936/963\n",
            "Training Step: 22745  | total loss: \u001b[1m\u001b[32m0.01672\u001b[0m\u001b[0m | time: 0.742s\n",
            "| Adam | epoch: 188 | loss: 0.01672 - acc: 0.9913 -- iter: 944/963\n",
            "Training Step: 22746  | total loss: \u001b[1m\u001b[32m0.01506\u001b[0m\u001b[0m | time: 0.747s\n",
            "| Adam | epoch: 188 | loss: 0.01506 - acc: 0.9922 -- iter: 952/963\n",
            "Training Step: 22747  | total loss: \u001b[1m\u001b[32m0.01506\u001b[0m\u001b[0m | time: 0.752s\n",
            "| Adam | epoch: 188 | loss: 0.01506 - acc: 0.9930 -- iter: 960/963\n",
            "Training Step: 22748  | total loss: \u001b[1m\u001b[32m0.01357\u001b[0m\u001b[0m | time: 0.758s\n",
            "| Adam | epoch: 188 | loss: 0.01357 - acc: 0.9937 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22749  | total loss: \u001b[1m\u001b[32m0.01223\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 189 | loss: 0.01223 - acc: 0.9943 -- iter: 008/963\n",
            "Training Step: 22750  | total loss: \u001b[1m\u001b[32m0.01104\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 189 | loss: 0.01104 - acc: 0.9949 -- iter: 016/963\n",
            "Training Step: 22751  | total loss: \u001b[1m\u001b[32m0.00995\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 189 | loss: 0.00995 - acc: 0.9954 -- iter: 024/963\n",
            "Training Step: 22752  | total loss: \u001b[1m\u001b[32m0.00813\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 189 | loss: 0.00813 - acc: 0.9959 -- iter: 032/963\n",
            "Training Step: 22753  | total loss: \u001b[1m\u001b[32m0.00734\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 189 | loss: 0.00734 - acc: 0.9963 -- iter: 040/963\n",
            "Training Step: 22754  | total loss: \u001b[1m\u001b[32m0.00664\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 189 | loss: 0.00664 - acc: 0.9966 -- iter: 048/963\n",
            "Training Step: 22755  | total loss: \u001b[1m\u001b[32m0.00664\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 189 | loss: 0.00664 - acc: 0.9970 -- iter: 056/963\n",
            "Training Step: 22756  | total loss: \u001b[1m\u001b[32m0.00549\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 189 | loss: 0.00549 - acc: 0.9973 -- iter: 064/963\n",
            "Training Step: 22757  | total loss: \u001b[1m\u001b[32m0.00496\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 189 | loss: 0.00496 - acc: 0.9976 -- iter: 072/963\n",
            "Training Step: 22758  | total loss: \u001b[1m\u001b[32m0.00448\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 189 | loss: 0.00448 - acc: 0.9978 -- iter: 080/963\n",
            "Training Step: 22759  | total loss: \u001b[1m\u001b[32m0.00448\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 189 | loss: 0.00448 - acc: 0.9980 -- iter: 088/963\n",
            "Training Step: 22760  | total loss: \u001b[1m\u001b[32m0.01254\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 189 | loss: 0.01254 - acc: 0.9982 -- iter: 096/963\n",
            "Training Step: 22761  | total loss: \u001b[1m\u001b[32m0.01130\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 189 | loss: 0.01130 - acc: 0.9984 -- iter: 104/963\n",
            "Training Step: 22762  | total loss: \u001b[1m\u001b[32m0.01021\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 189 | loss: 0.01021 - acc: 0.9986 -- iter: 112/963\n",
            "Training Step: 22763  | total loss: \u001b[1m\u001b[32m0.00922\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 189 | loss: 0.00922 - acc: 0.9987 -- iter: 120/963\n",
            "Training Step: 22764  | total loss: \u001b[1m\u001b[32m0.00830\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 189 | loss: 0.00830 - acc: 0.9988 -- iter: 128/963\n",
            "Training Step: 22765  | total loss: \u001b[1m\u001b[32m0.00748\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 189 | loss: 0.00748 - acc: 0.9989 -- iter: 136/963\n",
            "Training Step: 22766  | total loss: \u001b[1m\u001b[32m0.00620\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 189 | loss: 0.00620 - acc: 0.9991 -- iter: 144/963\n",
            "Training Step: 22767  | total loss: \u001b[1m\u001b[32m0.00562\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 189 | loss: 0.00562 - acc: 0.9991 -- iter: 152/963\n",
            "Training Step: 22768  | total loss: \u001b[1m\u001b[32m0.00562\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 189 | loss: 0.00562 - acc: 0.9992 -- iter: 160/963\n",
            "Training Step: 22769  | total loss: \u001b[1m\u001b[32m0.02962\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 189 | loss: 0.02962 - acc: 0.9868 -- iter: 168/963\n",
            "Training Step: 22770  | total loss: \u001b[1m\u001b[32m0.02672\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 189 | loss: 0.02672 - acc: 0.9881 -- iter: 176/963\n",
            "Training Step: 22771  | total loss: \u001b[1m\u001b[32m0.02407\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 189 | loss: 0.02407 - acc: 0.9893 -- iter: 184/963\n",
            "Training Step: 22772  | total loss: \u001b[1m\u001b[32m0.02177\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 189 | loss: 0.02177 - acc: 0.9904 -- iter: 192/963\n",
            "Training Step: 22773  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 0.132s\n",
            "| Adam | epoch: 189 | loss: 0.01766 - acc: 0.9913 -- iter: 200/963\n",
            "Training Step: 22774  | total loss: \u001b[1m\u001b[32m0.01766\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 189 | loss: 0.01766 - acc: 0.9922 -- iter: 208/963\n",
            "Training Step: 22775  | total loss: \u001b[1m\u001b[32m0.01591\u001b[0m\u001b[0m | time: 0.142s\n",
            "| Adam | epoch: 189 | loss: 0.01591 - acc: 0.9930 -- iter: 216/963\n",
            "Training Step: 22776  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.146s\n",
            "| Adam | epoch: 189 | loss: 0.01731 - acc: 0.9937 -- iter: 224/963\n",
            "Training Step: 22777  | total loss: \u001b[1m\u001b[32m0.01560\u001b[0m\u001b[0m | time: 0.148s\n",
            "| Adam | epoch: 189 | loss: 0.01560 - acc: 0.9943 -- iter: 232/963\n",
            "Training Step: 22778  | total loss: \u001b[1m\u001b[32m0.01567\u001b[0m\u001b[0m | time: 0.149s\n",
            "| Adam | epoch: 189 | loss: 0.01567 - acc: 0.9949 -- iter: 240/963\n",
            "Training Step: 22779  | total loss: \u001b[1m\u001b[32m0.01597\u001b[0m\u001b[0m | time: 0.152s\n",
            "| Adam | epoch: 189 | loss: 0.01597 - acc: 0.9954 -- iter: 248/963\n",
            "Training Step: 22780  | total loss: \u001b[1m\u001b[32m0.01469\u001b[0m\u001b[0m | time: 0.155s\n",
            "| Adam | epoch: 189 | loss: 0.01469 - acc: 0.9959 -- iter: 256/963\n",
            "Training Step: 22781  | total loss: \u001b[1m\u001b[32m0.01325\u001b[0m\u001b[0m | time: 0.157s\n",
            "| Adam | epoch: 189 | loss: 0.01325 - acc: 0.9963 -- iter: 264/963\n",
            "Training Step: 22782  | total loss: \u001b[1m\u001b[32m0.01197\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 189 | loss: 0.01197 - acc: 0.9966 -- iter: 272/963\n",
            "Training Step: 22783  | total loss: \u001b[1m\u001b[32m0.01078\u001b[0m\u001b[0m | time: 0.161s\n",
            "| Adam | epoch: 189 | loss: 0.01078 - acc: 0.9970 -- iter: 280/963\n",
            "Training Step: 22784  | total loss: \u001b[1m\u001b[32m0.00972\u001b[0m\u001b[0m | time: 0.163s\n",
            "| Adam | epoch: 189 | loss: 0.00972 - acc: 0.9973 -- iter: 288/963\n",
            "Training Step: 22785  | total loss: \u001b[1m\u001b[32m0.00876\u001b[0m\u001b[0m | time: 0.165s\n",
            "| Adam | epoch: 189 | loss: 0.00876 - acc: 0.9976 -- iter: 296/963\n",
            "Training Step: 22786  | total loss: \u001b[1m\u001b[32m0.00790\u001b[0m\u001b[0m | time: 0.170s\n",
            "| Adam | epoch: 189 | loss: 0.00790 - acc: 0.9978 -- iter: 304/963\n",
            "Training Step: 22787  | total loss: \u001b[1m\u001b[32m0.01501\u001b[0m\u001b[0m | time: 0.173s\n",
            "| Adam | epoch: 189 | loss: 0.01501 - acc: 0.9980 -- iter: 312/963\n",
            "Training Step: 22788  | total loss: \u001b[1m\u001b[32m0.01352\u001b[0m\u001b[0m | time: 0.175s\n",
            "| Adam | epoch: 189 | loss: 0.01352 - acc: 0.9982 -- iter: 320/963\n",
            "Training Step: 22789  | total loss: \u001b[1m\u001b[32m0.01326\u001b[0m\u001b[0m | time: 0.177s\n",
            "| Adam | epoch: 189 | loss: 0.01326 - acc: 0.9984 -- iter: 328/963\n",
            "Training Step: 22790  | total loss: \u001b[1m\u001b[32m0.01195\u001b[0m\u001b[0m | time: 0.181s\n",
            "| Adam | epoch: 189 | loss: 0.01195 - acc: 0.9986 -- iter: 336/963\n",
            "Training Step: 22791  | total loss: \u001b[1m\u001b[32m0.01166\u001b[0m\u001b[0m | time: 0.185s\n",
            "| Adam | epoch: 189 | loss: 0.01166 - acc: 0.9987 -- iter: 344/963\n",
            "Training Step: 22792  | total loss: \u001b[1m\u001b[32m0.01723\u001b[0m\u001b[0m | time: 0.187s\n",
            "| Adam | epoch: 189 | loss: 0.01723 - acc: 0.9988 -- iter: 352/963\n",
            "Training Step: 22793  | total loss: \u001b[1m\u001b[32m0.01553\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 189 | loss: 0.01553 - acc: 0.9989 -- iter: 360/963\n",
            "Training Step: 22794  | total loss: \u001b[1m\u001b[32m0.01401\u001b[0m\u001b[0m | time: 0.191s\n",
            "| Adam | epoch: 189 | loss: 0.01401 - acc: 0.9991 -- iter: 368/963\n",
            "Training Step: 22795  | total loss: \u001b[1m\u001b[32m0.01263\u001b[0m\u001b[0m | time: 0.196s\n",
            "| Adam | epoch: 189 | loss: 0.01263 - acc: 0.9991 -- iter: 376/963\n",
            "Training Step: 22796  | total loss: \u001b[1m\u001b[32m0.01141\u001b[0m\u001b[0m | time: 0.200s\n",
            "| Adam | epoch: 189 | loss: 0.01141 - acc: 0.9992 -- iter: 384/963\n",
            "Training Step: 22797  | total loss: \u001b[1m\u001b[32m0.01037\u001b[0m\u001b[0m | time: 0.201s\n",
            "| Adam | epoch: 189 | loss: 0.01037 - acc: 0.9993 -- iter: 392/963\n",
            "Training Step: 22798  | total loss: \u001b[1m\u001b[32m0.01160\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 189 | loss: 0.01160 - acc: 0.9994 -- iter: 400/963\n",
            "Training Step: 22799  | total loss: \u001b[1m\u001b[32m0.01053\u001b[0m\u001b[0m | time: 0.256s\n",
            "| Adam | epoch: 189 | loss: 0.01053 - acc: 0.9994 -- iter: 408/963\n",
            "Training Step: 22800  | total loss: \u001b[1m\u001b[32m0.00950\u001b[0m\u001b[0m | time: 0.259s\n",
            "| Adam | epoch: 189 | loss: 0.00950 - acc: 0.9995 -- iter: 416/963\n",
            "Training Step: 22801  | total loss: \u001b[1m\u001b[32m0.00857\u001b[0m\u001b[0m | time: 0.261s\n",
            "| Adam | epoch: 189 | loss: 0.00857 - acc: 0.9995 -- iter: 424/963\n",
            "Training Step: 22802  | total loss: \u001b[1m\u001b[32m0.00773\u001b[0m\u001b[0m | time: 0.264s\n",
            "| Adam | epoch: 189 | loss: 0.00773 - acc: 0.9996 -- iter: 432/963\n",
            "Training Step: 22803  | total loss: \u001b[1m\u001b[32m0.00697\u001b[0m\u001b[0m | time: 0.269s\n",
            "| Adam | epoch: 189 | loss: 0.00697 - acc: 0.9996 -- iter: 440/963\n",
            "Training Step: 22804  | total loss: \u001b[1m\u001b[32m0.00631\u001b[0m\u001b[0m | time: 0.274s\n",
            "| Adam | epoch: 189 | loss: 0.00631 - acc: 0.9997 -- iter: 448/963\n",
            "Training Step: 22805  | total loss: \u001b[1m\u001b[32m0.00573\u001b[0m\u001b[0m | time: 0.281s\n",
            "| Adam | epoch: 189 | loss: 0.00573 - acc: 0.9997 -- iter: 456/963\n",
            "Training Step: 22806  | total loss: \u001b[1m\u001b[32m0.00517\u001b[0m\u001b[0m | time: 0.287s\n",
            "| Adam | epoch: 189 | loss: 0.00517 - acc: 0.9997 -- iter: 464/963\n",
            "Training Step: 22807  | total loss: \u001b[1m\u001b[32m0.00467\u001b[0m\u001b[0m | time: 0.291s\n",
            "| Adam | epoch: 189 | loss: 0.00467 - acc: 0.9998 -- iter: 472/963\n",
            "Training Step: 22808  | total loss: \u001b[1m\u001b[32m0.00423\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 189 | loss: 0.00423 - acc: 0.9998 -- iter: 480/963\n",
            "Training Step: 22809  | total loss: \u001b[1m\u001b[32m0.00382\u001b[0m\u001b[0m | time: 0.301s\n",
            "| Adam | epoch: 189 | loss: 0.00382 - acc: 0.9998 -- iter: 488/963\n",
            "Training Step: 22810  | total loss: \u001b[1m\u001b[32m0.00347\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 189 | loss: 0.00347 - acc: 0.9998 -- iter: 496/963\n",
            "Training Step: 22811  | total loss: \u001b[1m\u001b[32m0.00316\u001b[0m\u001b[0m | time: 0.307s\n",
            "| Adam | epoch: 189 | loss: 0.00316 - acc: 0.9998 -- iter: 504/963\n",
            "Training Step: 22812  | total loss: \u001b[1m\u001b[32m0.00311\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 189 | loss: 0.00311 - acc: 0.9999 -- iter: 512/963\n",
            "Training Step: 22813  | total loss: \u001b[1m\u001b[32m0.03147\u001b[0m\u001b[0m | time: 0.311s\n",
            "| Adam | epoch: 189 | loss: 0.03147 - acc: 0.9874 -- iter: 520/963\n",
            "Training Step: 22814  | total loss: \u001b[1m\u001b[32m0.02834\u001b[0m\u001b[0m | time: 0.316s\n",
            "| Adam | epoch: 189 | loss: 0.02834 - acc: 0.9886 -- iter: 528/963\n",
            "Training Step: 22815  | total loss: \u001b[1m\u001b[32m0.02552\u001b[0m\u001b[0m | time: 0.319s\n",
            "| Adam | epoch: 189 | loss: 0.02552 - acc: 0.9898 -- iter: 536/963\n",
            "Training Step: 22816  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.321s\n",
            "| Adam | epoch: 189 | loss: 0.02297 - acc: 0.9908 -- iter: 544/963\n",
            "Training Step: 22817  | total loss: \u001b[1m\u001b[32m0.02069\u001b[0m\u001b[0m | time: 0.323s\n",
            "| Adam | epoch: 189 | loss: 0.02069 - acc: 0.9917 -- iter: 552/963\n",
            "Training Step: 22818  | total loss: \u001b[1m\u001b[32m0.01864\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 189 | loss: 0.01864 - acc: 0.9925 -- iter: 560/963\n",
            "Training Step: 22819  | total loss: \u001b[1m\u001b[32m0.01679\u001b[0m\u001b[0m | time: 0.327s\n",
            "| Adam | epoch: 189 | loss: 0.01679 - acc: 0.9933 -- iter: 568/963\n",
            "Training Step: 22820  | total loss: \u001b[1m\u001b[32m0.01513\u001b[0m\u001b[0m | time: 0.329s\n",
            "| Adam | epoch: 189 | loss: 0.01513 - acc: 0.9940 -- iter: 576/963\n",
            "Training Step: 22821  | total loss: \u001b[1m\u001b[32m0.08811\u001b[0m\u001b[0m | time: 0.331s\n",
            "| Adam | epoch: 189 | loss: 0.08811 - acc: 0.9696 -- iter: 584/963\n",
            "Training Step: 22822  | total loss: \u001b[1m\u001b[32m0.09521\u001b[0m\u001b[0m | time: 0.333s\n",
            "| Adam | epoch: 189 | loss: 0.09521 - acc: 0.9601 -- iter: 592/963\n",
            "Training Step: 22823  | total loss: \u001b[1m\u001b[32m0.08569\u001b[0m\u001b[0m | time: 0.335s\n",
            "| Adam | epoch: 189 | loss: 0.08569 - acc: 0.9641 -- iter: 600/963\n",
            "Training Step: 22824  | total loss: \u001b[1m\u001b[32m0.07715\u001b[0m\u001b[0m | time: 0.337s\n",
            "| Adam | epoch: 189 | loss: 0.07715 - acc: 0.9677 -- iter: 608/963\n",
            "Training Step: 22825  | total loss: \u001b[1m\u001b[32m0.06945\u001b[0m\u001b[0m | time: 0.339s\n",
            "| Adam | epoch: 189 | loss: 0.06945 - acc: 0.9709 -- iter: 616/963\n",
            "Training Step: 22826  | total loss: \u001b[1m\u001b[32m0.06251\u001b[0m\u001b[0m | time: 0.340s\n",
            "| Adam | epoch: 189 | loss: 0.06251 - acc: 0.9738 -- iter: 624/963\n",
            "Training Step: 22827  | total loss: \u001b[1m\u001b[32m0.05630\u001b[0m\u001b[0m | time: 0.342s\n",
            "| Adam | epoch: 189 | loss: 0.05630 - acc: 0.9764 -- iter: 632/963\n",
            "Training Step: 22828  | total loss: \u001b[1m\u001b[32m0.05067\u001b[0m\u001b[0m | time: 0.344s\n",
            "| Adam | epoch: 189 | loss: 0.05067 - acc: 0.9788 -- iter: 640/963\n",
            "Training Step: 22829  | total loss: \u001b[1m\u001b[32m0.05864\u001b[0m\u001b[0m | time: 0.346s\n",
            "| Adam | epoch: 189 | loss: 0.05864 - acc: 0.9684 -- iter: 648/963\n",
            "Training Step: 22830  | total loss: \u001b[1m\u001b[32m0.05282\u001b[0m\u001b[0m | time: 0.348s\n",
            "| Adam | epoch: 189 | loss: 0.05282 - acc: 0.9716 -- iter: 656/963\n",
            "Training Step: 22831  | total loss: \u001b[1m\u001b[32m0.04757\u001b[0m\u001b[0m | time: 0.352s\n",
            "| Adam | epoch: 189 | loss: 0.04757 - acc: 0.9744 -- iter: 664/963\n",
            "Training Step: 22832  | total loss: \u001b[1m\u001b[32m0.04284\u001b[0m\u001b[0m | time: 0.354s\n",
            "| Adam | epoch: 189 | loss: 0.04284 - acc: 0.9770 -- iter: 672/963\n",
            "Training Step: 22833  | total loss: \u001b[1m\u001b[32m0.03856\u001b[0m\u001b[0m | time: 0.357s\n",
            "| Adam | epoch: 189 | loss: 0.03856 - acc: 0.9793 -- iter: 680/963\n",
            "Training Step: 22834  | total loss: \u001b[1m\u001b[32m0.03474\u001b[0m\u001b[0m | time: 0.359s\n",
            "| Adam | epoch: 189 | loss: 0.03474 - acc: 0.9814 -- iter: 688/963\n",
            "Training Step: 22835  | total loss: \u001b[1m\u001b[32m0.03128\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 189 | loss: 0.03128 - acc: 0.9832 -- iter: 696/963\n",
            "Training Step: 22836  | total loss: \u001b[1m\u001b[32m0.03058\u001b[0m\u001b[0m | time: 0.369s\n",
            "| Adam | epoch: 189 | loss: 0.03058 - acc: 0.9849 -- iter: 704/963\n",
            "Training Step: 22837  | total loss: \u001b[1m\u001b[32m0.02755\u001b[0m\u001b[0m | time: 0.372s\n",
            "| Adam | epoch: 189 | loss: 0.02755 - acc: 0.9864 -- iter: 712/963\n",
            "Training Step: 22838  | total loss: \u001b[1m\u001b[32m0.02480\u001b[0m\u001b[0m | time: 0.375s\n",
            "| Adam | epoch: 189 | loss: 0.02480 - acc: 0.9878 -- iter: 720/963\n",
            "Training Step: 22839  | total loss: \u001b[1m\u001b[32m0.02236\u001b[0m\u001b[0m | time: 0.378s\n",
            "| Adam | epoch: 189 | loss: 0.02236 - acc: 0.9890 -- iter: 728/963\n",
            "Training Step: 22840  | total loss: \u001b[1m\u001b[32m0.02015\u001b[0m\u001b[0m | time: 0.380s\n",
            "| Adam | epoch: 189 | loss: 0.02015 - acc: 0.9901 -- iter: 736/963\n",
            "Training Step: 22841  | total loss: \u001b[1m\u001b[32m0.01830\u001b[0m\u001b[0m | time: 0.383s\n",
            "| Adam | epoch: 189 | loss: 0.01830 - acc: 0.9911 -- iter: 744/963\n",
            "Training Step: 22842  | total loss: \u001b[1m\u001b[32m0.01650\u001b[0m\u001b[0m | time: 0.385s\n",
            "| Adam | epoch: 189 | loss: 0.01650 - acc: 0.9920 -- iter: 752/963\n",
            "Training Step: 22843  | total loss: \u001b[1m\u001b[32m0.01510\u001b[0m\u001b[0m | time: 0.387s\n",
            "| Adam | epoch: 189 | loss: 0.01510 - acc: 0.9928 -- iter: 760/963\n",
            "Training Step: 22844  | total loss: \u001b[1m\u001b[32m0.02541\u001b[0m\u001b[0m | time: 0.389s\n",
            "| Adam | epoch: 189 | loss: 0.02541 - acc: 0.9810 -- iter: 768/963\n",
            "Training Step: 22845  | total loss: \u001b[1m\u001b[32m0.02297\u001b[0m\u001b[0m | time: 0.392s\n",
            "| Adam | epoch: 189 | loss: 0.02297 - acc: 0.9829 -- iter: 776/963\n",
            "Training Step: 22846  | total loss: \u001b[1m\u001b[32m0.02070\u001b[0m\u001b[0m | time: 0.394s\n",
            "| Adam | epoch: 189 | loss: 0.02070 - acc: 0.9846 -- iter: 784/963\n",
            "Training Step: 22847  | total loss: \u001b[1m\u001b[32m0.01866\u001b[0m\u001b[0m | time: 0.396s\n",
            "| Adam | epoch: 189 | loss: 0.01866 - acc: 0.9861 -- iter: 792/963\n",
            "Training Step: 22848  | total loss: \u001b[1m\u001b[32m0.01681\u001b[0m\u001b[0m | time: 0.398s\n",
            "| Adam | epoch: 189 | loss: 0.01681 - acc: 0.9875 -- iter: 800/963\n",
            "Training Step: 22849  | total loss: \u001b[1m\u001b[32m0.01515\u001b[0m\u001b[0m | time: 0.400s\n",
            "| Adam | epoch: 189 | loss: 0.01515 - acc: 0.9888 -- iter: 808/963\n",
            "Training Step: 22850  | total loss: \u001b[1m\u001b[32m0.01368\u001b[0m\u001b[0m | time: 0.402s\n",
            "| Adam | epoch: 189 | loss: 0.01368 - acc: 0.9899 -- iter: 816/963\n",
            "Training Step: 22851  | total loss: \u001b[1m\u001b[32m0.01250\u001b[0m\u001b[0m | time: 0.404s\n",
            "| Adam | epoch: 189 | loss: 0.01250 - acc: 0.9909 -- iter: 824/963\n",
            "Training Step: 22852  | total loss: \u001b[1m\u001b[32m0.01129\u001b[0m\u001b[0m | time: 0.407s\n",
            "| Adam | epoch: 189 | loss: 0.01129 - acc: 0.9918 -- iter: 832/963\n",
            "Training Step: 22853  | total loss: \u001b[1m\u001b[32m0.01020\u001b[0m\u001b[0m | time: 0.409s\n",
            "| Adam | epoch: 189 | loss: 0.01020 - acc: 0.9926 -- iter: 840/963\n",
            "Training Step: 22854  | total loss: \u001b[1m\u001b[32m0.00937\u001b[0m\u001b[0m | time: 0.411s\n",
            "| Adam | epoch: 189 | loss: 0.00937 - acc: 0.9934 -- iter: 848/963\n",
            "Training Step: 22855  | total loss: \u001b[1m\u001b[32m0.04903\u001b[0m\u001b[0m | time: 0.414s\n",
            "| Adam | epoch: 189 | loss: 0.04903 - acc: 0.9815 -- iter: 856/963\n",
            "Training Step: 22856  | total loss: \u001b[1m\u001b[32m0.04413\u001b[0m\u001b[0m | time: 0.416s\n",
            "| Adam | epoch: 189 | loss: 0.04413 - acc: 0.9834 -- iter: 864/963\n",
            "Training Step: 22857  | total loss: \u001b[1m\u001b[32m0.03976\u001b[0m\u001b[0m | time: 0.418s\n",
            "| Adam | epoch: 189 | loss: 0.03976 - acc: 0.9850 -- iter: 872/963\n",
            "Training Step: 22858  | total loss: \u001b[1m\u001b[32m0.03582\u001b[0m\u001b[0m | time: 0.421s\n",
            "| Adam | epoch: 189 | loss: 0.03582 - acc: 0.9865 -- iter: 880/963\n",
            "Training Step: 22859  | total loss: \u001b[1m\u001b[32m0.03225\u001b[0m\u001b[0m | time: 0.423s\n",
            "| Adam | epoch: 189 | loss: 0.03225 - acc: 0.9879 -- iter: 888/963\n",
            "Training Step: 22860  | total loss: \u001b[1m\u001b[32m0.02903\u001b[0m\u001b[0m | time: 0.426s\n",
            "| Adam | epoch: 189 | loss: 0.02903 - acc: 0.9891 -- iter: 896/963\n",
            "Training Step: 22861  | total loss: \u001b[1m\u001b[32m0.02617\u001b[0m\u001b[0m | time: 0.428s\n",
            "| Adam | epoch: 189 | loss: 0.02617 - acc: 0.9902 -- iter: 904/963\n",
            "Training Step: 22862  | total loss: \u001b[1m\u001b[32m0.02367\u001b[0m\u001b[0m | time: 0.431s\n",
            "| Adam | epoch: 189 | loss: 0.02367 - acc: 0.9912 -- iter: 912/963\n",
            "Training Step: 22863  | total loss: \u001b[1m\u001b[32m0.02132\u001b[0m\u001b[0m | time: 0.433s\n",
            "| Adam | epoch: 189 | loss: 0.02132 - acc: 0.9921 -- iter: 920/963\n",
            "Training Step: 22864  | total loss: \u001b[1m\u001b[32m0.02255\u001b[0m\u001b[0m | time: 0.436s\n",
            "| Adam | epoch: 189 | loss: 0.02255 - acc: 0.9928 -- iter: 928/963\n",
            "Training Step: 22865  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.438s\n",
            "| Adam | epoch: 189 | loss: 0.02044 - acc: 0.9936 -- iter: 936/963\n",
            "Training Step: 22866  | total loss: \u001b[1m\u001b[32m0.01841\u001b[0m\u001b[0m | time: 0.441s\n",
            "| Adam | epoch: 189 | loss: 0.01841 - acc: 0.9942 -- iter: 944/963\n",
            "Training Step: 22867  | total loss: \u001b[1m\u001b[32m0.01662\u001b[0m\u001b[0m | time: 0.443s\n",
            "| Adam | epoch: 189 | loss: 0.01662 - acc: 0.9948 -- iter: 952/963\n",
            "Training Step: 22868  | total loss: \u001b[1m\u001b[32m0.01987\u001b[0m\u001b[0m | time: 0.445s\n",
            "| Adam | epoch: 189 | loss: 0.01987 - acc: 0.9953 -- iter: 960/963\n",
            "Training Step: 22869  | total loss: \u001b[1m\u001b[32m0.01792\u001b[0m\u001b[0m | time: 0.447s\n",
            "| Adam | epoch: 189 | loss: 0.01792 - acc: 0.9958 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22870  | total loss: \u001b[1m\u001b[32m0.01633\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 190 | loss: 0.01633 - acc: 0.9962 -- iter: 008/963\n",
            "Training Step: 22871  | total loss: \u001b[1m\u001b[32m0.01476\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 190 | loss: 0.01476 - acc: 0.9966 -- iter: 016/963\n",
            "Training Step: 22872  | total loss: \u001b[1m\u001b[32m0.01330\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 190 | loss: 0.01330 - acc: 0.9969 -- iter: 024/963\n",
            "Training Step: 22873  | total loss: \u001b[1m\u001b[32m0.02520\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 190 | loss: 0.02520 - acc: 0.9847 -- iter: 032/963\n",
            "Training Step: 22874  | total loss: \u001b[1m\u001b[32m0.02275\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 190 | loss: 0.02275 - acc: 0.9863 -- iter: 040/963\n",
            "Training Step: 22875  | total loss: \u001b[1m\u001b[32m0.02049\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 190 | loss: 0.02049 - acc: 0.9876 -- iter: 048/963\n",
            "Training Step: 22876  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 190 | loss: 0.01846 - acc: 0.9889 -- iter: 056/963\n",
            "Training Step: 22877  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 190 | loss: 0.01813 - acc: 0.9900 -- iter: 064/963\n",
            "Training Step: 22878  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 190 | loss: 0.01635 - acc: 0.9910 -- iter: 072/963\n",
            "Training Step: 22879  | total loss: \u001b[1m\u001b[32m0.01547\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 190 | loss: 0.01547 - acc: 0.9919 -- iter: 080/963\n",
            "Training Step: 22880  | total loss: \u001b[1m\u001b[32m0.01622\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 190 | loss: 0.01622 - acc: 0.9927 -- iter: 088/963\n",
            "Training Step: 22881  | total loss: \u001b[1m\u001b[32m0.01466\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 190 | loss: 0.01466 - acc: 0.9934 -- iter: 096/963\n",
            "Training Step: 22882  | total loss: \u001b[1m\u001b[32m0.01320\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 190 | loss: 0.01320 - acc: 0.9941 -- iter: 104/963\n",
            "Training Step: 22883  | total loss: \u001b[1m\u001b[32m0.01191\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 190 | loss: 0.01191 - acc: 0.9947 -- iter: 112/963\n",
            "Training Step: 22884  | total loss: \u001b[1m\u001b[32m0.01075\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 190 | loss: 0.01075 - acc: 0.9952 -- iter: 120/963\n",
            "Training Step: 22885  | total loss: \u001b[1m\u001b[32m0.00969\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 190 | loss: 0.00969 - acc: 0.9957 -- iter: 128/963\n",
            "Training Step: 22886  | total loss: \u001b[1m\u001b[32m0.00874\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 190 | loss: 0.00874 - acc: 0.9961 -- iter: 136/963\n",
            "Training Step: 22887  | total loss: \u001b[1m\u001b[32m0.00788\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 190 | loss: 0.00788 - acc: 0.9965 -- iter: 144/963\n",
            "Training Step: 22888  | total loss: \u001b[1m\u001b[32m0.03793\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 190 | loss: 0.03793 - acc: 0.9844 -- iter: 152/963\n",
            "Training Step: 22889  | total loss: \u001b[1m\u001b[32m0.03414\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 190 | loss: 0.03414 - acc: 0.9873 -- iter: 160/963\n",
            "Training Step: 22890  | total loss: \u001b[1m\u001b[32m0.03075\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 190 | loss: 0.03075 - acc: 0.9873 -- iter: 168/963\n",
            "Training Step: 22891  | total loss: \u001b[1m\u001b[32m0.02770\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 190 | loss: 0.02770 - acc: 0.9886 -- iter: 176/963\n",
            "Training Step: 22892  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.140s\n",
            "| Adam | epoch: 190 | loss: 0.02512 - acc: 0.9897 -- iter: 184/963\n",
            "Training Step: 22893  | total loss: \u001b[1m\u001b[32m0.02281\u001b[0m\u001b[0m | time: 0.146s\n",
            "| Adam | epoch: 190 | loss: 0.02281 - acc: 0.9908 -- iter: 192/963\n",
            "Training Step: 22894  | total loss: \u001b[1m\u001b[32m0.05202\u001b[0m\u001b[0m | time: 0.150s\n",
            "| Adam | epoch: 190 | loss: 0.05202 - acc: 0.9792 -- iter: 200/963\n",
            "Training Step: 22895  | total loss: \u001b[1m\u001b[32m0.04693\u001b[0m\u001b[0m | time: 0.155s\n",
            "| Adam | epoch: 190 | loss: 0.04693 - acc: 0.9813 -- iter: 208/963\n",
            "Training Step: 22896  | total loss: \u001b[1m\u001b[32m0.04234\u001b[0m\u001b[0m | time: 0.157s\n",
            "| Adam | epoch: 190 | loss: 0.04234 - acc: 0.9831 -- iter: 216/963\n",
            "Training Step: 22897  | total loss: \u001b[1m\u001b[32m0.03812\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 190 | loss: 0.03812 - acc: 0.9848 -- iter: 224/963\n",
            "Training Step: 22898  | total loss: \u001b[1m\u001b[32m0.03433\u001b[0m\u001b[0m | time: 0.163s\n",
            "| Adam | epoch: 190 | loss: 0.03433 - acc: 0.9863 -- iter: 232/963\n",
            "Training Step: 22899  | total loss: \u001b[1m\u001b[32m0.03091\u001b[0m\u001b[0m | time: 0.165s\n",
            "| Adam | epoch: 190 | loss: 0.03091 - acc: 0.9877 -- iter: 240/963\n",
            "Training Step: 22900  | total loss: \u001b[1m\u001b[32m0.02795\u001b[0m\u001b[0m | time: 0.168s\n",
            "| Adam | epoch: 190 | loss: 0.02795 - acc: 0.9889 -- iter: 248/963\n",
            "Training Step: 22901  | total loss: \u001b[1m\u001b[32m0.02999\u001b[0m\u001b[0m | time: 0.170s\n",
            "| Adam | epoch: 190 | loss: 0.02999 - acc: 0.9900 -- iter: 256/963\n",
            "Training Step: 22902  | total loss: \u001b[1m\u001b[32m0.02707\u001b[0m\u001b[0m | time: 0.173s\n",
            "| Adam | epoch: 190 | loss: 0.02707 - acc: 0.9910 -- iter: 264/963\n",
            "Training Step: 22903  | total loss: \u001b[1m\u001b[32m0.02440\u001b[0m\u001b[0m | time: 0.176s\n",
            "| Adam | epoch: 190 | loss: 0.02440 - acc: 0.9919 -- iter: 272/963\n",
            "Training Step: 22904  | total loss: \u001b[1m\u001b[32m0.02214\u001b[0m\u001b[0m | time: 0.179s\n",
            "| Adam | epoch: 190 | loss: 0.02214 - acc: 0.9927 -- iter: 280/963\n",
            "Training Step: 22905  | total loss: \u001b[1m\u001b[32m0.01995\u001b[0m\u001b[0m | time: 0.181s\n",
            "| Adam | epoch: 190 | loss: 0.01995 - acc: 0.9935 -- iter: 288/963\n",
            "Training Step: 22906  | total loss: \u001b[1m\u001b[32m0.01801\u001b[0m\u001b[0m | time: 0.184s\n",
            "| Adam | epoch: 190 | loss: 0.01801 - acc: 0.9941 -- iter: 296/963\n",
            "Training Step: 22907  | total loss: \u001b[1m\u001b[32m0.01623\u001b[0m\u001b[0m | time: 0.186s\n",
            "| Adam | epoch: 190 | loss: 0.01623 - acc: 0.9947 -- iter: 304/963\n",
            "Training Step: 22908  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 190 | loss: 0.01462 - acc: 0.9952 -- iter: 312/963\n",
            "Training Step: 22909  | total loss: \u001b[1m\u001b[32m0.01317\u001b[0m\u001b[0m | time: 0.191s\n",
            "| Adam | epoch: 190 | loss: 0.01317 - acc: 0.9957 -- iter: 320/963\n",
            "Training Step: 22910  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.193s\n",
            "| Adam | epoch: 190 | loss: 0.01944 - acc: 0.9961 -- iter: 328/963\n",
            "Training Step: 22911  | total loss: \u001b[1m\u001b[32m0.01751\u001b[0m\u001b[0m | time: 0.196s\n",
            "| Adam | epoch: 190 | loss: 0.01751 - acc: 0.9965 -- iter: 336/963\n",
            "Training Step: 22912  | total loss: \u001b[1m\u001b[32m0.01577\u001b[0m\u001b[0m | time: 0.198s\n",
            "| Adam | epoch: 190 | loss: 0.01577 - acc: 0.9969 -- iter: 344/963\n",
            "Training Step: 22913  | total loss: \u001b[1m\u001b[32m0.01425\u001b[0m\u001b[0m | time: 0.201s\n",
            "| Adam | epoch: 190 | loss: 0.01425 - acc: 0.9972 -- iter: 352/963\n",
            "Training Step: 22914  | total loss: \u001b[1m\u001b[32m0.01306\u001b[0m\u001b[0m | time: 0.203s\n",
            "| Adam | epoch: 190 | loss: 0.01306 - acc: 0.9975 -- iter: 360/963\n",
            "Training Step: 22915  | total loss: \u001b[1m\u001b[32m0.01177\u001b[0m\u001b[0m | time: 0.206s\n",
            "| Adam | epoch: 190 | loss: 0.01177 - acc: 0.9977 -- iter: 368/963\n",
            "Training Step: 22916  | total loss: \u001b[1m\u001b[32m0.01086\u001b[0m\u001b[0m | time: 0.208s\n",
            "| Adam | epoch: 190 | loss: 0.01086 - acc: 0.9980 -- iter: 376/963\n",
            "Training Step: 22917  | total loss: \u001b[1m\u001b[32m0.01108\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 190 | loss: 0.01108 - acc: 0.9982 -- iter: 384/963\n",
            "Training Step: 22918  | total loss: \u001b[1m\u001b[32m0.01000\u001b[0m\u001b[0m | time: 0.213s\n",
            "| Adam | epoch: 190 | loss: 0.01000 - acc: 0.9983 -- iter: 392/963\n",
            "Training Step: 22919  | total loss: \u001b[1m\u001b[32m0.01796\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 190 | loss: 0.01796 - acc: 0.9860 -- iter: 400/963\n",
            "Training Step: 22920  | total loss: \u001b[1m\u001b[32m0.01618\u001b[0m\u001b[0m | time: 0.218s\n",
            "| Adam | epoch: 190 | loss: 0.01618 - acc: 0.9874 -- iter: 408/963\n",
            "Training Step: 22921  | total loss: \u001b[1m\u001b[32m0.01457\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 190 | loss: 0.01457 - acc: 0.9887 -- iter: 416/963\n",
            "Training Step: 22922  | total loss: \u001b[1m\u001b[32m0.01314\u001b[0m\u001b[0m | time: 0.222s\n",
            "| Adam | epoch: 190 | loss: 0.01314 - acc: 0.9898 -- iter: 424/963\n",
            "Training Step: 22923  | total loss: \u001b[1m\u001b[32m0.01184\u001b[0m\u001b[0m | time: 0.225s\n",
            "| Adam | epoch: 190 | loss: 0.01184 - acc: 0.9908 -- iter: 432/963\n",
            "Training Step: 22924  | total loss: \u001b[1m\u001b[32m0.04244\u001b[0m\u001b[0m | time: 0.229s\n",
            "| Adam | epoch: 190 | loss: 0.04244 - acc: 0.9792 -- iter: 440/963\n",
            "Training Step: 22925  | total loss: \u001b[1m\u001b[32m0.03840\u001b[0m\u001b[0m | time: 0.232s\n",
            "| Adam | epoch: 190 | loss: 0.03840 - acc: 0.9813 -- iter: 448/963\n",
            "Training Step: 22926  | total loss: \u001b[1m\u001b[32m0.03462\u001b[0m\u001b[0m | time: 0.238s\n",
            "| Adam | epoch: 190 | loss: 0.03462 - acc: 0.9832 -- iter: 456/963\n",
            "Training Step: 22927  | total loss: \u001b[1m\u001b[32m0.03131\u001b[0m\u001b[0m | time: 0.241s\n",
            "| Adam | epoch: 190 | loss: 0.03131 - acc: 0.9849 -- iter: 464/963\n",
            "Training Step: 22928  | total loss: \u001b[1m\u001b[32m0.02822\u001b[0m\u001b[0m | time: 0.244s\n",
            "| Adam | epoch: 190 | loss: 0.02822 - acc: 0.9864 -- iter: 472/963\n",
            "Training Step: 22929  | total loss: \u001b[1m\u001b[32m0.02542\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 190 | loss: 0.02542 - acc: 0.9877 -- iter: 480/963\n",
            "Training Step: 22930  | total loss: \u001b[1m\u001b[32m0.02290\u001b[0m\u001b[0m | time: 0.249s\n",
            "| Adam | epoch: 190 | loss: 0.02290 - acc: 0.9890 -- iter: 488/963\n",
            "Training Step: 22931  | total loss: \u001b[1m\u001b[32m0.02072\u001b[0m\u001b[0m | time: 0.252s\n",
            "| Adam | epoch: 190 | loss: 0.02072 - acc: 0.9901 -- iter: 496/963\n",
            "Training Step: 22932  | total loss: \u001b[1m\u001b[32m0.02108\u001b[0m\u001b[0m | time: 0.256s\n",
            "| Adam | epoch: 190 | loss: 0.02108 - acc: 0.9911 -- iter: 504/963\n",
            "Training Step: 22933  | total loss: \u001b[1m\u001b[32m0.01898\u001b[0m\u001b[0m | time: 0.259s\n",
            "| Adam | epoch: 190 | loss: 0.01898 - acc: 0.9920 -- iter: 512/963\n",
            "Training Step: 22934  | total loss: \u001b[1m\u001b[32m0.02191\u001b[0m\u001b[0m | time: 0.261s\n",
            "| Adam | epoch: 190 | loss: 0.02191 - acc: 0.9928 -- iter: 520/963\n",
            "Training Step: 22935  | total loss: \u001b[1m\u001b[32m0.01977\u001b[0m\u001b[0m | time: 0.264s\n",
            "| Adam | epoch: 190 | loss: 0.01977 - acc: 0.9935 -- iter: 528/963\n",
            "Training Step: 22936  | total loss: \u001b[1m\u001b[32m0.01781\u001b[0m\u001b[0m | time: 0.282s\n",
            "| Adam | epoch: 190 | loss: 0.01781 - acc: 0.9941 -- iter: 536/963\n",
            "Training Step: 22937  | total loss: \u001b[1m\u001b[32m0.01605\u001b[0m\u001b[0m | time: 0.288s\n",
            "| Adam | epoch: 190 | loss: 0.01605 - acc: 0.9947 -- iter: 544/963\n",
            "Training Step: 22938  | total loss: \u001b[1m\u001b[32m0.05175\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 190 | loss: 0.05175 - acc: 0.9827 -- iter: 552/963\n",
            "Training Step: 22939  | total loss: \u001b[1m\u001b[32m0.04658\u001b[0m\u001b[0m | time: 0.299s\n",
            "| Adam | epoch: 190 | loss: 0.04658 - acc: 0.9845 -- iter: 560/963\n",
            "Training Step: 22940  | total loss: \u001b[1m\u001b[32m0.04195\u001b[0m\u001b[0m | time: 0.301s\n",
            "| Adam | epoch: 190 | loss: 0.04195 - acc: 0.9860 -- iter: 568/963\n",
            "Training Step: 22941  | total loss: \u001b[1m\u001b[32m0.03776\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 190 | loss: 0.03776 - acc: 0.9874 -- iter: 576/963\n",
            "Training Step: 22942  | total loss: \u001b[1m\u001b[32m0.03400\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 190 | loss: 0.03400 - acc: 0.9887 -- iter: 584/963\n",
            "Training Step: 22943  | total loss: \u001b[1m\u001b[32m0.03066\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 190 | loss: 0.03066 - acc: 0.9898 -- iter: 592/963\n",
            "Training Step: 22944  | total loss: \u001b[1m\u001b[32m0.03006\u001b[0m\u001b[0m | time: 0.317s\n",
            "| Adam | epoch: 190 | loss: 0.03006 - acc: 0.9908 -- iter: 600/963\n",
            "Training Step: 22945  | total loss: \u001b[1m\u001b[32m0.02713\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 190 | loss: 0.02713 - acc: 0.9917 -- iter: 608/963\n",
            "Training Step: 22946  | total loss: \u001b[1m\u001b[32m0.02724\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 190 | loss: 0.02724 - acc: 0.9926 -- iter: 616/963\n",
            "Training Step: 22947  | total loss: \u001b[1m\u001b[32m0.02452\u001b[0m\u001b[0m | time: 0.329s\n",
            "| Adam | epoch: 190 | loss: 0.02452 - acc: 0.9933 -- iter: 624/963\n",
            "Training Step: 22948  | total loss: \u001b[1m\u001b[32m0.02214\u001b[0m\u001b[0m | time: 0.333s\n",
            "| Adam | epoch: 190 | loss: 0.02214 - acc: 0.9940 -- iter: 632/963\n",
            "Training Step: 22949  | total loss: \u001b[1m\u001b[32m0.02008\u001b[0m\u001b[0m | time: 0.338s\n",
            "| Adam | epoch: 190 | loss: 0.02008 - acc: 0.9946 -- iter: 640/963\n",
            "Training Step: 22950  | total loss: \u001b[1m\u001b[32m0.01809\u001b[0m\u001b[0m | time: 0.342s\n",
            "| Adam | epoch: 190 | loss: 0.01809 - acc: 0.9951 -- iter: 648/963\n",
            "Training Step: 22951  | total loss: \u001b[1m\u001b[32m0.01635\u001b[0m\u001b[0m | time: 0.345s\n",
            "| Adam | epoch: 190 | loss: 0.01635 - acc: 0.9956 -- iter: 656/963\n",
            "Training Step: 22952  | total loss: \u001b[1m\u001b[32m0.01473\u001b[0m\u001b[0m | time: 0.353s\n",
            "| Adam | epoch: 190 | loss: 0.01473 - acc: 0.9961 -- iter: 664/963\n",
            "Training Step: 22953  | total loss: \u001b[1m\u001b[32m0.01327\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 190 | loss: 0.01327 - acc: 0.9964 -- iter: 672/963\n",
            "Training Step: 22954  | total loss: \u001b[1m\u001b[32m0.01196\u001b[0m\u001b[0m | time: 0.365s\n",
            "| Adam | epoch: 190 | loss: 0.01196 - acc: 0.9968 -- iter: 680/963\n",
            "Training Step: 22955  | total loss: \u001b[1m\u001b[32m0.01082\u001b[0m\u001b[0m | time: 0.371s\n",
            "| Adam | epoch: 190 | loss: 0.01082 - acc: 0.9971 -- iter: 688/963\n",
            "Training Step: 22956  | total loss: \u001b[1m\u001b[32m0.00989\u001b[0m\u001b[0m | time: 0.378s\n",
            "| Adam | epoch: 190 | loss: 0.00989 - acc: 0.9974 -- iter: 696/963\n",
            "Training Step: 22957  | total loss: \u001b[1m\u001b[32m0.01075\u001b[0m\u001b[0m | time: 0.384s\n",
            "| Adam | epoch: 190 | loss: 0.01075 - acc: 0.9977 -- iter: 704/963\n",
            "Training Step: 22958  | total loss: \u001b[1m\u001b[32m0.02808\u001b[0m\u001b[0m | time: 0.387s\n",
            "| Adam | epoch: 190 | loss: 0.02808 - acc: 0.9854 -- iter: 712/963\n",
            "Training Step: 22959  | total loss: \u001b[1m\u001b[32m0.02529\u001b[0m\u001b[0m | time: 0.390s\n",
            "| Adam | epoch: 190 | loss: 0.02529 - acc: 0.9869 -- iter: 720/963\n",
            "Training Step: 22960  | total loss: \u001b[1m\u001b[32m0.02353\u001b[0m\u001b[0m | time: 0.392s\n",
            "| Adam | epoch: 190 | loss: 0.02353 - acc: 0.9882 -- iter: 728/963\n",
            "Training Step: 22961  | total loss: \u001b[1m\u001b[32m0.02120\u001b[0m\u001b[0m | time: 0.400s\n",
            "| Adam | epoch: 190 | loss: 0.02120 - acc: 0.9894 -- iter: 736/963\n",
            "Training Step: 22962  | total loss: \u001b[1m\u001b[32m0.01909\u001b[0m\u001b[0m | time: 0.405s\n",
            "| Adam | epoch: 190 | loss: 0.01909 - acc: 0.9904 -- iter: 744/963\n",
            "Training Step: 22963  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.409s\n",
            "| Adam | epoch: 190 | loss: 0.01721 - acc: 0.9914 -- iter: 752/963\n",
            "Training Step: 22964  | total loss: \u001b[1m\u001b[32m0.01555\u001b[0m\u001b[0m | time: 0.412s\n",
            "| Adam | epoch: 190 | loss: 0.01555 - acc: 0.9922 -- iter: 760/963\n",
            "Training Step: 22965  | total loss: \u001b[1m\u001b[32m0.01401\u001b[0m\u001b[0m | time: 0.415s\n",
            "| Adam | epoch: 190 | loss: 0.01401 - acc: 0.9930 -- iter: 768/963\n",
            "Training Step: 22966  | total loss: \u001b[1m\u001b[32m0.01264\u001b[0m\u001b[0m | time: 0.418s\n",
            "| Adam | epoch: 190 | loss: 0.01264 - acc: 0.9937 -- iter: 776/963\n",
            "Training Step: 22967  | total loss: \u001b[1m\u001b[32m0.01140\u001b[0m\u001b[0m | time: 0.420s\n",
            "| Adam | epoch: 190 | loss: 0.01140 - acc: 0.9943 -- iter: 784/963\n",
            "Training Step: 22968  | total loss: \u001b[1m\u001b[32m0.01106\u001b[0m\u001b[0m | time: 0.423s\n",
            "| Adam | epoch: 190 | loss: 0.01106 - acc: 0.9949 -- iter: 792/963\n",
            "Training Step: 22969  | total loss: \u001b[1m\u001b[32m0.03904\u001b[0m\u001b[0m | time: 0.425s\n",
            "| Adam | epoch: 190 | loss: 0.03904 - acc: 0.9829 -- iter: 800/963\n",
            "Training Step: 22970  | total loss: \u001b[1m\u001b[32m0.03516\u001b[0m\u001b[0m | time: 0.427s\n",
            "| Adam | epoch: 190 | loss: 0.03516 - acc: 0.9846 -- iter: 808/963\n",
            "Training Step: 22971  | total loss: \u001b[1m\u001b[32m0.03170\u001b[0m\u001b[0m | time: 0.430s\n",
            "| Adam | epoch: 190 | loss: 0.03170 - acc: 0.9862 -- iter: 816/963\n",
            "Training Step: 22972  | total loss: \u001b[1m\u001b[32m0.05104\u001b[0m\u001b[0m | time: 0.432s\n",
            "| Adam | epoch: 190 | loss: 0.05104 - acc: 0.9750 -- iter: 824/963\n",
            "Training Step: 22973  | total loss: \u001b[1m\u001b[32m0.04596\u001b[0m\u001b[0m | time: 0.434s\n",
            "| Adam | epoch: 190 | loss: 0.04596 - acc: 0.9775 -- iter: 832/963\n",
            "Training Step: 22974  | total loss: \u001b[1m\u001b[32m0.04137\u001b[0m\u001b[0m | time: 0.437s\n",
            "| Adam | epoch: 190 | loss: 0.04137 - acc: 0.9798 -- iter: 840/963\n",
            "Training Step: 22975  | total loss: \u001b[1m\u001b[32m0.03725\u001b[0m\u001b[0m | time: 0.439s\n",
            "| Adam | epoch: 190 | loss: 0.03725 - acc: 0.9818 -- iter: 848/963\n",
            "Training Step: 22976  | total loss: \u001b[1m\u001b[32m0.03372\u001b[0m\u001b[0m | time: 0.441s\n",
            "| Adam | epoch: 190 | loss: 0.03372 - acc: 0.9836 -- iter: 856/963\n",
            "Training Step: 22977  | total loss: \u001b[1m\u001b[32m0.03368\u001b[0m\u001b[0m | time: 0.443s\n",
            "| Adam | epoch: 190 | loss: 0.03368 - acc: 0.9853 -- iter: 864/963\n",
            "Training Step: 22978  | total loss: \u001b[1m\u001b[32m0.03032\u001b[0m\u001b[0m | time: 0.446s\n",
            "| Adam | epoch: 190 | loss: 0.03032 - acc: 0.9867 -- iter: 872/963\n",
            "Training Step: 22979  | total loss: \u001b[1m\u001b[32m0.02731\u001b[0m\u001b[0m | time: 0.448s\n",
            "| Adam | epoch: 190 | loss: 0.02731 - acc: 0.9881 -- iter: 880/963\n",
            "Training Step: 22980  | total loss: \u001b[1m\u001b[32m0.02464\u001b[0m\u001b[0m | time: 0.450s\n",
            "| Adam | epoch: 190 | loss: 0.02464 - acc: 0.9893 -- iter: 888/963\n",
            "Training Step: 22981  | total loss: \u001b[1m\u001b[32m0.02219\u001b[0m\u001b[0m | time: 0.453s\n",
            "| Adam | epoch: 190 | loss: 0.02219 - acc: 0.9903 -- iter: 896/963\n",
            "Training Step: 22982  | total loss: \u001b[1m\u001b[32m0.02000\u001b[0m\u001b[0m | time: 0.458s\n",
            "| Adam | epoch: 190 | loss: 0.02000 - acc: 0.9913 -- iter: 904/963\n",
            "Training Step: 22983  | total loss: \u001b[1m\u001b[32m0.01803\u001b[0m\u001b[0m | time: 0.461s\n",
            "| Adam | epoch: 190 | loss: 0.01803 - acc: 0.9922 -- iter: 912/963\n",
            "Training Step: 22984  | total loss: \u001b[1m\u001b[32m0.01625\u001b[0m\u001b[0m | time: 0.464s\n",
            "| Adam | epoch: 190 | loss: 0.01625 - acc: 0.9930 -- iter: 920/963\n",
            "Training Step: 22985  | total loss: \u001b[1m\u001b[32m0.01467\u001b[0m\u001b[0m | time: 0.468s\n",
            "| Adam | epoch: 190 | loss: 0.01467 - acc: 0.9937 -- iter: 928/963\n",
            "Training Step: 22986  | total loss: \u001b[1m\u001b[32m0.01321\u001b[0m\u001b[0m | time: 0.471s\n",
            "| Adam | epoch: 190 | loss: 0.01321 - acc: 0.9943 -- iter: 936/963\n",
            "Training Step: 22987  | total loss: \u001b[1m\u001b[32m0.01190\u001b[0m\u001b[0m | time: 0.474s\n",
            "| Adam | epoch: 190 | loss: 0.01190 - acc: 0.9949 -- iter: 944/963\n",
            "Training Step: 22988  | total loss: \u001b[1m\u001b[32m0.01075\u001b[0m\u001b[0m | time: 0.476s\n",
            "| Adam | epoch: 190 | loss: 0.01075 - acc: 0.9954 -- iter: 952/963\n",
            "Training Step: 22989  | total loss: \u001b[1m\u001b[32m0.00968\u001b[0m\u001b[0m | time: 0.478s\n",
            "| Adam | epoch: 190 | loss: 0.00968 - acc: 0.9958 -- iter: 960/963\n",
            "Training Step: 22990  | total loss: \u001b[1m\u001b[32m0.04498\u001b[0m\u001b[0m | time: 0.481s\n",
            "| Adam | epoch: 190 | loss: 0.04498 - acc: 0.9838 -- iter: 963/963\n",
            "--\n",
            "Training Step: 22991  | total loss: \u001b[1m\u001b[32m0.04050\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 191 | loss: 0.04050 - acc: 0.9854 -- iter: 008/963\n",
            "Training Step: 22992  | total loss: \u001b[1m\u001b[32m0.03650\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 191 | loss: 0.03650 - acc: 0.9868 -- iter: 016/963\n",
            "Training Step: 22993  | total loss: \u001b[1m\u001b[32m0.03292\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 191 | loss: 0.03292 - acc: 0.9882 -- iter: 024/963\n",
            "Training Step: 22994  | total loss: \u001b[1m\u001b[32m0.02965\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 191 | loss: 0.02965 - acc: 0.9893 -- iter: 032/963\n",
            "Training Step: 22995  | total loss: \u001b[1m\u001b[32m0.02673\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 191 | loss: 0.02673 - acc: 0.9904 -- iter: 040/963\n",
            "Training Step: 22996  | total loss: \u001b[1m\u001b[32m0.02406\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 191 | loss: 0.02406 - acc: 0.9914 -- iter: 048/963\n",
            "Training Step: 22997  | total loss: \u001b[1m\u001b[32m0.02167\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 191 | loss: 0.02167 - acc: 0.9922 -- iter: 056/963\n",
            "Training Step: 22998  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 191 | loss: 0.01951 - acc: 0.9930 -- iter: 064/963\n",
            "Training Step: 22999  | total loss: \u001b[1m\u001b[32m0.01758\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 191 | loss: 0.01758 - acc: 0.9937 -- iter: 072/963\n",
            "Training Step: 23000  | total loss: \u001b[1m\u001b[32m0.01599\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 191 | loss: 0.01599 - acc: 0.9943 -- iter: 080/963\n",
            "Training Step: 23001  | total loss: \u001b[1m\u001b[32m0.01443\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 191 | loss: 0.01443 - acc: 0.9949 -- iter: 088/963\n",
            "Training Step: 23002  | total loss: \u001b[1m\u001b[32m0.01304\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 191 | loss: 0.01304 - acc: 0.9954 -- iter: 096/963\n",
            "Training Step: 23003  | total loss: \u001b[1m\u001b[32m0.01177\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 191 | loss: 0.01177 - acc: 0.9959 -- iter: 104/963\n",
            "Training Step: 23004  | total loss: \u001b[1m\u001b[32m0.01068\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 191 | loss: 0.01068 - acc: 0.9963 -- iter: 112/963\n",
            "Training Step: 23005  | total loss: \u001b[1m\u001b[32m0.00964\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 191 | loss: 0.00964 - acc: 0.9967 -- iter: 120/963\n",
            "Training Step: 23006  | total loss: \u001b[1m\u001b[32m0.00870\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 191 | loss: 0.00870 - acc: 0.9970 -- iter: 128/963\n",
            "Training Step: 23007  | total loss: \u001b[1m\u001b[32m0.00784\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 191 | loss: 0.00784 - acc: 0.9973 -- iter: 136/963\n",
            "Training Step: 23008  | total loss: \u001b[1m\u001b[32m0.00706\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 191 | loss: 0.00706 - acc: 0.9976 -- iter: 144/963\n",
            "Training Step: 23009  | total loss: \u001b[1m\u001b[32m0.00639\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 191 | loss: 0.00639 - acc: 0.9978 -- iter: 152/963\n",
            "Training Step: 23010  | total loss: \u001b[1m\u001b[32m0.01926\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 191 | loss: 0.01926 - acc: 0.9855 -- iter: 160/963\n",
            "Training Step: 23011  | total loss: \u001b[1m\u001b[32m0.01736\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 191 | loss: 0.01736 - acc: 0.9870 -- iter: 168/963\n",
            "Training Step: 23012  | total loss: \u001b[1m\u001b[32m0.01566\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 191 | loss: 0.01566 - acc: 0.9883 -- iter: 176/963\n",
            "Training Step: 23013  | total loss: \u001b[1m\u001b[32m0.01417\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 191 | loss: 0.01417 - acc: 0.9894 -- iter: 184/963\n",
            "Training Step: 23014  | total loss: \u001b[1m\u001b[32m0.01277\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 191 | loss: 0.01277 - acc: 0.9905 -- iter: 192/963\n",
            "Training Step: 23015  | total loss: \u001b[1m\u001b[32m0.01151\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 191 | loss: 0.01151 - acc: 0.9915 -- iter: 200/963\n",
            "Training Step: 23016  | total loss: \u001b[1m\u001b[32m0.01906\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 191 | loss: 0.01906 - acc: 0.9923 -- iter: 208/963\n",
            "Training Step: 23017  | total loss: \u001b[1m\u001b[32m0.01717\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 191 | loss: 0.01717 - acc: 0.9931 -- iter: 216/963\n",
            "Training Step: 23018  | total loss: \u001b[1m\u001b[32m0.01548\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 191 | loss: 0.01548 - acc: 0.9938 -- iter: 224/963\n",
            "Training Step: 23019  | total loss: \u001b[1m\u001b[32m0.01395\u001b[0m\u001b[0m | time: 0.127s\n",
            "| Adam | epoch: 191 | loss: 0.01395 - acc: 0.9944 -- iter: 232/963\n",
            "Training Step: 23020  | total loss: \u001b[1m\u001b[32m0.01256\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 191 | loss: 0.01256 - acc: 0.9950 -- iter: 240/963\n",
            "Training Step: 23021  | total loss: \u001b[1m\u001b[32m0.01619\u001b[0m\u001b[0m | time: 0.132s\n",
            "| Adam | epoch: 191 | loss: 0.01619 - acc: 0.9955 -- iter: 248/963\n",
            "Training Step: 23022  | total loss: \u001b[1m\u001b[32m0.01457\u001b[0m\u001b[0m | time: 0.134s\n",
            "| Adam | epoch: 191 | loss: 0.01457 - acc: 0.9959 -- iter: 256/963\n",
            "Training Step: 23023  | total loss: \u001b[1m\u001b[32m0.01313\u001b[0m\u001b[0m | time: 0.136s\n",
            "| Adam | epoch: 191 | loss: 0.01313 - acc: 0.9963 -- iter: 264/963\n",
            "Training Step: 23024  | total loss: \u001b[1m\u001b[32m0.01183\u001b[0m\u001b[0m | time: 0.139s\n",
            "| Adam | epoch: 191 | loss: 0.01183 - acc: 0.9967 -- iter: 272/963\n",
            "Training Step: 23025  | total loss: \u001b[1m\u001b[32m0.01067\u001b[0m\u001b[0m | time: 0.141s\n",
            "| Adam | epoch: 191 | loss: 0.01067 - acc: 0.9970 -- iter: 280/963\n",
            "Training Step: 23026  | total loss: \u001b[1m\u001b[32m0.00964\u001b[0m\u001b[0m | time: 0.143s\n",
            "| Adam | epoch: 191 | loss: 0.00964 - acc: 0.9973 -- iter: 288/963\n",
            "Training Step: 23027  | total loss: \u001b[1m\u001b[32m0.03873\u001b[0m\u001b[0m | time: 0.146s\n",
            "| Adam | epoch: 191 | loss: 0.03873 - acc: 0.9851 -- iter: 296/963\n",
            "Training Step: 23028  | total loss: \u001b[1m\u001b[32m0.03492\u001b[0m\u001b[0m | time: 0.148s\n",
            "| Adam | epoch: 191 | loss: 0.03492 - acc: 0.9866 -- iter: 304/963\n",
            "Training Step: 23029  | total loss: \u001b[1m\u001b[32m0.03145\u001b[0m\u001b[0m | time: 0.151s\n",
            "| Adam | epoch: 191 | loss: 0.03145 - acc: 0.9879 -- iter: 312/963\n",
            "Training Step: 23030  | total loss: \u001b[1m\u001b[32m0.02833\u001b[0m\u001b[0m | time: 0.153s\n",
            "| Adam | epoch: 191 | loss: 0.02833 - acc: 0.9891 -- iter: 320/963\n",
            "Training Step: 23031  | total loss: \u001b[1m\u001b[32m0.02553\u001b[0m\u001b[0m | time: 0.157s\n",
            "| Adam | epoch: 191 | loss: 0.02553 - acc: 0.9902 -- iter: 328/963\n",
            "Training Step: 23032  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.164s\n",
            "| Adam | epoch: 191 | loss: 0.02299 - acc: 0.9912 -- iter: 336/963\n",
            "Training Step: 23033  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 191 | loss: 0.02073 - acc: 0.9921 -- iter: 344/963\n",
            "Training Step: 23034  | total loss: \u001b[1m\u001b[32m0.01868\u001b[0m\u001b[0m | time: 0.173s\n",
            "| Adam | epoch: 191 | loss: 0.01868 - acc: 0.9929 -- iter: 352/963\n",
            "Training Step: 23035  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.176s\n",
            "| Adam | epoch: 191 | loss: 0.01682 - acc: 0.9936 -- iter: 360/963\n",
            "Training Step: 23036  | total loss: \u001b[1m\u001b[32m0.01515\u001b[0m\u001b[0m | time: 0.183s\n",
            "| Adam | epoch: 191 | loss: 0.01515 - acc: 0.9942 -- iter: 368/963\n",
            "Training Step: 23037  | total loss: \u001b[1m\u001b[32m0.01364\u001b[0m\u001b[0m | time: 0.186s\n",
            "| Adam | epoch: 191 | loss: 0.01364 - acc: 0.9948 -- iter: 376/963\n",
            "Training Step: 23038  | total loss: \u001b[1m\u001b[32m0.01229\u001b[0m\u001b[0m | time: 0.190s\n",
            "| Adam | epoch: 191 | loss: 0.01229 - acc: 0.9953 -- iter: 384/963\n",
            "Training Step: 23039  | total loss: \u001b[1m\u001b[32m0.01106\u001b[0m\u001b[0m | time: 0.195s\n",
            "| Adam | epoch: 191 | loss: 0.01106 - acc: 0.9958 -- iter: 392/963\n",
            "Training Step: 23040  | total loss: \u001b[1m\u001b[32m0.00997\u001b[0m\u001b[0m | time: 0.197s\n",
            "| Adam | epoch: 191 | loss: 0.00997 - acc: 0.9962 -- iter: 400/963\n",
            "Training Step: 23041  | total loss: \u001b[1m\u001b[32m0.00899\u001b[0m\u001b[0m | time: 0.201s\n",
            "| Adam | epoch: 191 | loss: 0.00899 - acc: 0.9966 -- iter: 408/963\n",
            "Training Step: 23042  | total loss: \u001b[1m\u001b[32m0.00810\u001b[0m\u001b[0m | time: 0.204s\n",
            "| Adam | epoch: 191 | loss: 0.00810 - acc: 0.9969 -- iter: 416/963\n",
            "Training Step: 23043  | total loss: \u001b[1m\u001b[32m0.00736\u001b[0m\u001b[0m | time: 0.209s\n",
            "| Adam | epoch: 191 | loss: 0.00736 - acc: 0.9972 -- iter: 424/963\n",
            "Training Step: 23044  | total loss: \u001b[1m\u001b[32m0.00666\u001b[0m\u001b[0m | time: 0.213s\n",
            "| Adam | epoch: 191 | loss: 0.00666 - acc: 0.9975 -- iter: 432/963\n",
            "Training Step: 23045  | total loss: \u001b[1m\u001b[32m0.00605\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 191 | loss: 0.00605 - acc: 0.9978 -- iter: 440/963\n",
            "Training Step: 23046  | total loss: \u001b[1m\u001b[32m0.00545\u001b[0m\u001b[0m | time: 0.218s\n",
            "| Adam | epoch: 191 | loss: 0.00545 - acc: 0.9980 -- iter: 448/963\n",
            "Training Step: 23047  | total loss: \u001b[1m\u001b[32m0.00493\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 191 | loss: 0.00493 - acc: 0.9982 -- iter: 456/963\n",
            "Training Step: 23048  | total loss: \u001b[1m\u001b[32m0.00446\u001b[0m\u001b[0m | time: 0.227s\n",
            "| Adam | epoch: 191 | loss: 0.00446 - acc: 0.9984 -- iter: 464/963\n",
            "Training Step: 23049  | total loss: \u001b[1m\u001b[32m0.00406\u001b[0m\u001b[0m | time: 0.232s\n",
            "| Adam | epoch: 191 | loss: 0.00406 - acc: 0.9985 -- iter: 472/963\n",
            "Training Step: 23050  | total loss: \u001b[1m\u001b[32m0.00367\u001b[0m\u001b[0m | time: 0.235s\n",
            "| Adam | epoch: 191 | loss: 0.00367 - acc: 0.9987 -- iter: 480/963\n",
            "Training Step: 23051  | total loss: \u001b[1m\u001b[32m0.00331\u001b[0m\u001b[0m | time: 0.238s\n",
            "| Adam | epoch: 191 | loss: 0.00331 - acc: 0.9988 -- iter: 488/963\n",
            "Training Step: 23052  | total loss: \u001b[1m\u001b[32m0.00299\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 191 | loss: 0.00299 - acc: 0.9989 -- iter: 496/963\n",
            "Training Step: 23053  | total loss: \u001b[1m\u001b[32m0.00384\u001b[0m\u001b[0m | time: 0.245s\n",
            "| Adam | epoch: 191 | loss: 0.00384 - acc: 0.9990 -- iter: 504/963\n",
            "Training Step: 23054  | total loss: \u001b[1m\u001b[32m0.03496\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 191 | loss: 0.03496 - acc: 0.9866 -- iter: 512/963\n",
            "Training Step: 23055  | total loss: \u001b[1m\u001b[32m0.03159\u001b[0m\u001b[0m | time: 0.254s\n",
            "| Adam | epoch: 191 | loss: 0.03159 - acc: 0.9880 -- iter: 520/963\n",
            "Training Step: 23056  | total loss: \u001b[1m\u001b[32m0.02843\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 191 | loss: 0.02843 - acc: 0.9892 -- iter: 528/963\n",
            "Training Step: 23057  | total loss: \u001b[1m\u001b[32m0.02562\u001b[0m\u001b[0m | time: 0.260s\n",
            "| Adam | epoch: 191 | loss: 0.02562 - acc: 0.9903 -- iter: 536/963\n",
            "Training Step: 23058  | total loss: \u001b[1m\u001b[32m0.02306\u001b[0m\u001b[0m | time: 0.262s\n",
            "| Adam | epoch: 191 | loss: 0.02306 - acc: 0.9912 -- iter: 544/963\n",
            "Training Step: 23059  | total loss: \u001b[1m\u001b[32m0.02076\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 191 | loss: 0.02076 - acc: 0.9921 -- iter: 552/963\n",
            "Training Step: 23060  | total loss: \u001b[1m\u001b[32m0.01873\u001b[0m\u001b[0m | time: 0.270s\n",
            "| Adam | epoch: 191 | loss: 0.01873 - acc: 0.9929 -- iter: 560/963\n",
            "Training Step: 23061  | total loss: \u001b[1m\u001b[32m0.01686\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 191 | loss: 0.01686 - acc: 0.9936 -- iter: 568/963\n",
            "Training Step: 23062  | total loss: \u001b[1m\u001b[32m0.01518\u001b[0m\u001b[0m | time: 0.275s\n",
            "| Adam | epoch: 191 | loss: 0.01518 - acc: 0.9942 -- iter: 576/963\n",
            "Training Step: 23063  | total loss: \u001b[1m\u001b[32m0.01367\u001b[0m\u001b[0m | time: 0.278s\n",
            "| Adam | epoch: 191 | loss: 0.01367 - acc: 0.9948 -- iter: 584/963\n",
            "Training Step: 23064  | total loss: \u001b[1m\u001b[32m0.01233\u001b[0m\u001b[0m | time: 0.281s\n",
            "| Adam | epoch: 191 | loss: 0.01233 - acc: 0.9953 -- iter: 592/963\n",
            "Training Step: 23065  | total loss: \u001b[1m\u001b[32m0.01113\u001b[0m\u001b[0m | time: 0.283s\n",
            "| Adam | epoch: 191 | loss: 0.01113 - acc: 0.9958 -- iter: 600/963\n",
            "Training Step: 23066  | total loss: \u001b[1m\u001b[32m0.01019\u001b[0m\u001b[0m | time: 0.286s\n",
            "| Adam | epoch: 191 | loss: 0.01019 - acc: 0.9962 -- iter: 608/963\n",
            "Training Step: 23067  | total loss: \u001b[1m\u001b[32m0.01024\u001b[0m\u001b[0m | time: 0.289s\n",
            "| Adam | epoch: 191 | loss: 0.01024 - acc: 0.9966 -- iter: 616/963\n",
            "Training Step: 23068  | total loss: \u001b[1m\u001b[32m0.00923\u001b[0m\u001b[0m | time: 0.292s\n",
            "| Adam | epoch: 191 | loss: 0.00923 - acc: 0.9969 -- iter: 624/963\n",
            "Training Step: 23069  | total loss: \u001b[1m\u001b[32m0.00831\u001b[0m\u001b[0m | time: 0.295s\n",
            "| Adam | epoch: 191 | loss: 0.00831 - acc: 0.9972 -- iter: 632/963\n",
            "Training Step: 23070  | total loss: \u001b[1m\u001b[32m0.01785\u001b[0m\u001b[0m | time: 0.298s\n",
            "| Adam | epoch: 191 | loss: 0.01785 - acc: 0.9850 -- iter: 640/963\n",
            "Training Step: 23071  | total loss: \u001b[1m\u001b[32m0.01616\u001b[0m\u001b[0m | time: 0.301s\n",
            "| Adam | epoch: 191 | loss: 0.01616 - acc: 0.9865 -- iter: 648/963\n",
            "Training Step: 23072  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 191 | loss: 0.02044 - acc: 0.9879 -- iter: 656/963\n",
            "Training Step: 23073  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 191 | loss: 0.01846 - acc: 0.9891 -- iter: 664/963\n",
            "Training Step: 23074  | total loss: \u001b[1m\u001b[32m0.01665\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 191 | loss: 0.01665 - acc: 0.9902 -- iter: 672/963\n",
            "Training Step: 23075  | total loss: \u001b[1m\u001b[32m0.01499\u001b[0m\u001b[0m | time: 0.312s\n",
            "| Adam | epoch: 191 | loss: 0.01499 - acc: 0.9912 -- iter: 680/963\n",
            "Training Step: 23076  | total loss: \u001b[1m\u001b[32m0.01351\u001b[0m\u001b[0m | time: 0.315s\n",
            "| Adam | epoch: 191 | loss: 0.01351 - acc: 0.9920 -- iter: 688/963\n",
            "Training Step: 23077  | total loss: \u001b[1m\u001b[32m0.01216\u001b[0m\u001b[0m | time: 0.318s\n",
            "| Adam | epoch: 191 | loss: 0.01216 - acc: 0.9928 -- iter: 696/963\n",
            "Training Step: 23078  | total loss: \u001b[1m\u001b[32m0.01100\u001b[0m\u001b[0m | time: 0.321s\n",
            "| Adam | epoch: 191 | loss: 0.01100 - acc: 0.9936 -- iter: 704/963\n",
            "Training Step: 23079  | total loss: \u001b[1m\u001b[32m0.00994\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 191 | loss: 0.00994 - acc: 0.9942 -- iter: 712/963\n",
            "Training Step: 23080  | total loss: \u001b[1m\u001b[32m0.00897\u001b[0m\u001b[0m | time: 0.330s\n",
            "| Adam | epoch: 191 | loss: 0.00897 - acc: 0.9948 -- iter: 720/963\n",
            "Training Step: 23081  | total loss: \u001b[1m\u001b[32m0.02696\u001b[0m\u001b[0m | time: 0.335s\n",
            "| Adam | epoch: 191 | loss: 0.02696 - acc: 0.9828 -- iter: 728/963\n",
            "Training Step: 23082  | total loss: \u001b[1m\u001b[32m0.02428\u001b[0m\u001b[0m | time: 0.338s\n",
            "| Adam | epoch: 191 | loss: 0.02428 - acc: 0.9845 -- iter: 736/963\n",
            "Training Step: 23083  | total loss: \u001b[1m\u001b[32m0.02187\u001b[0m\u001b[0m | time: 0.341s\n",
            "| Adam | epoch: 191 | loss: 0.02187 - acc: 0.9861 -- iter: 744/963\n",
            "Training Step: 23084  | total loss: \u001b[1m\u001b[32m0.01970\u001b[0m\u001b[0m | time: 0.344s\n",
            "| Adam | epoch: 191 | loss: 0.01970 - acc: 0.9875 -- iter: 752/963\n",
            "Training Step: 23085  | total loss: \u001b[1m\u001b[32m0.01776\u001b[0m\u001b[0m | time: 0.348s\n",
            "| Adam | epoch: 191 | loss: 0.01776 - acc: 0.9887 -- iter: 760/963\n",
            "Training Step: 23086  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.351s\n",
            "| Adam | epoch: 191 | loss: 0.01602 - acc: 0.9898 -- iter: 768/963\n",
            "Training Step: 23087  | total loss: \u001b[1m\u001b[32m0.01444\u001b[0m\u001b[0m | time: 0.355s\n",
            "| Adam | epoch: 191 | loss: 0.01444 - acc: 0.9909 -- iter: 776/963\n",
            "Training Step: 23088  | total loss: \u001b[1m\u001b[32m0.01304\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 191 | loss: 0.01304 - acc: 0.9918 -- iter: 784/963\n",
            "Training Step: 23089  | total loss: \u001b[1m\u001b[32m0.01175\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 191 | loss: 0.01175 - acc: 0.9926 -- iter: 792/963\n",
            "Training Step: 23090  | total loss: \u001b[1m\u001b[32m0.01061\u001b[0m\u001b[0m | time: 0.364s\n",
            "| Adam | epoch: 191 | loss: 0.01061 - acc: 0.9933 -- iter: 800/963\n",
            "Training Step: 23091  | total loss: \u001b[1m\u001b[32m0.00958\u001b[0m\u001b[0m | time: 0.369s\n",
            "| Adam | epoch: 191 | loss: 0.00958 - acc: 0.9940 -- iter: 808/963\n",
            "Training Step: 23092  | total loss: \u001b[1m\u001b[32m0.00864\u001b[0m\u001b[0m | time: 0.373s\n",
            "| Adam | epoch: 191 | loss: 0.00864 - acc: 0.9946 -- iter: 816/963\n",
            "Training Step: 23093  | total loss: \u001b[1m\u001b[32m0.00778\u001b[0m\u001b[0m | time: 0.378s\n",
            "| Adam | epoch: 191 | loss: 0.00778 - acc: 0.9951 -- iter: 824/963\n",
            "Training Step: 23094  | total loss: \u001b[1m\u001b[32m0.00982\u001b[0m\u001b[0m | time: 0.381s\n",
            "| Adam | epoch: 191 | loss: 0.00982 - acc: 0.9956 -- iter: 832/963\n",
            "Training Step: 23095  | total loss: \u001b[1m\u001b[32m0.00887\u001b[0m\u001b[0m | time: 0.384s\n",
            "| Adam | epoch: 191 | loss: 0.00887 - acc: 0.9961 -- iter: 840/963\n",
            "Training Step: 23096  | total loss: \u001b[1m\u001b[32m0.04150\u001b[0m\u001b[0m | time: 0.386s\n",
            "| Adam | epoch: 191 | loss: 0.04150 - acc: 0.9840 -- iter: 848/963\n",
            "Training Step: 23097  | total loss: \u001b[1m\u001b[32m0.03738\u001b[0m\u001b[0m | time: 0.389s\n",
            "| Adam | epoch: 191 | loss: 0.03738 - acc: 0.9856 -- iter: 856/963\n",
            "Training Step: 23098  | total loss: \u001b[1m\u001b[32m0.03368\u001b[0m\u001b[0m | time: 0.391s\n",
            "| Adam | epoch: 191 | loss: 0.03368 - acc: 0.9870 -- iter: 864/963\n",
            "Training Step: 23099  | total loss: \u001b[1m\u001b[32m0.03032\u001b[0m\u001b[0m | time: 0.394s\n",
            "| Adam | epoch: 191 | loss: 0.03032 - acc: 0.9883 -- iter: 872/963\n",
            "Training Step: 23100  | total loss: \u001b[1m\u001b[32m0.02730\u001b[0m\u001b[0m | time: 0.397s\n",
            "| Adam | epoch: 191 | loss: 0.02730 - acc: 0.9895 -- iter: 880/963\n",
            "Training Step: 23101  | total loss: \u001b[1m\u001b[32m0.02459\u001b[0m\u001b[0m | time: 0.400s\n",
            "| Adam | epoch: 191 | loss: 0.02459 - acc: 0.9905 -- iter: 888/963\n",
            "Training Step: 23102  | total loss: \u001b[1m\u001b[32m0.02317\u001b[0m\u001b[0m | time: 0.403s\n",
            "| Adam | epoch: 191 | loss: 0.02317 - acc: 0.9915 -- iter: 896/963\n",
            "Training Step: 23103  | total loss: \u001b[1m\u001b[32m0.02088\u001b[0m\u001b[0m | time: 0.408s\n",
            "| Adam | epoch: 191 | loss: 0.02088 - acc: 0.9923 -- iter: 904/963\n",
            "Training Step: 23104  | total loss: \u001b[1m\u001b[32m0.01882\u001b[0m\u001b[0m | time: 0.415s\n",
            "| Adam | epoch: 191 | loss: 0.01882 - acc: 0.9931 -- iter: 912/963\n",
            "Training Step: 23105  | total loss: \u001b[1m\u001b[32m0.04322\u001b[0m\u001b[0m | time: 0.421s\n",
            "| Adam | epoch: 191 | loss: 0.04322 - acc: 0.9688 -- iter: 920/963\n",
            "Training Step: 23106  | total loss: \u001b[1m\u001b[32m0.03891\u001b[0m\u001b[0m | time: 0.424s\n",
            "| Adam | epoch: 191 | loss: 0.03891 - acc: 0.9719 -- iter: 928/963\n",
            "Training Step: 23107  | total loss: \u001b[1m\u001b[32m0.03503\u001b[0m\u001b[0m | time: 0.429s\n",
            "| Adam | epoch: 191 | loss: 0.03503 - acc: 0.9747 -- iter: 936/963\n",
            "Training Step: 23108  | total loss: \u001b[1m\u001b[32m0.03177\u001b[0m\u001b[0m | time: 0.434s\n",
            "| Adam | epoch: 191 | loss: 0.03177 - acc: 0.9772 -- iter: 944/963\n",
            "Training Step: 23109  | total loss: \u001b[1m\u001b[32m0.02861\u001b[0m\u001b[0m | time: 0.438s\n",
            "| Adam | epoch: 191 | loss: 0.02861 - acc: 0.9795 -- iter: 952/963\n",
            "Training Step: 23110  | total loss: \u001b[1m\u001b[32m0.02576\u001b[0m\u001b[0m | time: 0.441s\n",
            "| Adam | epoch: 191 | loss: 0.02576 - acc: 0.9816 -- iter: 960/963\n",
            "Training Step: 23111  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.444s\n",
            "| Adam | epoch: 191 | loss: 0.02325 - acc: 0.9834 -- iter: 963/963\n",
            "--\n",
            "Training Step: 23112  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 192 | loss: 0.02105 - acc: 0.9851 -- iter: 008/963\n",
            "Training Step: 23113  | total loss: \u001b[1m\u001b[32m0.01897\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 192 | loss: 0.01897 - acc: 0.9866 -- iter: 016/963\n",
            "Training Step: 23114  | total loss: \u001b[1m\u001b[32m0.01712\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 192 | loss: 0.01712 - acc: 0.9879 -- iter: 024/963\n",
            "Training Step: 23115  | total loss: \u001b[1m\u001b[32m0.01550\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 192 | loss: 0.01550 - acc: 0.9891 -- iter: 032/963\n",
            "Training Step: 23116  | total loss: \u001b[1m\u001b[32m0.01405\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 192 | loss: 0.01405 - acc: 0.9902 -- iter: 040/963\n",
            "Training Step: 23117  | total loss: \u001b[1m\u001b[32m0.01266\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 192 | loss: 0.01266 - acc: 0.9912 -- iter: 048/963\n",
            "Training Step: 23118  | total loss: \u001b[1m\u001b[32m0.01141\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 192 | loss: 0.01141 - acc: 0.9921 -- iter: 056/963\n",
            "Training Step: 23119  | total loss: \u001b[1m\u001b[32m0.01029\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 192 | loss: 0.01029 - acc: 0.9929 -- iter: 064/963\n",
            "Training Step: 23120  | total loss: \u001b[1m\u001b[32m0.03948\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 192 | loss: 0.03948 - acc: 0.9811 -- iter: 072/963\n",
            "Training Step: 23121  | total loss: \u001b[1m\u001b[32m0.03554\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 192 | loss: 0.03554 - acc: 0.9847 -- iter: 080/963\n",
            "Training Step: 23122  | total loss: \u001b[1m\u001b[32m0.03204\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 192 | loss: 0.03204 - acc: 0.9847 -- iter: 088/963\n",
            "Training Step: 23123  | total loss: \u001b[1m\u001b[32m0.02884\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 192 | loss: 0.02884 - acc: 0.9862 -- iter: 096/963\n",
            "Training Step: 23124  | total loss: \u001b[1m\u001b[32m0.02599\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 192 | loss: 0.02599 - acc: 0.9876 -- iter: 104/963\n",
            "Training Step: 23125  | total loss: \u001b[1m\u001b[32m0.02340\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 192 | loss: 0.02340 - acc: 0.9888 -- iter: 112/963\n",
            "Training Step: 23126  | total loss: \u001b[1m\u001b[32m0.02110\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 192 | loss: 0.02110 - acc: 0.9899 -- iter: 120/963\n",
            "Training Step: 23127  | total loss: \u001b[1m\u001b[32m0.01908\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 192 | loss: 0.01908 - acc: 0.9909 -- iter: 128/963\n",
            "Training Step: 23128  | total loss: \u001b[1m\u001b[32m0.01723\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 192 | loss: 0.01723 - acc: 0.9919 -- iter: 136/963\n",
            "Training Step: 23129  | total loss: \u001b[1m\u001b[32m0.01554\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 192 | loss: 0.01554 - acc: 0.9927 -- iter: 144/963\n",
            "Training Step: 23130  | total loss: \u001b[1m\u001b[32m0.01402\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 192 | loss: 0.01402 - acc: 0.9934 -- iter: 152/963\n",
            "Training Step: 23131  | total loss: \u001b[1m\u001b[32m0.01262\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 192 | loss: 0.01262 - acc: 0.9941 -- iter: 160/963\n",
            "Training Step: 23132  | total loss: \u001b[1m\u001b[32m0.01137\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 192 | loss: 0.01137 - acc: 0.9947 -- iter: 168/963\n",
            "Training Step: 23133  | total loss: \u001b[1m\u001b[32m0.01025\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 192 | loss: 0.01025 - acc: 0.9952 -- iter: 176/963\n",
            "Training Step: 23134  | total loss: \u001b[1m\u001b[32m0.00924\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 192 | loss: 0.00924 - acc: 0.9957 -- iter: 184/963\n",
            "Training Step: 23135  | total loss: \u001b[1m\u001b[32m0.00833\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 192 | loss: 0.00833 - acc: 0.9961 -- iter: 192/963\n",
            "Training Step: 23136  | total loss: \u001b[1m\u001b[32m0.00755\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 192 | loss: 0.00755 - acc: 0.9965 -- iter: 200/963\n",
            "Training Step: 23137  | total loss: \u001b[1m\u001b[32m0.04316\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 192 | loss: 0.04316 - acc: 0.9843 -- iter: 208/963\n",
            "Training Step: 23138  | total loss: \u001b[1m\u001b[32m0.03887\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 192 | loss: 0.03887 - acc: 0.9859 -- iter: 216/963\n",
            "Training Step: 23139  | total loss: \u001b[1m\u001b[32m0.03500\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 192 | loss: 0.03500 - acc: 0.9873 -- iter: 224/963\n",
            "Training Step: 23140  | total loss: \u001b[1m\u001b[32m0.03152\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 192 | loss: 0.03152 - acc: 0.9886 -- iter: 232/963\n",
            "Training Step: 23141  | total loss: \u001b[1m\u001b[32m0.02839\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 192 | loss: 0.02839 - acc: 0.9897 -- iter: 240/963\n",
            "Training Step: 23142  | total loss: \u001b[1m\u001b[32m0.06090\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 192 | loss: 0.06090 - acc: 0.9783 -- iter: 248/963\n",
            "Training Step: 23143  | total loss: \u001b[1m\u001b[32m0.05482\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 192 | loss: 0.05482 - acc: 0.9804 -- iter: 256/963\n",
            "Training Step: 23144  | total loss: \u001b[1m\u001b[32m0.04957\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 192 | loss: 0.04957 - acc: 0.9824 -- iter: 264/963\n",
            "Training Step: 23145  | total loss: \u001b[1m\u001b[32m0.04463\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 192 | loss: 0.04463 - acc: 0.9841 -- iter: 272/963\n",
            "Training Step: 23146  | total loss: \u001b[1m\u001b[32m0.04018\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 192 | loss: 0.04018 - acc: 0.9857 -- iter: 280/963\n",
            "Training Step: 23147  | total loss: \u001b[1m\u001b[32m0.03617\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 192 | loss: 0.03617 - acc: 0.9872 -- iter: 288/963\n",
            "Training Step: 23148  | total loss: \u001b[1m\u001b[32m0.04348\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 192 | loss: 0.04348 - acc: 0.9759 -- iter: 296/963\n",
            "Training Step: 23149  | total loss: \u001b[1m\u001b[32m0.03914\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 192 | loss: 0.03914 - acc: 0.9783 -- iter: 304/963\n",
            "Training Step: 23150  | total loss: \u001b[1m\u001b[32m0.03524\u001b[0m\u001b[0m | time: 0.131s\n",
            "| Adam | epoch: 192 | loss: 0.03524 - acc: 0.9805 -- iter: 312/963\n",
            "Training Step: 23151  | total loss: \u001b[1m\u001b[32m0.03172\u001b[0m\u001b[0m | time: 0.133s\n",
            "| Adam | epoch: 192 | loss: 0.03172 - acc: 0.9825 -- iter: 320/963\n",
            "Training Step: 23152  | total loss: \u001b[1m\u001b[32m0.02857\u001b[0m\u001b[0m | time: 0.136s\n",
            "| Adam | epoch: 192 | loss: 0.02857 - acc: 0.9842 -- iter: 328/963\n",
            "Training Step: 23153  | total loss: \u001b[1m\u001b[32m0.02575\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 192 | loss: 0.02575 - acc: 0.9858 -- iter: 336/963\n",
            "Training Step: 23154  | total loss: \u001b[1m\u001b[32m0.02319\u001b[0m\u001b[0m | time: 0.141s\n",
            "| Adam | epoch: 192 | loss: 0.02319 - acc: 0.9872 -- iter: 344/963\n",
            "Training Step: 23155  | total loss: \u001b[1m\u001b[32m0.02089\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 192 | loss: 0.02089 - acc: 0.9896 -- iter: 352/963\n",
            "Training Step: 23156  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 192 | loss: 0.01881 - acc: 0.9896 -- iter: 360/963\n",
            "Training Step: 23157  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.149s\n",
            "| Adam | epoch: 192 | loss: 0.01694 - acc: 0.9907 -- iter: 368/963\n",
            "Training Step: 23158  | total loss: \u001b[1m\u001b[32m0.01526\u001b[0m\u001b[0m | time: 0.152s\n",
            "| Adam | epoch: 192 | loss: 0.01526 - acc: 0.9916 -- iter: 376/963\n",
            "Training Step: 23159  | total loss: \u001b[1m\u001b[32m0.01374\u001b[0m\u001b[0m | time: 0.155s\n",
            "| Adam | epoch: 192 | loss: 0.01374 - acc: 0.9925 -- iter: 384/963\n",
            "Training Step: 23160  | total loss: \u001b[1m\u001b[32m0.01251\u001b[0m\u001b[0m | time: 0.160s\n",
            "| Adam | epoch: 192 | loss: 0.01251 - acc: 0.9932 -- iter: 392/963\n",
            "Training Step: 23161  | total loss: \u001b[1m\u001b[32m0.01128\u001b[0m\u001b[0m | time: 0.163s\n",
            "| Adam | epoch: 192 | loss: 0.01128 - acc: 0.9939 -- iter: 400/963\n",
            "Training Step: 23162  | total loss: \u001b[1m\u001b[32m0.01016\u001b[0m\u001b[0m | time: 0.166s\n",
            "| Adam | epoch: 192 | loss: 0.01016 - acc: 0.9945 -- iter: 408/963\n",
            "Training Step: 23163  | total loss: \u001b[1m\u001b[32m0.00919\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 192 | loss: 0.00919 - acc: 0.9950 -- iter: 416/963\n",
            "Training Step: 23164  | total loss: \u001b[1m\u001b[32m0.00857\u001b[0m\u001b[0m | time: 0.173s\n",
            "| Adam | epoch: 192 | loss: 0.00857 - acc: 0.9955 -- iter: 424/963\n",
            "Training Step: 23165  | total loss: \u001b[1m\u001b[32m0.00774\u001b[0m\u001b[0m | time: 0.177s\n",
            "| Adam | epoch: 192 | loss: 0.00774 - acc: 0.9960 -- iter: 432/963\n",
            "Training Step: 23166  | total loss: \u001b[1m\u001b[32m0.00700\u001b[0m\u001b[0m | time: 0.180s\n",
            "| Adam | epoch: 192 | loss: 0.00700 - acc: 0.9964 -- iter: 440/963\n",
            "Training Step: 23167  | total loss: \u001b[1m\u001b[32m0.00632\u001b[0m\u001b[0m | time: 0.183s\n",
            "| Adam | epoch: 192 | loss: 0.00632 - acc: 0.9968 -- iter: 448/963\n",
            "Training Step: 23168  | total loss: \u001b[1m\u001b[32m0.00570\u001b[0m\u001b[0m | time: 0.187s\n",
            "| Adam | epoch: 192 | loss: 0.00570 - acc: 0.9971 -- iter: 456/963\n",
            "Training Step: 23169  | total loss: \u001b[1m\u001b[32m0.00515\u001b[0m\u001b[0m | time: 0.190s\n",
            "| Adam | epoch: 192 | loss: 0.00515 - acc: 0.9974 -- iter: 464/963\n",
            "Training Step: 23170  | total loss: \u001b[1m\u001b[32m0.00477\u001b[0m\u001b[0m | time: 0.193s\n",
            "| Adam | epoch: 192 | loss: 0.00477 - acc: 0.9976 -- iter: 472/963\n",
            "Training Step: 23171  | total loss: \u001b[1m\u001b[32m0.00444\u001b[0m\u001b[0m | time: 0.196s\n",
            "| Adam | epoch: 192 | loss: 0.00444 - acc: 0.9979 -- iter: 480/963\n",
            "Training Step: 23172  | total loss: \u001b[1m\u001b[32m0.00402\u001b[0m\u001b[0m | time: 0.199s\n",
            "| Adam | epoch: 192 | loss: 0.00402 - acc: 0.9981 -- iter: 488/963\n",
            "Training Step: 23173  | total loss: \u001b[1m\u001b[32m0.00379\u001b[0m\u001b[0m | time: 0.202s\n",
            "| Adam | epoch: 192 | loss: 0.00379 - acc: 0.9983 -- iter: 496/963\n",
            "Training Step: 23174  | total loss: \u001b[1m\u001b[32m0.01670\u001b[0m\u001b[0m | time: 0.205s\n",
            "| Adam | epoch: 192 | loss: 0.01670 - acc: 0.9859 -- iter: 504/963\n",
            "Training Step: 23175  | total loss: \u001b[1m\u001b[32m0.01504\u001b[0m\u001b[0m | time: 0.208s\n",
            "| Adam | epoch: 192 | loss: 0.01504 - acc: 0.9874 -- iter: 512/963\n",
            "Training Step: 23176  | total loss: \u001b[1m\u001b[32m0.01357\u001b[0m\u001b[0m | time: 0.210s\n",
            "| Adam | epoch: 192 | loss: 0.01357 - acc: 0.9886 -- iter: 520/963\n",
            "Training Step: 23177  | total loss: \u001b[1m\u001b[32m0.01222\u001b[0m\u001b[0m | time: 0.213s\n",
            "| Adam | epoch: 192 | loss: 0.01222 - acc: 0.9898 -- iter: 528/963\n",
            "Training Step: 23178  | total loss: \u001b[1m\u001b[32m0.01101\u001b[0m\u001b[0m | time: 0.217s\n",
            "| Adam | epoch: 192 | loss: 0.01101 - acc: 0.9908 -- iter: 536/963\n",
            "Training Step: 23179  | total loss: \u001b[1m\u001b[32m0.04433\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 192 | loss: 0.04433 - acc: 0.9792 -- iter: 544/963\n",
            "Training Step: 23180  | total loss: \u001b[1m\u001b[32m0.03994\u001b[0m\u001b[0m | time: 0.228s\n",
            "| Adam | epoch: 192 | loss: 0.03994 - acc: 0.9813 -- iter: 552/963\n",
            "Training Step: 23181  | total loss: \u001b[1m\u001b[32m0.03599\u001b[0m\u001b[0m | time: 0.233s\n",
            "| Adam | epoch: 192 | loss: 0.03599 - acc: 0.9832 -- iter: 560/963\n",
            "Training Step: 23182  | total loss: \u001b[1m\u001b[32m0.03376\u001b[0m\u001b[0m | time: 0.237s\n",
            "| Adam | epoch: 192 | loss: 0.03376 - acc: 0.9848 -- iter: 568/963\n",
            "Training Step: 23183  | total loss: \u001b[1m\u001b[32m0.03040\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 192 | loss: 0.03040 - acc: 0.9864 -- iter: 576/963\n",
            "Training Step: 23184  | total loss: \u001b[1m\u001b[32m0.02737\u001b[0m\u001b[0m | time: 0.243s\n",
            "| Adam | epoch: 192 | loss: 0.02737 - acc: 0.9877 -- iter: 584/963\n",
            "Training Step: 23185  | total loss: \u001b[1m\u001b[32m0.02465\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 192 | loss: 0.02465 - acc: 0.9889 -- iter: 592/963\n",
            "Training Step: 23186  | total loss: \u001b[1m\u001b[32m0.02222\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 192 | loss: 0.02222 - acc: 0.9901 -- iter: 600/963\n",
            "Training Step: 23187  | total loss: \u001b[1m\u001b[32m0.03612\u001b[0m\u001b[0m | time: 0.254s\n",
            "| Adam | epoch: 192 | loss: 0.03612 - acc: 0.9785 -- iter: 608/963\n",
            "Training Step: 23188  | total loss: \u001b[1m\u001b[32m0.03251\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 192 | loss: 0.03251 - acc: 0.9807 -- iter: 616/963\n",
            "Training Step: 23189  | total loss: \u001b[1m\u001b[32m0.02950\u001b[0m\u001b[0m | time: 0.261s\n",
            "| Adam | epoch: 192 | loss: 0.02950 - acc: 0.9826 -- iter: 624/963\n",
            "Training Step: 23190  | total loss: \u001b[1m\u001b[32m0.02657\u001b[0m\u001b[0m | time: 0.264s\n",
            "| Adam | epoch: 192 | loss: 0.02657 - acc: 0.9844 -- iter: 632/963\n",
            "Training Step: 23191  | total loss: \u001b[1m\u001b[32m0.02395\u001b[0m\u001b[0m | time: 0.268s\n",
            "| Adam | epoch: 192 | loss: 0.02395 - acc: 0.9859 -- iter: 640/963\n",
            "Training Step: 23192  | total loss: \u001b[1m\u001b[32m0.02157\u001b[0m\u001b[0m | time: 0.271s\n",
            "| Adam | epoch: 192 | loss: 0.02157 - acc: 0.9873 -- iter: 648/963\n",
            "Training Step: 23193  | total loss: \u001b[1m\u001b[32m0.05191\u001b[0m\u001b[0m | time: 0.274s\n",
            "| Adam | epoch: 192 | loss: 0.05191 - acc: 0.9761 -- iter: 656/963\n",
            "Training Step: 23194  | total loss: \u001b[1m\u001b[32m0.04674\u001b[0m\u001b[0m | time: 0.277s\n",
            "| Adam | epoch: 192 | loss: 0.04674 - acc: 0.9785 -- iter: 664/963\n",
            "Training Step: 23195  | total loss: \u001b[1m\u001b[32m0.04208\u001b[0m\u001b[0m | time: 0.280s\n",
            "| Adam | epoch: 192 | loss: 0.04208 - acc: 0.9806 -- iter: 672/963\n",
            "Training Step: 23196  | total loss: \u001b[1m\u001b[32m0.03788\u001b[0m\u001b[0m | time: 0.283s\n",
            "| Adam | epoch: 192 | loss: 0.03788 - acc: 0.9826 -- iter: 680/963\n",
            "Training Step: 23197  | total loss: \u001b[1m\u001b[32m0.03410\u001b[0m\u001b[0m | time: 0.286s\n",
            "| Adam | epoch: 192 | loss: 0.03410 - acc: 0.9843 -- iter: 688/963\n",
            "Training Step: 23198  | total loss: \u001b[1m\u001b[32m0.03070\u001b[0m\u001b[0m | time: 0.291s\n",
            "| Adam | epoch: 192 | loss: 0.03070 - acc: 0.9859 -- iter: 696/963\n",
            "Training Step: 23199  | total loss: \u001b[1m\u001b[32m0.02766\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 192 | loss: 0.02766 - acc: 0.9873 -- iter: 704/963\n",
            "Training Step: 23200  | total loss: \u001b[1m\u001b[32m0.02491\u001b[0m\u001b[0m | time: 0.299s\n",
            "| Adam | epoch: 192 | loss: 0.02491 - acc: 0.9886 -- iter: 712/963\n",
            "Training Step: 23201  | total loss: \u001b[1m\u001b[32m0.02243\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 192 | loss: 0.02243 - acc: 0.9897 -- iter: 720/963\n",
            "Training Step: 23202  | total loss: \u001b[1m\u001b[32m0.02021\u001b[0m\u001b[0m | time: 0.307s\n",
            "| Adam | epoch: 192 | loss: 0.02021 - acc: 0.9907 -- iter: 728/963\n",
            "Training Step: 23203  | total loss: \u001b[1m\u001b[32m0.01820\u001b[0m\u001b[0m | time: 0.311s\n",
            "| Adam | epoch: 192 | loss: 0.01820 - acc: 0.9917 -- iter: 736/963\n",
            "Training Step: 23204  | total loss: \u001b[1m\u001b[32m0.01640\u001b[0m\u001b[0m | time: 0.315s\n",
            "| Adam | epoch: 192 | loss: 0.01640 - acc: 0.9925 -- iter: 744/963\n",
            "Training Step: 23205  | total loss: \u001b[1m\u001b[32m0.03504\u001b[0m\u001b[0m | time: 0.322s\n",
            "| Adam | epoch: 192 | loss: 0.03504 - acc: 0.9807 -- iter: 752/963\n",
            "Training Step: 23206  | total loss: \u001b[1m\u001b[32m0.03712\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 192 | loss: 0.03712 - acc: 0.9827 -- iter: 760/963\n",
            "Training Step: 23207  | total loss: \u001b[1m\u001b[32m0.03342\u001b[0m\u001b[0m | time: 0.328s\n",
            "| Adam | epoch: 192 | loss: 0.03342 - acc: 0.9844 -- iter: 768/963\n",
            "Training Step: 23208  | total loss: \u001b[1m\u001b[32m0.03956\u001b[0m\u001b[0m | time: 0.330s\n",
            "| Adam | epoch: 192 | loss: 0.03956 - acc: 0.9735 -- iter: 776/963\n",
            "Training Step: 23209  | total loss: \u001b[1m\u001b[32m0.04387\u001b[0m\u001b[0m | time: 0.333s\n",
            "| Adam | epoch: 192 | loss: 0.04387 - acc: 0.9761 -- iter: 784/963\n",
            "Training Step: 23210  | total loss: \u001b[1m\u001b[32m0.03960\u001b[0m\u001b[0m | time: 0.336s\n",
            "| Adam | epoch: 192 | loss: 0.03960 - acc: 0.9785 -- iter: 792/963\n",
            "Training Step: 23211  | total loss: \u001b[1m\u001b[32m0.04437\u001b[0m\u001b[0m | time: 0.339s\n",
            "| Adam | epoch: 192 | loss: 0.04437 - acc: 0.9807 -- iter: 800/963\n",
            "Training Step: 23212  | total loss: \u001b[1m\u001b[32m0.03994\u001b[0m\u001b[0m | time: 0.342s\n",
            "| Adam | epoch: 192 | loss: 0.03994 - acc: 0.9826 -- iter: 808/963\n",
            "Training Step: 23213  | total loss: \u001b[1m\u001b[32m0.04087\u001b[0m\u001b[0m | time: 0.345s\n",
            "| Adam | epoch: 192 | loss: 0.04087 - acc: 0.9843 -- iter: 816/963\n",
            "Training Step: 23214  | total loss: \u001b[1m\u001b[32m0.03680\u001b[0m\u001b[0m | time: 0.348s\n",
            "| Adam | epoch: 192 | loss: 0.03680 - acc: 0.9859 -- iter: 824/963\n",
            "Training Step: 23215  | total loss: \u001b[1m\u001b[32m0.03313\u001b[0m\u001b[0m | time: 0.351s\n",
            "| Adam | epoch: 192 | loss: 0.03313 - acc: 0.9873 -- iter: 832/963\n",
            "Training Step: 23216  | total loss: \u001b[1m\u001b[32m0.02986\u001b[0m\u001b[0m | time: 0.353s\n",
            "| Adam | epoch: 192 | loss: 0.02986 - acc: 0.9886 -- iter: 840/963\n",
            "Training Step: 23217  | total loss: \u001b[1m\u001b[32m0.02692\u001b[0m\u001b[0m | time: 0.356s\n",
            "| Adam | epoch: 192 | loss: 0.02692 - acc: 0.9897 -- iter: 848/963\n",
            "Training Step: 23218  | total loss: \u001b[1m\u001b[32m0.05217\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 192 | loss: 0.05217 - acc: 0.9782 -- iter: 856/963\n",
            "Training Step: 23219  | total loss: \u001b[1m\u001b[32m0.04696\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 192 | loss: 0.04696 - acc: 0.9804 -- iter: 864/963\n",
            "Training Step: 23220  | total loss: \u001b[1m\u001b[32m0.04230\u001b[0m\u001b[0m | time: 0.364s\n",
            "| Adam | epoch: 192 | loss: 0.04230 - acc: 0.9824 -- iter: 872/963\n",
            "Training Step: 23221  | total loss: \u001b[1m\u001b[32m0.03809\u001b[0m\u001b[0m | time: 0.366s\n",
            "| Adam | epoch: 192 | loss: 0.03809 - acc: 0.9841 -- iter: 880/963\n",
            "Training Step: 23222  | total loss: \u001b[1m\u001b[32m0.03429\u001b[0m\u001b[0m | time: 0.369s\n",
            "| Adam | epoch: 192 | loss: 0.03429 - acc: 0.9857 -- iter: 888/963\n",
            "Training Step: 23223  | total loss: \u001b[1m\u001b[32m0.05519\u001b[0m\u001b[0m | time: 0.371s\n",
            "| Adam | epoch: 192 | loss: 0.05519 - acc: 0.9747 -- iter: 896/963\n",
            "Training Step: 23224  | total loss: \u001b[1m\u001b[32m0.04970\u001b[0m\u001b[0m | time: 0.374s\n",
            "| Adam | epoch: 192 | loss: 0.04970 - acc: 0.9772 -- iter: 904/963\n",
            "Training Step: 23225  | total loss: \u001b[1m\u001b[32m0.04484\u001b[0m\u001b[0m | time: 0.377s\n",
            "| Adam | epoch: 192 | loss: 0.04484 - acc: 0.9795 -- iter: 912/963\n",
            "Training Step: 23226  | total loss: \u001b[1m\u001b[32m0.04038\u001b[0m\u001b[0m | time: 0.379s\n",
            "| Adam | epoch: 192 | loss: 0.04038 - acc: 0.9815 -- iter: 920/963\n",
            "Training Step: 23227  | total loss: \u001b[1m\u001b[32m0.03635\u001b[0m\u001b[0m | time: 0.382s\n",
            "| Adam | epoch: 192 | loss: 0.03635 - acc: 0.9834 -- iter: 928/963\n",
            "Training Step: 23228  | total loss: \u001b[1m\u001b[32m0.03274\u001b[0m\u001b[0m | time: 0.385s\n",
            "| Adam | epoch: 192 | loss: 0.03274 - acc: 0.9850 -- iter: 936/963\n",
            "Training Step: 23229  | total loss: \u001b[1m\u001b[32m0.02950\u001b[0m\u001b[0m | time: 0.388s\n",
            "| Adam | epoch: 192 | loss: 0.02950 - acc: 0.9865 -- iter: 944/963\n",
            "Training Step: 23230  | total loss: \u001b[1m\u001b[32m0.02657\u001b[0m\u001b[0m | time: 0.393s\n",
            "| Adam | epoch: 192 | loss: 0.02657 - acc: 0.9879 -- iter: 952/963\n",
            "Training Step: 23231  | total loss: \u001b[1m\u001b[32m0.03019\u001b[0m\u001b[0m | time: 0.397s\n",
            "| Adam | epoch: 192 | loss: 0.03019 - acc: 0.9891 -- iter: 960/963\n",
            "Training Step: 23232  | total loss: \u001b[1m\u001b[32m0.02724\u001b[0m\u001b[0m | time: 0.400s\n",
            "| Adam | epoch: 192 | loss: 0.02724 - acc: 0.9902 -- iter: 963/963\n",
            "--\n",
            "Training Step: 23233  | total loss: \u001b[1m\u001b[32m0.02459\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 193 | loss: 0.02459 - acc: 0.9912 -- iter: 008/963\n",
            "Training Step: 23234  | total loss: \u001b[1m\u001b[32m0.02218\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 193 | loss: 0.02218 - acc: 0.9920 -- iter: 016/963\n",
            "Training Step: 23235  | total loss: \u001b[1m\u001b[32m0.01997\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 193 | loss: 0.01997 - acc: 0.9928 -- iter: 024/963\n",
            "Training Step: 23236  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 193 | loss: 0.01839 - acc: 0.9936 -- iter: 032/963\n",
            "Training Step: 23237  | total loss: \u001b[1m\u001b[32m0.01657\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 193 | loss: 0.01657 - acc: 0.9942 -- iter: 040/963\n",
            "Training Step: 23238  | total loss: \u001b[1m\u001b[32m0.02318\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 193 | loss: 0.02318 - acc: 0.9948 -- iter: 048/963\n",
            "Training Step: 23239  | total loss: \u001b[1m\u001b[32m0.02089\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 193 | loss: 0.02089 - acc: 0.9953 -- iter: 056/963\n",
            "Training Step: 23240  | total loss: \u001b[1m\u001b[32m0.01881\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 193 | loss: 0.01881 - acc: 0.9958 -- iter: 064/963\n",
            "Training Step: 23241  | total loss: \u001b[1m\u001b[32m0.01694\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 193 | loss: 0.01694 - acc: 0.9962 -- iter: 072/963\n",
            "Training Step: 23242  | total loss: \u001b[1m\u001b[32m0.01525\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 193 | loss: 0.01525 - acc: 0.9966 -- iter: 080/963\n",
            "Training Step: 23243  | total loss: \u001b[1m\u001b[32m0.01378\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 193 | loss: 0.01378 - acc: 0.9969 -- iter: 088/963\n",
            "Training Step: 23244  | total loss: \u001b[1m\u001b[32m0.01242\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 193 | loss: 0.01242 - acc: 0.9972 -- iter: 096/963\n",
            "Training Step: 23245  | total loss: \u001b[1m\u001b[32m0.01123\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 193 | loss: 0.01123 - acc: 0.9975 -- iter: 104/963\n",
            "Training Step: 23246  | total loss: \u001b[1m\u001b[32m0.05247\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 193 | loss: 0.05247 - acc: 0.9853 -- iter: 112/963\n",
            "Training Step: 23247  | total loss: \u001b[1m\u001b[32m0.04943\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 193 | loss: 0.04943 - acc: 0.9867 -- iter: 120/963\n",
            "Training Step: 23248  | total loss: \u001b[1m\u001b[32m0.08322\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 193 | loss: 0.08322 - acc: 0.9756 -- iter: 128/963\n",
            "Training Step: 23249  | total loss: \u001b[1m\u001b[32m0.07492\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 193 | loss: 0.07492 - acc: 0.9780 -- iter: 136/963\n",
            "Training Step: 23250  | total loss: \u001b[1m\u001b[32m0.06744\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 193 | loss: 0.06744 - acc: 0.9802 -- iter: 144/963\n",
            "Training Step: 23251  | total loss: \u001b[1m\u001b[32m0.06071\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 193 | loss: 0.06071 - acc: 0.9822 -- iter: 152/963\n",
            "Training Step: 23252  | total loss: \u001b[1m\u001b[32m0.05468\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 193 | loss: 0.05468 - acc: 0.9840 -- iter: 160/963\n",
            "Training Step: 23253  | total loss: \u001b[1m\u001b[32m0.04923\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 193 | loss: 0.04923 - acc: 0.9856 -- iter: 168/963\n",
            "Training Step: 23254  | total loss: \u001b[1m\u001b[32m0.04433\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 193 | loss: 0.04433 - acc: 0.9870 -- iter: 176/963\n",
            "Training Step: 23255  | total loss: \u001b[1m\u001b[32m0.03990\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 193 | loss: 0.03990 - acc: 0.9883 -- iter: 184/963\n",
            "Training Step: 23256  | total loss: \u001b[1m\u001b[32m0.03593\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 193 | loss: 0.03593 - acc: 0.9895 -- iter: 192/963\n",
            "Training Step: 23257  | total loss: \u001b[1m\u001b[32m0.03235\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 193 | loss: 0.03235 - acc: 0.9905 -- iter: 200/963\n",
            "Training Step: 23258  | total loss: \u001b[1m\u001b[32m0.02913\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 193 | loss: 0.02913 - acc: 0.9915 -- iter: 208/963\n",
            "Training Step: 23259  | total loss: \u001b[1m\u001b[32m0.02624\u001b[0m\u001b[0m | time: 0.103s\n",
            "| Adam | epoch: 193 | loss: 0.02624 - acc: 0.9923 -- iter: 216/963\n",
            "Training Step: 23260  | total loss: \u001b[1m\u001b[32m0.02364\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 193 | loss: 0.02364 - acc: 0.9931 -- iter: 224/963\n",
            "Training Step: 23261  | total loss: \u001b[1m\u001b[32m0.02131\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 193 | loss: 0.02131 - acc: 0.9938 -- iter: 232/963\n",
            "Training Step: 23262  | total loss: \u001b[1m\u001b[32m0.01921\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 193 | loss: 0.01921 - acc: 0.9944 -- iter: 240/963\n",
            "Training Step: 23263  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 193 | loss: 0.01729 - acc: 0.9950 -- iter: 248/963\n",
            "Training Step: 23264  | total loss: \u001b[1m\u001b[32m0.01560\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 193 | loss: 0.01560 - acc: 0.9955 -- iter: 256/963\n",
            "Training Step: 23265  | total loss: \u001b[1m\u001b[32m0.01407\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 193 | loss: 0.01407 - acc: 0.9959 -- iter: 264/963\n",
            "Training Step: 23266  | total loss: \u001b[1m\u001b[32m0.01271\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 193 | loss: 0.01271 - acc: 0.9963 -- iter: 272/963\n",
            "Training Step: 23267  | total loss: \u001b[1m\u001b[32m0.01147\u001b[0m\u001b[0m | time: 0.127s\n",
            "| Adam | epoch: 193 | loss: 0.01147 - acc: 0.9967 -- iter: 280/963\n",
            "Training Step: 23268  | total loss: \u001b[1m\u001b[32m0.01035\u001b[0m\u001b[0m | time: 0.130s\n",
            "| Adam | epoch: 193 | loss: 0.01035 - acc: 0.9970 -- iter: 288/963\n",
            "Training Step: 23269  | total loss: \u001b[1m\u001b[32m0.00932\u001b[0m\u001b[0m | time: 0.133s\n",
            "| Adam | epoch: 193 | loss: 0.00932 - acc: 0.9973 -- iter: 296/963\n",
            "Training Step: 23270  | total loss: \u001b[1m\u001b[32m0.00840\u001b[0m\u001b[0m | time: 0.135s\n",
            "| Adam | epoch: 193 | loss: 0.00840 - acc: 0.9976 -- iter: 304/963\n",
            "Training Step: 23271  | total loss: \u001b[1m\u001b[32m0.00757\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 193 | loss: 0.00757 - acc: 0.9978 -- iter: 312/963\n",
            "Training Step: 23272  | total loss: \u001b[1m\u001b[32m0.00683\u001b[0m\u001b[0m | time: 0.140s\n",
            "| Adam | epoch: 193 | loss: 0.00683 - acc: 0.9981 -- iter: 320/963\n",
            "Training Step: 23273  | total loss: \u001b[1m\u001b[32m0.00617\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 193 | loss: 0.00617 - acc: 0.9982 -- iter: 328/963\n",
            "Training Step: 23274  | total loss: \u001b[1m\u001b[32m0.00558\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 193 | loss: 0.00558 - acc: 0.9984 -- iter: 336/963\n",
            "Training Step: 23275  | total loss: \u001b[1m\u001b[32m0.00504\u001b[0m\u001b[0m | time: 0.149s\n",
            "| Adam | epoch: 193 | loss: 0.00504 - acc: 0.9986 -- iter: 344/963\n",
            "Training Step: 23276  | total loss: \u001b[1m\u001b[32m0.00456\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 193 | loss: 0.00456 - acc: 0.9987 -- iter: 352/963\n",
            "Training Step: 23277  | total loss: \u001b[1m\u001b[32m0.00411\u001b[0m\u001b[0m | time: 0.158s\n",
            "| Adam | epoch: 193 | loss: 0.00411 - acc: 0.9988 -- iter: 360/963\n",
            "Training Step: 23278  | total loss: \u001b[1m\u001b[32m0.00376\u001b[0m\u001b[0m | time: 0.161s\n",
            "| Adam | epoch: 193 | loss: 0.00376 - acc: 0.9990 -- iter: 368/963\n",
            "Training Step: 23279  | total loss: \u001b[1m\u001b[32m0.00342\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 193 | loss: 0.00342 - acc: 0.9991 -- iter: 376/963\n",
            "Training Step: 23280  | total loss: \u001b[1m\u001b[32m0.00309\u001b[0m\u001b[0m | time: 0.172s\n",
            "| Adam | epoch: 193 | loss: 0.00309 - acc: 0.9992 -- iter: 384/963\n",
            "Training Step: 23281  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.181s\n",
            "| Adam | epoch: 193 | loss: 0.00283 - acc: 0.9992 -- iter: 392/963\n",
            "Training Step: 23282  | total loss: \u001b[1m\u001b[32m0.00257\u001b[0m\u001b[0m | time: 0.188s\n",
            "| Adam | epoch: 193 | loss: 0.00257 - acc: 0.9993 -- iter: 400/963\n",
            "Training Step: 23283  | total loss: \u001b[1m\u001b[32m0.00233\u001b[0m\u001b[0m | time: 0.193s\n",
            "| Adam | epoch: 193 | loss: 0.00233 - acc: 0.9994 -- iter: 408/963\n",
            "Training Step: 23284  | total loss: \u001b[1m\u001b[32m0.00215\u001b[0m\u001b[0m | time: 0.200s\n",
            "| Adam | epoch: 193 | loss: 0.00215 - acc: 0.9994 -- iter: 416/963\n",
            "Training Step: 23285  | total loss: \u001b[1m\u001b[32m0.00195\u001b[0m\u001b[0m | time: 0.203s\n",
            "| Adam | epoch: 193 | loss: 0.00195 - acc: 0.9995 -- iter: 424/963\n",
            "Training Step: 23286  | total loss: \u001b[1m\u001b[32m0.00179\u001b[0m\u001b[0m | time: 0.207s\n",
            "| Adam | epoch: 193 | loss: 0.00179 - acc: 0.9996 -- iter: 432/963\n",
            "Training Step: 23287  | total loss: \u001b[1m\u001b[32m0.00162\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 193 | loss: 0.00162 - acc: 0.9996 -- iter: 440/963\n",
            "Training Step: 23288  | total loss: \u001b[1m\u001b[32m0.00153\u001b[0m\u001b[0m | time: 0.214s\n",
            "| Adam | epoch: 193 | loss: 0.00153 - acc: 0.9996 -- iter: 448/963\n",
            "Training Step: 23289  | total loss: \u001b[1m\u001b[32m0.00138\u001b[0m\u001b[0m | time: 0.218s\n",
            "| Adam | epoch: 193 | loss: 0.00138 - acc: 0.9997 -- iter: 456/963\n",
            "Training Step: 23290  | total loss: \u001b[1m\u001b[32m0.00125\u001b[0m\u001b[0m | time: 0.221s\n",
            "| Adam | epoch: 193 | loss: 0.00125 - acc: 0.9997 -- iter: 464/963\n",
            "Training Step: 23291  | total loss: \u001b[1m\u001b[32m0.00114\u001b[0m\u001b[0m | time: 0.224s\n",
            "| Adam | epoch: 193 | loss: 0.00114 - acc: 0.9997 -- iter: 472/963\n",
            "Training Step: 23292  | total loss: \u001b[1m\u001b[32m0.03487\u001b[0m\u001b[0m | time: 0.228s\n",
            "| Adam | epoch: 193 | loss: 0.03487 - acc: 0.9873 -- iter: 480/963\n",
            "Training Step: 23293  | total loss: \u001b[1m\u001b[32m0.03142\u001b[0m\u001b[0m | time: 0.231s\n",
            "| Adam | epoch: 193 | loss: 0.03142 - acc: 0.9885 -- iter: 488/963\n",
            "Training Step: 23294  | total loss: \u001b[1m\u001b[32m0.02838\u001b[0m\u001b[0m | time: 0.235s\n",
            "| Adam | epoch: 193 | loss: 0.02838 - acc: 0.9897 -- iter: 496/963\n",
            "Training Step: 23295  | total loss: \u001b[1m\u001b[32m0.02556\u001b[0m\u001b[0m | time: 0.238s\n",
            "| Adam | epoch: 193 | loss: 0.02556 - acc: 0.9907 -- iter: 504/963\n",
            "Training Step: 23296  | total loss: \u001b[1m\u001b[32m0.02303\u001b[0m\u001b[0m | time: 0.241s\n",
            "| Adam | epoch: 193 | loss: 0.02303 - acc: 0.9916 -- iter: 512/963\n",
            "Training Step: 23297  | total loss: \u001b[1m\u001b[32m0.02075\u001b[0m\u001b[0m | time: 0.244s\n",
            "| Adam | epoch: 193 | loss: 0.02075 - acc: 0.9925 -- iter: 520/963\n",
            "Training Step: 23298  | total loss: \u001b[1m\u001b[32m0.01868\u001b[0m\u001b[0m | time: 0.247s\n",
            "| Adam | epoch: 193 | loss: 0.01868 - acc: 0.9932 -- iter: 528/963\n",
            "Training Step: 23299  | total loss: \u001b[1m\u001b[32m0.01686\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 193 | loss: 0.01686 - acc: 0.9939 -- iter: 536/963\n",
            "Training Step: 23300  | total loss: \u001b[1m\u001b[32m0.01521\u001b[0m\u001b[0m | time: 0.255s\n",
            "| Adam | epoch: 193 | loss: 0.01521 - acc: 0.9945 -- iter: 544/963\n",
            "Training Step: 23301  | total loss: \u001b[1m\u001b[32m0.01372\u001b[0m\u001b[0m | time: 0.258s\n",
            "| Adam | epoch: 193 | loss: 0.01372 - acc: 0.9951 -- iter: 552/963\n",
            "Training Step: 23302  | total loss: \u001b[1m\u001b[32m0.01236\u001b[0m\u001b[0m | time: 0.262s\n",
            "| Adam | epoch: 193 | loss: 0.01236 - acc: 0.9956 -- iter: 560/963\n",
            "Training Step: 23303  | total loss: \u001b[1m\u001b[32m0.01113\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 193 | loss: 0.01113 - acc: 0.9960 -- iter: 568/963\n",
            "Training Step: 23304  | total loss: \u001b[1m\u001b[32m0.01003\u001b[0m\u001b[0m | time: 0.268s\n",
            "| Adam | epoch: 193 | loss: 0.01003 - acc: 0.9964 -- iter: 576/963\n",
            "Training Step: 23305  | total loss: \u001b[1m\u001b[32m0.02038\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 193 | loss: 0.02038 - acc: 0.9843 -- iter: 584/963\n",
            "Training Step: 23306  | total loss: \u001b[1m\u001b[32m0.01836\u001b[0m\u001b[0m | time: 0.275s\n",
            "| Adam | epoch: 193 | loss: 0.01836 - acc: 0.9858 -- iter: 592/963\n",
            "Training Step: 23307  | total loss: \u001b[1m\u001b[32m0.01655\u001b[0m\u001b[0m | time: 0.278s\n",
            "| Adam | epoch: 193 | loss: 0.01655 - acc: 0.9873 -- iter: 600/963\n",
            "Training Step: 23308  | total loss: \u001b[1m\u001b[32m0.03768\u001b[0m\u001b[0m | time: 0.282s\n",
            "| Adam | epoch: 193 | loss: 0.03768 - acc: 0.9760 -- iter: 608/963\n",
            "Training Step: 23309  | total loss: \u001b[1m\u001b[32m0.03394\u001b[0m\u001b[0m | time: 0.286s\n",
            "| Adam | epoch: 193 | loss: 0.03394 - acc: 0.9784 -- iter: 616/963\n",
            "Training Step: 23310  | total loss: \u001b[1m\u001b[32m0.03055\u001b[0m\u001b[0m | time: 0.289s\n",
            "| Adam | epoch: 193 | loss: 0.03055 - acc: 0.9806 -- iter: 624/963\n",
            "Training Step: 23311  | total loss: \u001b[1m\u001b[32m0.02751\u001b[0m\u001b[0m | time: 0.292s\n",
            "| Adam | epoch: 193 | loss: 0.02751 - acc: 0.9825 -- iter: 632/963\n",
            "Training Step: 23312  | total loss: \u001b[1m\u001b[32m0.02479\u001b[0m\u001b[0m | time: 0.294s\n",
            "| Adam | epoch: 193 | loss: 0.02479 - acc: 0.9843 -- iter: 640/963\n",
            "Training Step: 23313  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 0.297s\n",
            "| Adam | epoch: 193 | loss: 0.02233 - acc: 0.9858 -- iter: 648/963\n",
            "Training Step: 23314  | total loss: \u001b[1m\u001b[32m0.02011\u001b[0m\u001b[0m | time: 0.300s\n",
            "| Adam | epoch: 193 | loss: 0.02011 - acc: 0.9873 -- iter: 656/963\n",
            "Training Step: 23315  | total loss: \u001b[1m\u001b[32m0.01811\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 193 | loss: 0.01811 - acc: 0.9885 -- iter: 664/963\n",
            "Training Step: 23316  | total loss: \u001b[1m\u001b[32m0.01704\u001b[0m\u001b[0m | time: 0.307s\n",
            "| Adam | epoch: 193 | loss: 0.01704 - acc: 0.9897 -- iter: 672/963\n",
            "Training Step: 23317  | total loss: \u001b[1m\u001b[32m0.01535\u001b[0m\u001b[0m | time: 0.310s\n",
            "| Adam | epoch: 193 | loss: 0.01535 - acc: 0.9907 -- iter: 680/963\n",
            "Training Step: 23318  | total loss: \u001b[1m\u001b[32m0.01382\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 193 | loss: 0.01382 - acc: 0.9916 -- iter: 688/963\n",
            "Training Step: 23319  | total loss: \u001b[1m\u001b[32m0.01249\u001b[0m\u001b[0m | time: 0.316s\n",
            "| Adam | epoch: 193 | loss: 0.01249 - acc: 0.9925 -- iter: 696/963\n",
            "Training Step: 23320  | total loss: \u001b[1m\u001b[32m0.04465\u001b[0m\u001b[0m | time: 0.319s\n",
            "| Adam | epoch: 193 | loss: 0.04465 - acc: 0.9807 -- iter: 704/963\n",
            "Training Step: 23321  | total loss: \u001b[1m\u001b[32m0.05158\u001b[0m\u001b[0m | time: 0.322s\n",
            "| Adam | epoch: 193 | loss: 0.05158 - acc: 0.9702 -- iter: 712/963\n",
            "Training Step: 23322  | total loss: \u001b[1m\u001b[32m0.04796\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 193 | loss: 0.04796 - acc: 0.9731 -- iter: 720/963\n",
            "Training Step: 23323  | total loss: \u001b[1m\u001b[32m0.04318\u001b[0m\u001b[0m | time: 0.328s\n",
            "| Adam | epoch: 193 | loss: 0.04318 - acc: 0.9758 -- iter: 728/963\n",
            "Training Step: 23324  | total loss: \u001b[1m\u001b[32m0.03888\u001b[0m\u001b[0m | time: 0.331s\n",
            "| Adam | epoch: 193 | loss: 0.03888 - acc: 0.9782 -- iter: 736/963\n",
            "Training Step: 23325  | total loss: \u001b[1m\u001b[32m0.03500\u001b[0m\u001b[0m | time: 0.334s\n",
            "| Adam | epoch: 193 | loss: 0.03500 - acc: 0.9804 -- iter: 744/963\n",
            "Training Step: 23326  | total loss: \u001b[1m\u001b[32m0.03153\u001b[0m\u001b[0m | time: 0.339s\n",
            "| Adam | epoch: 193 | loss: 0.03153 - acc: 0.9824 -- iter: 752/963\n",
            "Training Step: 23327  | total loss: \u001b[1m\u001b[32m0.03183\u001b[0m\u001b[0m | time: 0.343s\n",
            "| Adam | epoch: 193 | loss: 0.03183 - acc: 0.9841 -- iter: 760/963\n",
            "Training Step: 23328  | total loss: \u001b[1m\u001b[32m0.02866\u001b[0m\u001b[0m | time: 0.346s\n",
            "| Adam | epoch: 193 | loss: 0.02866 - acc: 0.9857 -- iter: 768/963\n",
            "Training Step: 23329  | total loss: \u001b[1m\u001b[32m0.02582\u001b[0m\u001b[0m | time: 0.349s\n",
            "| Adam | epoch: 193 | loss: 0.02582 - acc: 0.9872 -- iter: 776/963\n",
            "Training Step: 23330  | total loss: \u001b[1m\u001b[32m0.02325\u001b[0m\u001b[0m | time: 0.352s\n",
            "| Adam | epoch: 193 | loss: 0.02325 - acc: 0.9884 -- iter: 784/963\n",
            "Training Step: 23331  | total loss: \u001b[1m\u001b[32m0.04657\u001b[0m\u001b[0m | time: 0.355s\n",
            "| Adam | epoch: 193 | loss: 0.04657 - acc: 0.9771 -- iter: 792/963\n",
            "Training Step: 23332  | total loss: \u001b[1m\u001b[32m0.04600\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 193 | loss: 0.04600 - acc: 0.9794 -- iter: 800/963\n",
            "Training Step: 23333  | total loss: \u001b[1m\u001b[32m0.04152\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 193 | loss: 0.04152 - acc: 0.9814 -- iter: 808/963\n",
            "Training Step: 23334  | total loss: \u001b[1m\u001b[32m0.03739\u001b[0m\u001b[0m | time: 0.364s\n",
            "| Adam | epoch: 193 | loss: 0.03739 - acc: 0.9833 -- iter: 816/963\n",
            "Training Step: 23335  | total loss: \u001b[1m\u001b[32m0.03587\u001b[0m\u001b[0m | time: 0.367s\n",
            "| Adam | epoch: 193 | loss: 0.03587 - acc: 0.9850 -- iter: 824/963\n",
            "Training Step: 23336  | total loss: \u001b[1m\u001b[32m0.03232\u001b[0m\u001b[0m | time: 0.370s\n",
            "| Adam | epoch: 193 | loss: 0.03232 - acc: 0.9865 -- iter: 832/963\n",
            "Training Step: 23337  | total loss: \u001b[1m\u001b[32m0.02909\u001b[0m\u001b[0m | time: 0.373s\n",
            "| Adam | epoch: 193 | loss: 0.02909 - acc: 0.9878 -- iter: 840/963\n",
            "Training Step: 23338  | total loss: \u001b[1m\u001b[32m0.02620\u001b[0m\u001b[0m | time: 0.378s\n",
            "| Adam | epoch: 193 | loss: 0.02620 - acc: 0.9890 -- iter: 848/963\n",
            "Training Step: 23339  | total loss: \u001b[1m\u001b[32m0.02361\u001b[0m\u001b[0m | time: 0.382s\n",
            "| Adam | epoch: 193 | loss: 0.02361 - acc: 0.9901 -- iter: 856/963\n",
            "Training Step: 23340  | total loss: \u001b[1m\u001b[32m0.02126\u001b[0m\u001b[0m | time: 0.387s\n",
            "| Adam | epoch: 193 | loss: 0.02126 - acc: 0.9911 -- iter: 864/963\n",
            "Training Step: 23341  | total loss: \u001b[1m\u001b[32m0.01914\u001b[0m\u001b[0m | time: 0.393s\n",
            "| Adam | epoch: 193 | loss: 0.01914 - acc: 0.9920 -- iter: 872/963\n",
            "Training Step: 23342  | total loss: \u001b[1m\u001b[32m0.02577\u001b[0m\u001b[0m | time: 0.399s\n",
            "| Adam | epoch: 193 | loss: 0.02577 - acc: 0.9928 -- iter: 880/963\n",
            "Training Step: 23343  | total loss: \u001b[1m\u001b[32m0.02322\u001b[0m\u001b[0m | time: 0.403s\n",
            "| Adam | epoch: 193 | loss: 0.02322 - acc: 0.9935 -- iter: 888/963\n",
            "Training Step: 23344  | total loss: \u001b[1m\u001b[32m0.02092\u001b[0m\u001b[0m | time: 0.407s\n",
            "| Adam | epoch: 193 | loss: 0.02092 - acc: 0.9942 -- iter: 896/963\n",
            "Training Step: 23345  | total loss: \u001b[1m\u001b[32m0.01920\u001b[0m\u001b[0m | time: 0.411s\n",
            "| Adam | epoch: 193 | loss: 0.01920 - acc: 0.9948 -- iter: 904/963\n",
            "Training Step: 23346  | total loss: \u001b[1m\u001b[32m0.01729\u001b[0m\u001b[0m | time: 0.414s\n",
            "| Adam | epoch: 193 | loss: 0.01729 - acc: 0.9953 -- iter: 912/963\n",
            "Training Step: 23347  | total loss: \u001b[1m\u001b[32m0.01561\u001b[0m\u001b[0m | time: 0.417s\n",
            "| Adam | epoch: 193 | loss: 0.01561 - acc: 0.9958 -- iter: 920/963\n",
            "Training Step: 23348  | total loss: \u001b[1m\u001b[32m0.01406\u001b[0m\u001b[0m | time: 0.420s\n",
            "| Adam | epoch: 193 | loss: 0.01406 - acc: 0.9962 -- iter: 928/963\n",
            "Training Step: 23349  | total loss: \u001b[1m\u001b[32m0.01267\u001b[0m\u001b[0m | time: 0.424s\n",
            "| Adam | epoch: 193 | loss: 0.01267 - acc: 0.9966 -- iter: 936/963\n",
            "Training Step: 23350  | total loss: \u001b[1m\u001b[32m0.01143\u001b[0m\u001b[0m | time: 0.427s\n",
            "| Adam | epoch: 193 | loss: 0.01143 - acc: 0.9969 -- iter: 944/963\n",
            "Training Step: 23351  | total loss: \u001b[1m\u001b[32m0.01031\u001b[0m\u001b[0m | time: 0.431s\n",
            "| Adam | epoch: 193 | loss: 0.01031 - acc: 0.9972 -- iter: 952/963\n",
            "Training Step: 23352  | total loss: \u001b[1m\u001b[32m0.00929\u001b[0m\u001b[0m | time: 0.435s\n",
            "| Adam | epoch: 193 | loss: 0.00929 - acc: 0.9975 -- iter: 960/963\n",
            "Training Step: 23353  | total loss: \u001b[1m\u001b[32m0.00837\u001b[0m\u001b[0m | time: 0.438s\n",
            "| Adam | epoch: 193 | loss: 0.00837 - acc: 0.9977 -- iter: 963/963\n",
            "--\n",
            "Training Step: 23354  | total loss: \u001b[1m\u001b[32m0.00755\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 194 | loss: 0.00755 - acc: 0.9980 -- iter: 008/963\n",
            "Training Step: 23355  | total loss: \u001b[1m\u001b[32m0.00681\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 194 | loss: 0.00681 - acc: 0.9982 -- iter: 016/963\n",
            "Training Step: 23356  | total loss: \u001b[1m\u001b[32m0.00614\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 194 | loss: 0.00614 - acc: 0.9984 -- iter: 024/963\n",
            "Training Step: 23357  | total loss: \u001b[1m\u001b[32m0.03944\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 194 | loss: 0.03944 - acc: 0.9860 -- iter: 032/963\n",
            "Training Step: 23358  | total loss: \u001b[1m\u001b[32m0.03551\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 194 | loss: 0.03551 - acc: 0.9874 -- iter: 040/963\n",
            "Training Step: 23359  | total loss: \u001b[1m\u001b[32m0.03198\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 194 | loss: 0.03198 - acc: 0.9887 -- iter: 048/963\n",
            "Training Step: 23360  | total loss: \u001b[1m\u001b[32m0.04031\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 194 | loss: 0.04031 - acc: 0.9773 -- iter: 056/963\n",
            "Training Step: 23361  | total loss: \u001b[1m\u001b[32m0.03631\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 194 | loss: 0.03631 - acc: 0.9796 -- iter: 064/963\n",
            "Training Step: 23362  | total loss: \u001b[1m\u001b[32m0.03269\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 194 | loss: 0.03269 - acc: 0.9816 -- iter: 072/963\n",
            "Training Step: 23363  | total loss: \u001b[1m\u001b[32m0.02945\u001b[0m\u001b[0m | time: 0.029s\n",
            "| Adam | epoch: 194 | loss: 0.02945 - acc: 0.9835 -- iter: 080/963\n",
            "Training Step: 23364  | total loss: \u001b[1m\u001b[32m0.02653\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 194 | loss: 0.02653 - acc: 0.9851 -- iter: 088/963\n",
            "Training Step: 23365  | total loss: \u001b[1m\u001b[32m0.02391\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 194 | loss: 0.02391 - acc: 0.9866 -- iter: 096/963\n",
            "Training Step: 23366  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 194 | loss: 0.02154 - acc: 0.9879 -- iter: 104/963\n",
            "Training Step: 23367  | total loss: \u001b[1m\u001b[32m0.01942\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 194 | loss: 0.01942 - acc: 0.9891 -- iter: 112/963\n",
            "Training Step: 23368  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 194 | loss: 0.01750 - acc: 0.9902 -- iter: 120/963\n",
            "Training Step: 23369  | total loss: \u001b[1m\u001b[32m0.01577\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 194 | loss: 0.01577 - acc: 0.9912 -- iter: 128/963\n",
            "Training Step: 23370  | total loss: \u001b[1m\u001b[32m0.01421\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 194 | loss: 0.01421 - acc: 0.9921 -- iter: 136/963\n",
            "Training Step: 23371  | total loss: \u001b[1m\u001b[32m0.03180\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 194 | loss: 0.03180 - acc: 0.9804 -- iter: 144/963\n",
            "Training Step: 23372  | total loss: \u001b[1m\u001b[32m0.05975\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 194 | loss: 0.05975 - acc: 0.9698 -- iter: 152/963\n",
            "Training Step: 23373  | total loss: \u001b[1m\u001b[32m0.05394\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 194 | loss: 0.05394 - acc: 0.9729 -- iter: 160/963\n",
            "Training Step: 23374  | total loss: \u001b[1m\u001b[32m0.04855\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 194 | loss: 0.04855 - acc: 0.9756 -- iter: 168/963\n",
            "Training Step: 23375  | total loss: \u001b[1m\u001b[32m0.04371\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 194 | loss: 0.04371 - acc: 0.9780 -- iter: 176/963\n",
            "Training Step: 23376  | total loss: \u001b[1m\u001b[32m0.03939\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 194 | loss: 0.03939 - acc: 0.9802 -- iter: 184/963\n",
            "Training Step: 23377  | total loss: \u001b[1m\u001b[32m0.03547\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 194 | loss: 0.03547 - acc: 0.9822 -- iter: 192/963\n",
            "Training Step: 23378  | total loss: \u001b[1m\u001b[32m0.03194\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 194 | loss: 0.03194 - acc: 0.9840 -- iter: 200/963\n",
            "Training Step: 23379  | total loss: \u001b[1m\u001b[32m0.02875\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 194 | loss: 0.02875 - acc: 0.9856 -- iter: 208/963\n",
            "Training Step: 23380  | total loss: \u001b[1m\u001b[32m0.02966\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 194 | loss: 0.02966 - acc: 0.9870 -- iter: 216/963\n",
            "Training Step: 23381  | total loss: \u001b[1m\u001b[32m0.02670\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 194 | loss: 0.02670 - acc: 0.9883 -- iter: 224/963\n",
            "Training Step: 23382  | total loss: \u001b[1m\u001b[32m0.02404\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 194 | loss: 0.02404 - acc: 0.9895 -- iter: 232/963\n",
            "Training Step: 23383  | total loss: \u001b[1m\u001b[32m0.02164\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 194 | loss: 0.02164 - acc: 0.9905 -- iter: 240/963\n",
            "Training Step: 23384  | total loss: \u001b[1m\u001b[32m0.01951\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 194 | loss: 0.01951 - acc: 0.9915 -- iter: 248/963\n",
            "Training Step: 23385  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 194 | loss: 0.01757 - acc: 0.9923 -- iter: 256/963\n",
            "Training Step: 23386  | total loss: \u001b[1m\u001b[32m0.01585\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 194 | loss: 0.01585 - acc: 0.9931 -- iter: 264/963\n",
            "Training Step: 23387  | total loss: \u001b[1m\u001b[32m0.01427\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 194 | loss: 0.01427 - acc: 0.9938 -- iter: 272/963\n",
            "Training Step: 23388  | total loss: \u001b[1m\u001b[32m0.01285\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 194 | loss: 0.01285 - acc: 0.9944 -- iter: 280/963\n",
            "Training Step: 23389  | total loss: \u001b[1m\u001b[32m0.01159\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 194 | loss: 0.01159 - acc: 0.9950 -- iter: 288/963\n",
            "Training Step: 23390  | total loss: \u001b[1m\u001b[32m0.01044\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 194 | loss: 0.01044 - acc: 0.9955 -- iter: 296/963\n",
            "Training Step: 23391  | total loss: \u001b[1m\u001b[32m0.00942\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 194 | loss: 0.00942 - acc: 0.9959 -- iter: 304/963\n",
            "Training Step: 23392  | total loss: \u001b[1m\u001b[32m0.00850\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 194 | loss: 0.00850 - acc: 0.9963 -- iter: 312/963\n",
            "Training Step: 23393  | total loss: \u001b[1m\u001b[32m0.00795\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 194 | loss: 0.00795 - acc: 0.9967 -- iter: 320/963\n",
            "Training Step: 23394  | total loss: \u001b[1m\u001b[32m0.00990\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 194 | loss: 0.00990 - acc: 0.9970 -- iter: 328/963\n",
            "Training Step: 23395  | total loss: \u001b[1m\u001b[32m0.00894\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 194 | loss: 0.00894 - acc: 0.9973 -- iter: 336/963\n",
            "Training Step: 23396  | total loss: \u001b[1m\u001b[32m0.00808\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 194 | loss: 0.00808 - acc: 0.9976 -- iter: 344/963\n",
            "Training Step: 23397  | total loss: \u001b[1m\u001b[32m0.00729\u001b[0m\u001b[0m | time: 0.126s\n",
            "| Adam | epoch: 194 | loss: 0.00729 - acc: 0.9978 -- iter: 352/963\n",
            "Training Step: 23398  | total loss: \u001b[1m\u001b[32m0.00657\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 194 | loss: 0.00657 - acc: 0.9981 -- iter: 360/963\n",
            "Training Step: 23399  | total loss: \u001b[1m\u001b[32m0.00596\u001b[0m\u001b[0m | time: 0.131s\n",
            "| Adam | epoch: 194 | loss: 0.00596 - acc: 0.9982 -- iter: 368/963\n",
            "Training Step: 23400  | total loss: \u001b[1m\u001b[32m0.00565\u001b[0m\u001b[0m | time: 0.133s\n",
            "| Adam | epoch: 194 | loss: 0.00565 - acc: 0.9984 -- iter: 376/963\n",
            "Training Step: 23401  | total loss: \u001b[1m\u001b[32m0.00510\u001b[0m\u001b[0m | time: 0.136s\n",
            "| Adam | epoch: 194 | loss: 0.00510 - acc: 0.9986 -- iter: 384/963\n",
            "Training Step: 23402  | total loss: \u001b[1m\u001b[32m0.00461\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 194 | loss: 0.00461 - acc: 0.9987 -- iter: 392/963\n",
            "Training Step: 23403  | total loss: \u001b[1m\u001b[32m0.00429\u001b[0m\u001b[0m | time: 0.141s\n",
            "| Adam | epoch: 194 | loss: 0.00429 - acc: 0.9988 -- iter: 400/963\n",
            "Training Step: 23404  | total loss: \u001b[1m\u001b[32m0.00389\u001b[0m\u001b[0m | time: 0.143s\n",
            "| Adam | epoch: 194 | loss: 0.00389 - acc: 0.9990 -- iter: 408/963\n",
            "Training Step: 23405  | total loss: \u001b[1m\u001b[32m0.00350\u001b[0m\u001b[0m | time: 0.146s\n",
            "| Adam | epoch: 194 | loss: 0.00350 - acc: 0.9991 -- iter: 416/963\n",
            "Training Step: 23406  | total loss: \u001b[1m\u001b[32m0.00316\u001b[0m\u001b[0m | time: 0.148s\n",
            "| Adam | epoch: 194 | loss: 0.00316 - acc: 0.9992 -- iter: 424/963\n",
            "Training Step: 23407  | total loss: \u001b[1m\u001b[32m0.00287\u001b[0m\u001b[0m | time: 0.151s\n",
            "| Adam | epoch: 194 | loss: 0.00287 - acc: 0.9992 -- iter: 432/963\n",
            "Training Step: 23408  | total loss: \u001b[1m\u001b[32m0.00259\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 194 | loss: 0.00259 - acc: 0.9993 -- iter: 440/963\n",
            "Training Step: 23409  | total loss: \u001b[1m\u001b[32m0.00370\u001b[0m\u001b[0m | time: 0.156s\n",
            "| Adam | epoch: 194 | loss: 0.00370 - acc: 0.9994 -- iter: 448/963\n",
            "Training Step: 23410  | total loss: \u001b[1m\u001b[32m0.00334\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 194 | loss: 0.00334 - acc: 0.9994 -- iter: 456/963\n",
            "Training Step: 23411  | total loss: \u001b[1m\u001b[32m0.00302\u001b[0m\u001b[0m | time: 0.161s\n",
            "| Adam | epoch: 194 | loss: 0.00302 - acc: 0.9995 -- iter: 464/963\n",
            "Training Step: 23412  | total loss: \u001b[1m\u001b[32m0.00273\u001b[0m\u001b[0m | time: 0.164s\n",
            "| Adam | epoch: 194 | loss: 0.00273 - acc: 0.9996 -- iter: 472/963\n",
            "Training Step: 23413  | total loss: \u001b[1m\u001b[32m0.00246\u001b[0m\u001b[0m | time: 0.166s\n",
            "| Adam | epoch: 194 | loss: 0.00246 - acc: 0.9996 -- iter: 480/963\n",
            "Training Step: 23414  | total loss: \u001b[1m\u001b[32m0.00227\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 194 | loss: 0.00227 - acc: 0.9996 -- iter: 488/963\n",
            "Training Step: 23415  | total loss: \u001b[1m\u001b[32m0.00676\u001b[0m\u001b[0m | time: 0.171s\n",
            "| Adam | epoch: 194 | loss: 0.00676 - acc: 0.9997 -- iter: 496/963\n",
            "Training Step: 23416  | total loss: \u001b[1m\u001b[32m0.00609\u001b[0m\u001b[0m | time: 0.174s\n",
            "| Adam | epoch: 194 | loss: 0.00609 - acc: 0.9997 -- iter: 504/963\n",
            "Training Step: 23417  | total loss: \u001b[1m\u001b[32m0.00549\u001b[0m\u001b[0m | time: 0.177s\n",
            "| Adam | epoch: 194 | loss: 0.00549 - acc: 0.9997 -- iter: 512/963\n",
            "Training Step: 23418  | total loss: \u001b[1m\u001b[32m0.03770\u001b[0m\u001b[0m | time: 0.179s\n",
            "| Adam | epoch: 194 | loss: 0.03770 - acc: 0.9873 -- iter: 520/963\n",
            "Training Step: 23419  | total loss: \u001b[1m\u001b[32m0.03396\u001b[0m\u001b[0m | time: 0.182s\n",
            "| Adam | epoch: 194 | loss: 0.03396 - acc: 0.9885 -- iter: 528/963\n",
            "Training Step: 23420  | total loss: \u001b[1m\u001b[32m0.03058\u001b[0m\u001b[0m | time: 0.184s\n",
            "| Adam | epoch: 194 | loss: 0.03058 - acc: 0.9897 -- iter: 536/963\n",
            "Training Step: 23421  | total loss: \u001b[1m\u001b[32m0.02756\u001b[0m\u001b[0m | time: 0.187s\n",
            "| Adam | epoch: 194 | loss: 0.02756 - acc: 0.9907 -- iter: 544/963\n",
            "Training Step: 23422  | total loss: \u001b[1m\u001b[32m0.02567\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 194 | loss: 0.02567 - acc: 0.9916 -- iter: 552/963\n",
            "Training Step: 23423  | total loss: \u001b[1m\u001b[32m0.02311\u001b[0m\u001b[0m | time: 0.192s\n",
            "| Adam | epoch: 194 | loss: 0.02311 - acc: 0.9925 -- iter: 560/963\n",
            "Training Step: 23424  | total loss: \u001b[1m\u001b[32m0.02083\u001b[0m\u001b[0m | time: 0.195s\n",
            "| Adam | epoch: 194 | loss: 0.02083 - acc: 0.9932 -- iter: 568/963\n",
            "Training Step: 23425  | total loss: \u001b[1m\u001b[32m0.01876\u001b[0m\u001b[0m | time: 0.198s\n",
            "| Adam | epoch: 194 | loss: 0.01876 - acc: 0.9939 -- iter: 576/963\n",
            "Training Step: 23426  | total loss: \u001b[1m\u001b[32m0.01690\u001b[0m\u001b[0m | time: 0.200s\n",
            "| Adam | epoch: 194 | loss: 0.01690 - acc: 0.9945 -- iter: 584/963\n",
            "Training Step: 23427  | total loss: \u001b[1m\u001b[32m0.01612\u001b[0m\u001b[0m | time: 0.203s\n",
            "| Adam | epoch: 194 | loss: 0.01612 - acc: 0.9951 -- iter: 592/963\n",
            "Training Step: 23428  | total loss: \u001b[1m\u001b[32m0.01455\u001b[0m\u001b[0m | time: 0.205s\n",
            "| Adam | epoch: 194 | loss: 0.01455 - acc: 0.9956 -- iter: 600/963\n",
            "Training Step: 23429  | total loss: \u001b[1m\u001b[32m0.01310\u001b[0m\u001b[0m | time: 0.208s\n",
            "| Adam | epoch: 194 | loss: 0.01310 - acc: 0.9960 -- iter: 608/963\n",
            "Training Step: 23430  | total loss: \u001b[1m\u001b[32m0.01180\u001b[0m\u001b[0m | time: 0.210s\n",
            "| Adam | epoch: 194 | loss: 0.01180 - acc: 0.9964 -- iter: 616/963\n",
            "Training Step: 23431  | total loss: \u001b[1m\u001b[32m0.01065\u001b[0m\u001b[0m | time: 0.214s\n",
            "| Adam | epoch: 194 | loss: 0.01065 - acc: 0.9968 -- iter: 624/963\n",
            "Training Step: 23432  | total loss: \u001b[1m\u001b[32m0.00964\u001b[0m\u001b[0m | time: 0.216s\n",
            "| Adam | epoch: 194 | loss: 0.00964 - acc: 0.9971 -- iter: 632/963\n",
            "Training Step: 23433  | total loss: \u001b[1m\u001b[32m0.00868\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 194 | loss: 0.00868 - acc: 0.9974 -- iter: 640/963\n",
            "Training Step: 23434  | total loss: \u001b[1m\u001b[32m0.03600\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 194 | loss: 0.03600 - acc: 0.9851 -- iter: 648/963\n",
            "Training Step: 23435  | total loss: \u001b[1m\u001b[32m0.03241\u001b[0m\u001b[0m | time: 0.226s\n",
            "| Adam | epoch: 194 | loss: 0.03241 - acc: 0.9866 -- iter: 656/963\n",
            "Training Step: 23436  | total loss: \u001b[1m\u001b[32m0.02918\u001b[0m\u001b[0m | time: 0.230s\n",
            "| Adam | epoch: 194 | loss: 0.02918 - acc: 0.9880 -- iter: 664/963\n",
            "Training Step: 23437  | total loss: \u001b[1m\u001b[32m0.05825\u001b[0m\u001b[0m | time: 0.233s\n",
            "| Adam | epoch: 194 | loss: 0.05825 - acc: 0.9767 -- iter: 672/963\n",
            "Training Step: 23438  | total loss: \u001b[1m\u001b[32m0.05244\u001b[0m\u001b[0m | time: 0.237s\n",
            "| Adam | epoch: 194 | loss: 0.05244 - acc: 0.9790 -- iter: 680/963\n",
            "Training Step: 23439  | total loss: \u001b[1m\u001b[32m0.04721\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 194 | loss: 0.04721 - acc: 0.9811 -- iter: 688/963\n",
            "Training Step: 23440  | total loss: \u001b[1m\u001b[32m0.04249\u001b[0m\u001b[0m | time: 0.243s\n",
            "| Adam | epoch: 194 | loss: 0.04249 - acc: 0.9830 -- iter: 696/963\n",
            "Training Step: 23441  | total loss: \u001b[1m\u001b[32m0.03826\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 194 | loss: 0.03826 - acc: 0.9847 -- iter: 704/963\n",
            "Training Step: 23442  | total loss: \u001b[1m\u001b[32m0.03446\u001b[0m\u001b[0m | time: 0.249s\n",
            "| Adam | epoch: 194 | loss: 0.03446 - acc: 0.9862 -- iter: 712/963\n",
            "Training Step: 23443  | total loss: \u001b[1m\u001b[32m0.03103\u001b[0m\u001b[0m | time: 0.251s\n",
            "| Adam | epoch: 194 | loss: 0.03103 - acc: 0.9876 -- iter: 720/963\n",
            "Training Step: 23444  | total loss: \u001b[1m\u001b[32m0.02794\u001b[0m\u001b[0m | time: 0.256s\n",
            "| Adam | epoch: 194 | loss: 0.02794 - acc: 0.9888 -- iter: 728/963\n",
            "Training Step: 23445  | total loss: \u001b[1m\u001b[32m0.02517\u001b[0m\u001b[0m | time: 0.259s\n",
            "| Adam | epoch: 194 | loss: 0.02517 - acc: 0.9900 -- iter: 736/963\n",
            "Training Step: 23446  | total loss: \u001b[1m\u001b[32m0.02267\u001b[0m\u001b[0m | time: 0.262s\n",
            "| Adam | epoch: 194 | loss: 0.02267 - acc: 0.9910 -- iter: 744/963\n",
            "Training Step: 23447  | total loss: \u001b[1m\u001b[32m0.02044\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 194 | loss: 0.02044 - acc: 0.9919 -- iter: 752/963\n",
            "Training Step: 23448  | total loss: \u001b[1m\u001b[32m0.02486\u001b[0m\u001b[0m | time: 0.267s\n",
            "| Adam | epoch: 194 | loss: 0.02486 - acc: 0.9927 -- iter: 760/963\n",
            "Training Step: 23449  | total loss: \u001b[1m\u001b[32m0.02238\u001b[0m\u001b[0m | time: 0.270s\n",
            "| Adam | epoch: 194 | loss: 0.02238 - acc: 0.9934 -- iter: 768/963\n",
            "Training Step: 23450  | total loss: \u001b[1m\u001b[32m0.02017\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 194 | loss: 0.02017 - acc: 0.9941 -- iter: 776/963\n",
            "Training Step: 23451  | total loss: \u001b[1m\u001b[32m0.01819\u001b[0m\u001b[0m | time: 0.275s\n",
            "| Adam | epoch: 194 | loss: 0.01819 - acc: 0.9947 -- iter: 784/963\n",
            "Training Step: 23452  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.277s\n",
            "| Adam | epoch: 194 | loss: 0.01639 - acc: 0.9957 -- iter: 792/963\n",
            "Training Step: 23453  | total loss: \u001b[1m\u001b[32m0.01477\u001b[0m\u001b[0m | time: 0.279s\n",
            "| Adam | epoch: 194 | loss: 0.01477 - acc: 0.9957 -- iter: 800/963\n",
            "Training Step: 23454  | total loss: \u001b[1m\u001b[32m0.01345\u001b[0m\u001b[0m | time: 0.281s\n",
            "| Adam | epoch: 194 | loss: 0.01345 - acc: 0.9961 -- iter: 808/963\n",
            "Training Step: 23455  | total loss: \u001b[1m\u001b[32m0.01211\u001b[0m\u001b[0m | time: 0.284s\n",
            "| Adam | epoch: 194 | loss: 0.01211 - acc: 0.9965 -- iter: 816/963\n",
            "Training Step: 23456  | total loss: \u001b[1m\u001b[32m0.01093\u001b[0m\u001b[0m | time: 0.286s\n",
            "| Adam | epoch: 194 | loss: 0.01093 - acc: 0.9968 -- iter: 824/963\n",
            "Training Step: 23457  | total loss: \u001b[1m\u001b[32m0.00986\u001b[0m\u001b[0m | time: 0.288s\n",
            "| Adam | epoch: 194 | loss: 0.00986 - acc: 0.9972 -- iter: 832/963\n",
            "Training Step: 23458  | total loss: \u001b[1m\u001b[32m0.01854\u001b[0m\u001b[0m | time: 0.290s\n",
            "| Adam | epoch: 194 | loss: 0.01854 - acc: 0.9849 -- iter: 840/963\n",
            "Training Step: 23459  | total loss: \u001b[1m\u001b[32m0.01670\u001b[0m\u001b[0m | time: 0.293s\n",
            "| Adam | epoch: 194 | loss: 0.01670 - acc: 0.9865 -- iter: 848/963\n",
            "Training Step: 23460  | total loss: \u001b[1m\u001b[32m0.01507\u001b[0m\u001b[0m | time: 0.295s\n",
            "| Adam | epoch: 194 | loss: 0.01507 - acc: 0.9878 -- iter: 856/963\n",
            "Training Step: 23461  | total loss: \u001b[1m\u001b[32m0.01358\u001b[0m\u001b[0m | time: 0.298s\n",
            "| Adam | epoch: 194 | loss: 0.01358 - acc: 0.9890 -- iter: 864/963\n",
            "Training Step: 23462  | total loss: \u001b[1m\u001b[32m0.01224\u001b[0m\u001b[0m | time: 0.300s\n",
            "| Adam | epoch: 194 | loss: 0.01224 - acc: 0.9901 -- iter: 872/963\n",
            "Training Step: 23463  | total loss: \u001b[1m\u001b[32m0.04928\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 194 | loss: 0.04928 - acc: 0.9786 -- iter: 880/963\n",
            "Training Step: 23464  | total loss: \u001b[1m\u001b[32m0.04436\u001b[0m\u001b[0m | time: 0.305s\n",
            "| Adam | epoch: 194 | loss: 0.04436 - acc: 0.9808 -- iter: 888/963\n",
            "Training Step: 23465  | total loss: \u001b[1m\u001b[32m0.04004\u001b[0m\u001b[0m | time: 0.307s\n",
            "| Adam | epoch: 194 | loss: 0.04004 - acc: 0.9827 -- iter: 896/963\n",
            "Training Step: 23466  | total loss: \u001b[1m\u001b[32m0.03615\u001b[0m\u001b[0m | time: 0.310s\n",
            "| Adam | epoch: 194 | loss: 0.03615 - acc: 0.9844 -- iter: 904/963\n",
            "Training Step: 23467  | total loss: \u001b[1m\u001b[32m0.03256\u001b[0m\u001b[0m | time: 0.312s\n",
            "| Adam | epoch: 194 | loss: 0.03256 - acc: 0.9860 -- iter: 912/963\n",
            "Training Step: 23468  | total loss: \u001b[1m\u001b[32m0.03861\u001b[0m\u001b[0m | time: 0.314s\n",
            "| Adam | epoch: 194 | loss: 0.03861 - acc: 0.9749 -- iter: 920/963\n",
            "Training Step: 23469  | total loss: \u001b[1m\u001b[32m0.06159\u001b[0m\u001b[0m | time: 0.317s\n",
            "| Adam | epoch: 194 | loss: 0.06159 - acc: 0.9649 -- iter: 928/963\n",
            "Training Step: 23470  | total loss: \u001b[1m\u001b[32m0.07590\u001b[0m\u001b[0m | time: 0.319s\n",
            "| Adam | epoch: 194 | loss: 0.07590 - acc: 0.9559 -- iter: 936/963\n",
            "Training Step: 23471  | total loss: \u001b[1m\u001b[32m0.06840\u001b[0m\u001b[0m | time: 0.322s\n",
            "| Adam | epoch: 194 | loss: 0.06840 - acc: 0.9603 -- iter: 944/963\n",
            "Training Step: 23472  | total loss: \u001b[1m\u001b[32m0.06158\u001b[0m\u001b[0m | time: 0.324s\n",
            "| Adam | epoch: 194 | loss: 0.06158 - acc: 0.9643 -- iter: 952/963\n",
            "Training Step: 23473  | total loss: \u001b[1m\u001b[32m0.05545\u001b[0m\u001b[0m | time: 0.326s\n",
            "| Adam | epoch: 194 | loss: 0.05545 - acc: 0.9678 -- iter: 960/963\n",
            "Training Step: 23474  | total loss: \u001b[1m\u001b[32m0.04991\u001b[0m\u001b[0m | time: 0.328s\n",
            "| Adam | epoch: 194 | loss: 0.04991 - acc: 0.9711 -- iter: 963/963\n",
            "--\n",
            "Training Step: 23475  | total loss: \u001b[1m\u001b[32m0.04495\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 195 | loss: 0.04495 - acc: 0.9740 -- iter: 008/963\n",
            "Training Step: 23476  | total loss: \u001b[1m\u001b[32m0.04047\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 195 | loss: 0.04047 - acc: 0.9766 -- iter: 016/963\n",
            "Training Step: 23477  | total loss: \u001b[1m\u001b[32m0.03648\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 195 | loss: 0.03648 - acc: 0.9789 -- iter: 024/963\n",
            "Training Step: 23478  | total loss: \u001b[1m\u001b[32m0.03284\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 195 | loss: 0.03284 - acc: 0.9810 -- iter: 032/963\n",
            "Training Step: 23479  | total loss: \u001b[1m\u001b[32m0.03201\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 195 | loss: 0.03201 - acc: 0.9829 -- iter: 040/963\n",
            "Training Step: 23480  | total loss: \u001b[1m\u001b[32m0.02882\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 195 | loss: 0.02882 - acc: 0.9846 -- iter: 048/963\n",
            "Training Step: 23481  | total loss: \u001b[1m\u001b[32m0.02594\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 195 | loss: 0.02594 - acc: 0.9862 -- iter: 056/963\n",
            "Training Step: 23482  | total loss: \u001b[1m\u001b[32m0.02337\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 195 | loss: 0.02337 - acc: 0.9875 -- iter: 064/963\n",
            "Training Step: 23483  | total loss: \u001b[1m\u001b[32m0.02105\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 195 | loss: 0.02105 - acc: 0.9888 -- iter: 072/963\n",
            "Training Step: 23484  | total loss: \u001b[1m\u001b[32m0.04951\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 195 | loss: 0.04951 - acc: 0.9774 -- iter: 080/963\n",
            "Training Step: 23485  | total loss: \u001b[1m\u001b[32m0.04460\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 195 | loss: 0.04460 - acc: 0.9797 -- iter: 088/963\n",
            "Training Step: 23486  | total loss: \u001b[1m\u001b[32m0.04017\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 195 | loss: 0.04017 - acc: 0.9817 -- iter: 096/963\n",
            "Training Step: 23487  | total loss: \u001b[1m\u001b[32m0.03616\u001b[0m\u001b[0m | time: 0.033s\n",
            "| Adam | epoch: 195 | loss: 0.03616 - acc: 0.9835 -- iter: 104/963\n",
            "Training Step: 23488  | total loss: \u001b[1m\u001b[32m0.03255\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 195 | loss: 0.03255 - acc: 0.9852 -- iter: 112/963\n",
            "Training Step: 23489  | total loss: \u001b[1m\u001b[32m0.02933\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 195 | loss: 0.02933 - acc: 0.9867 -- iter: 120/963\n",
            "Training Step: 23490  | total loss: \u001b[1m\u001b[32m0.02644\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 195 | loss: 0.02644 - acc: 0.9880 -- iter: 128/963\n",
            "Training Step: 23491  | total loss: \u001b[1m\u001b[32m0.02381\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 195 | loss: 0.02381 - acc: 0.9892 -- iter: 136/963\n",
            "Training Step: 23492  | total loss: \u001b[1m\u001b[32m0.02144\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 195 | loss: 0.02144 - acc: 0.9903 -- iter: 144/963\n",
            "Training Step: 23493  | total loss: \u001b[1m\u001b[32m0.01933\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 195 | loss: 0.01933 - acc: 0.9912 -- iter: 152/963\n",
            "Training Step: 23494  | total loss: \u001b[1m\u001b[32m0.01742\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 195 | loss: 0.01742 - acc: 0.9921 -- iter: 160/963\n",
            "Training Step: 23495  | total loss: \u001b[1m\u001b[32m0.01569\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 195 | loss: 0.01569 - acc: 0.9929 -- iter: 168/963\n",
            "Training Step: 23496  | total loss: \u001b[1m\u001b[32m0.01414\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 195 | loss: 0.01414 - acc: 0.9936 -- iter: 176/963\n",
            "Training Step: 23497  | total loss: \u001b[1m\u001b[32m0.01276\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 195 | loss: 0.01276 - acc: 0.9943 -- iter: 184/963\n",
            "Training Step: 23498  | total loss: \u001b[1m\u001b[32m0.01149\u001b[0m\u001b[0m | time: 0.059s\n",
            "| Adam | epoch: 195 | loss: 0.01149 - acc: 0.9948 -- iter: 192/963\n",
            "Training Step: 23499  | total loss: \u001b[1m\u001b[32m0.01035\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 195 | loss: 0.01035 - acc: 0.9953 -- iter: 200/963\n",
            "Training Step: 23500  | total loss: \u001b[1m\u001b[32m0.00932\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 195 | loss: 0.00932 - acc: 0.9958 -- iter: 208/963\n",
            "Training Step: 23501  | total loss: \u001b[1m\u001b[32m0.00840\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 195 | loss: 0.00840 - acc: 0.9962 -- iter: 216/963\n",
            "Training Step: 23502  | total loss: \u001b[1m\u001b[32m0.01697\u001b[0m\u001b[0m | time: 0.076s\n",
            "| Adam | epoch: 195 | loss: 0.01697 - acc: 0.9841 -- iter: 224/963\n",
            "Training Step: 23503  | total loss: \u001b[1m\u001b[32m0.01529\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 195 | loss: 0.01529 - acc: 0.9857 -- iter: 232/963\n",
            "Training Step: 23504  | total loss: \u001b[1m\u001b[32m0.01390\u001b[0m\u001b[0m | time: 0.081s\n",
            "| Adam | epoch: 195 | loss: 0.01390 - acc: 0.9871 -- iter: 240/963\n",
            "Training Step: 23505  | total loss: \u001b[1m\u001b[32m0.01267\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 195 | loss: 0.01267 - acc: 0.9884 -- iter: 248/963\n",
            "Training Step: 23506  | total loss: \u001b[1m\u001b[32m0.01141\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 195 | loss: 0.01141 - acc: 0.9896 -- iter: 256/963\n",
            "Training Step: 23507  | total loss: \u001b[1m\u001b[32m0.01029\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 195 | loss: 0.01029 - acc: 0.9906 -- iter: 264/963\n",
            "Training Step: 23508  | total loss: \u001b[1m\u001b[32m0.02013\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 195 | loss: 0.02013 - acc: 0.9791 -- iter: 272/963\n",
            "Training Step: 23509  | total loss: \u001b[1m\u001b[32m0.01813\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 195 | loss: 0.01813 - acc: 0.9811 -- iter: 280/963\n",
            "Training Step: 23510  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 195 | loss: 0.01637 - acc: 0.9830 -- iter: 288/963\n",
            "Training Step: 23511  | total loss: \u001b[1m\u001b[32m0.01475\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 195 | loss: 0.01475 - acc: 0.9847 -- iter: 296/963\n",
            "Training Step: 23512  | total loss: \u001b[1m\u001b[32m0.01329\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 195 | loss: 0.01329 - acc: 0.9863 -- iter: 304/963\n",
            "Training Step: 23513  | total loss: \u001b[1m\u001b[32m0.01198\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 195 | loss: 0.01198 - acc: 0.9876 -- iter: 312/963\n",
            "Training Step: 23514  | total loss: \u001b[1m\u001b[32m0.01079\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 195 | loss: 0.01079 - acc: 0.9889 -- iter: 320/963\n",
            "Training Step: 23515  | total loss: \u001b[1m\u001b[32m0.00975\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 195 | loss: 0.00975 - acc: 0.9900 -- iter: 328/963\n",
            "Training Step: 23516  | total loss: \u001b[1m\u001b[32m0.00888\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 195 | loss: 0.00888 - acc: 0.9910 -- iter: 336/963\n",
            "Training Step: 23517  | total loss: \u001b[1m\u001b[32m0.01178\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 195 | loss: 0.01178 - acc: 0.9919 -- iter: 344/963\n",
            "Training Step: 23518  | total loss: \u001b[1m\u001b[32m0.01065\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 195 | loss: 0.01065 - acc: 0.9927 -- iter: 352/963\n",
            "Training Step: 23519  | total loss: \u001b[1m\u001b[32m0.00968\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 195 | loss: 0.00968 - acc: 0.9934 -- iter: 360/963\n",
            "Training Step: 23520  | total loss: \u001b[1m\u001b[32m0.00872\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 195 | loss: 0.00872 - acc: 0.9941 -- iter: 368/963\n",
            "Training Step: 23521  | total loss: \u001b[1m\u001b[32m0.00786\u001b[0m\u001b[0m | time: 0.120s\n",
            "| Adam | epoch: 195 | loss: 0.00786 - acc: 0.9947 -- iter: 376/963\n",
            "Training Step: 23522  | total loss: \u001b[1m\u001b[32m0.00708\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 195 | loss: 0.00708 - acc: 0.9952 -- iter: 384/963\n",
            "Training Step: 23523  | total loss: \u001b[1m\u001b[32m0.00639\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 195 | loss: 0.00639 - acc: 0.9957 -- iter: 392/963\n",
            "Training Step: 23524  | total loss: \u001b[1m\u001b[32m0.00576\u001b[0m\u001b[0m | time: 0.127s\n",
            "| Adam | epoch: 195 | loss: 0.00576 - acc: 0.9961 -- iter: 400/963\n",
            "Training Step: 23525  | total loss: \u001b[1m\u001b[32m0.00523\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 195 | loss: 0.00523 - acc: 0.9965 -- iter: 408/963\n",
            "Training Step: 23526  | total loss: \u001b[1m\u001b[32m0.00473\u001b[0m\u001b[0m | time: 0.132s\n",
            "| Adam | epoch: 195 | loss: 0.00473 - acc: 0.9969 -- iter: 416/963\n",
            "Training Step: 23527  | total loss: \u001b[1m\u001b[32m0.00427\u001b[0m\u001b[0m | time: 0.134s\n",
            "| Adam | epoch: 195 | loss: 0.00427 - acc: 0.9972 -- iter: 424/963\n",
            "Training Step: 23528  | total loss: \u001b[1m\u001b[32m0.01333\u001b[0m\u001b[0m | time: 0.136s\n",
            "| Adam | epoch: 195 | loss: 0.01333 - acc: 0.9850 -- iter: 432/963\n",
            "Training Step: 23529  | total loss: \u001b[1m\u001b[32m0.01200\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 195 | loss: 0.01200 - acc: 0.9865 -- iter: 440/963\n",
            "Training Step: 23530  | total loss: \u001b[1m\u001b[32m0.01082\u001b[0m\u001b[0m | time: 0.141s\n",
            "| Adam | epoch: 195 | loss: 0.01082 - acc: 0.9878 -- iter: 448/963\n",
            "Training Step: 23531  | total loss: \u001b[1m\u001b[32m0.01037\u001b[0m\u001b[0m | time: 0.143s\n",
            "| Adam | epoch: 195 | loss: 0.01037 - acc: 0.9890 -- iter: 456/963\n",
            "Training Step: 23532  | total loss: \u001b[1m\u001b[32m0.00934\u001b[0m\u001b[0m | time: 0.146s\n",
            "| Adam | epoch: 195 | loss: 0.00934 - acc: 0.9901 -- iter: 464/963\n",
            "Training Step: 23533  | total loss: \u001b[1m\u001b[32m0.00842\u001b[0m\u001b[0m | time: 0.148s\n",
            "| Adam | epoch: 195 | loss: 0.00842 - acc: 0.9911 -- iter: 472/963\n",
            "Training Step: 23534  | total loss: \u001b[1m\u001b[32m0.00759\u001b[0m\u001b[0m | time: 0.150s\n",
            "| Adam | epoch: 195 | loss: 0.00759 - acc: 0.9920 -- iter: 480/963\n",
            "Training Step: 23535  | total loss: \u001b[1m\u001b[32m0.00693\u001b[0m\u001b[0m | time: 0.152s\n",
            "| Adam | epoch: 195 | loss: 0.00693 - acc: 0.9928 -- iter: 488/963\n",
            "Training Step: 23536  | total loss: \u001b[1m\u001b[32m0.00626\u001b[0m\u001b[0m | time: 0.155s\n",
            "| Adam | epoch: 195 | loss: 0.00626 - acc: 0.9935 -- iter: 496/963\n",
            "Training Step: 23537  | total loss: \u001b[1m\u001b[32m0.00711\u001b[0m\u001b[0m | time: 0.157s\n",
            "| Adam | epoch: 195 | loss: 0.00711 - acc: 0.9942 -- iter: 504/963\n",
            "Training Step: 23538  | total loss: \u001b[1m\u001b[32m0.00985\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 195 | loss: 0.00985 - acc: 0.9948 -- iter: 512/963\n",
            "Training Step: 23539  | total loss: \u001b[1m\u001b[32m0.03818\u001b[0m\u001b[0m | time: 0.162s\n",
            "| Adam | epoch: 195 | loss: 0.03818 - acc: 0.9828 -- iter: 520/963\n",
            "Training Step: 23540  | total loss: \u001b[1m\u001b[32m0.03437\u001b[0m\u001b[0m | time: 0.164s\n",
            "| Adam | epoch: 195 | loss: 0.03437 - acc: 0.9845 -- iter: 528/963\n",
            "Training Step: 23541  | total loss: \u001b[1m\u001b[32m0.03095\u001b[0m\u001b[0m | time: 0.166s\n",
            "| Adam | epoch: 195 | loss: 0.03095 - acc: 0.9861 -- iter: 536/963\n",
            "Training Step: 23542  | total loss: \u001b[1m\u001b[32m0.02835\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 195 | loss: 0.02835 - acc: 0.9874 -- iter: 544/963\n",
            "Training Step: 23543  | total loss: \u001b[1m\u001b[32m0.05058\u001b[0m\u001b[0m | time: 0.171s\n",
            "| Adam | epoch: 195 | loss: 0.05058 - acc: 0.9762 -- iter: 552/963\n",
            "Training Step: 23544  | total loss: \u001b[1m\u001b[32m0.04555\u001b[0m\u001b[0m | time: 0.173s\n",
            "| Adam | epoch: 195 | loss: 0.04555 - acc: 0.9786 -- iter: 560/963\n",
            "Training Step: 23545  | total loss: \u001b[1m\u001b[32m0.04101\u001b[0m\u001b[0m | time: 0.176s\n",
            "| Adam | epoch: 195 | loss: 0.04101 - acc: 0.9807 -- iter: 568/963\n",
            "Training Step: 23546  | total loss: \u001b[1m\u001b[32m0.03691\u001b[0m\u001b[0m | time: 0.178s\n",
            "| Adam | epoch: 195 | loss: 0.03691 - acc: 0.9827 -- iter: 576/963\n",
            "Training Step: 23547  | total loss: \u001b[1m\u001b[32m0.03323\u001b[0m\u001b[0m | time: 0.180s\n",
            "| Adam | epoch: 195 | loss: 0.03323 - acc: 0.9844 -- iter: 584/963\n",
            "Training Step: 23548  | total loss: \u001b[1m\u001b[32m0.02992\u001b[0m\u001b[0m | time: 0.183s\n",
            "| Adam | epoch: 195 | loss: 0.02992 - acc: 0.9859 -- iter: 592/963\n",
            "Training Step: 23549  | total loss: \u001b[1m\u001b[32m0.02695\u001b[0m\u001b[0m | time: 0.185s\n",
            "| Adam | epoch: 195 | loss: 0.02695 - acc: 0.9874 -- iter: 600/963\n",
            "Training Step: 23550  | total loss: \u001b[1m\u001b[32m0.02427\u001b[0m\u001b[0m | time: 0.187s\n",
            "| Adam | epoch: 195 | loss: 0.02427 - acc: 0.9886 -- iter: 608/963\n",
            "Training Step: 23551  | total loss: \u001b[1m\u001b[32m0.02186\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 195 | loss: 0.02186 - acc: 0.9898 -- iter: 616/963\n",
            "Training Step: 23552  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.192s\n",
            "| Adam | epoch: 195 | loss: 0.01972 - acc: 0.9908 -- iter: 624/963\n",
            "Training Step: 23553  | total loss: \u001b[1m\u001b[32m0.01779\u001b[0m\u001b[0m | time: 0.194s\n",
            "| Adam | epoch: 195 | loss: 0.01779 - acc: 0.9917 -- iter: 632/963\n",
            "Training Step: 23554  | total loss: \u001b[1m\u001b[32m0.01604\u001b[0m\u001b[0m | time: 0.196s\n",
            "| Adam | epoch: 195 | loss: 0.01604 - acc: 0.9925 -- iter: 640/963\n",
            "Training Step: 23555  | total loss: \u001b[1m\u001b[32m0.01446\u001b[0m\u001b[0m | time: 0.199s\n",
            "| Adam | epoch: 195 | loss: 0.01446 - acc: 0.9933 -- iter: 648/963\n",
            "Training Step: 23556  | total loss: \u001b[1m\u001b[32m0.02257\u001b[0m\u001b[0m | time: 0.201s\n",
            "| Adam | epoch: 195 | loss: 0.02257 - acc: 0.9815 -- iter: 656/963\n",
            "Training Step: 23557  | total loss: \u001b[1m\u001b[32m0.02033\u001b[0m\u001b[0m | time: 0.204s\n",
            "| Adam | epoch: 195 | loss: 0.02033 - acc: 0.9833 -- iter: 664/963\n",
            "Training Step: 23558  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.206s\n",
            "| Adam | epoch: 195 | loss: 0.01834 - acc: 0.9850 -- iter: 672/963\n",
            "Training Step: 23559  | total loss: \u001b[1m\u001b[32m0.01651\u001b[0m\u001b[0m | time: 0.209s\n",
            "| Adam | epoch: 195 | loss: 0.01651 - acc: 0.9865 -- iter: 680/963\n",
            "Training Step: 23560  | total loss: \u001b[1m\u001b[32m0.01488\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 195 | loss: 0.01488 - acc: 0.9878 -- iter: 688/963\n",
            "Training Step: 23561  | total loss: \u001b[1m\u001b[32m0.01340\u001b[0m\u001b[0m | time: 0.213s\n",
            "| Adam | epoch: 195 | loss: 0.01340 - acc: 0.9890 -- iter: 696/963\n",
            "Training Step: 23562  | total loss: \u001b[1m\u001b[32m0.01219\u001b[0m\u001b[0m | time: 0.216s\n",
            "| Adam | epoch: 195 | loss: 0.01219 - acc: 0.9901 -- iter: 704/963\n",
            "Training Step: 23563  | total loss: \u001b[1m\u001b[32m0.05136\u001b[0m\u001b[0m | time: 0.218s\n",
            "| Adam | epoch: 195 | loss: 0.05136 - acc: 0.9786 -- iter: 712/963\n",
            "Training Step: 23564  | total loss: \u001b[1m\u001b[32m0.04626\u001b[0m\u001b[0m | time: 0.226s\n",
            "| Adam | epoch: 195 | loss: 0.04626 - acc: 0.9808 -- iter: 720/963\n",
            "Training Step: 23565  | total loss: \u001b[1m\u001b[32m0.04165\u001b[0m\u001b[0m | time: 0.228s\n",
            "| Adam | epoch: 195 | loss: 0.04165 - acc: 0.9827 -- iter: 728/963\n",
            "Training Step: 23566  | total loss: \u001b[1m\u001b[32m0.03750\u001b[0m\u001b[0m | time: 0.232s\n",
            "| Adam | epoch: 195 | loss: 0.03750 - acc: 0.9844 -- iter: 736/963\n",
            "Training Step: 23567  | total loss: \u001b[1m\u001b[32m0.03380\u001b[0m\u001b[0m | time: 0.235s\n",
            "| Adam | epoch: 195 | loss: 0.03380 - acc: 0.9860 -- iter: 744/963\n",
            "Training Step: 23568  | total loss: \u001b[1m\u001b[32m0.03045\u001b[0m\u001b[0m | time: 0.237s\n",
            "| Adam | epoch: 195 | loss: 0.03045 - acc: 0.9874 -- iter: 752/963\n",
            "Training Step: 23569  | total loss: \u001b[1m\u001b[32m0.02741\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 195 | loss: 0.02741 - acc: 0.9886 -- iter: 760/963\n",
            "Training Step: 23570  | total loss: \u001b[1m\u001b[32m0.02471\u001b[0m\u001b[0m | time: 0.243s\n",
            "| Adam | epoch: 195 | loss: 0.02471 - acc: 0.9898 -- iter: 768/963\n",
            "Training Step: 23571  | total loss: \u001b[1m\u001b[32m0.02226\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 195 | loss: 0.02226 - acc: 0.9908 -- iter: 776/963\n",
            "Training Step: 23572  | total loss: \u001b[1m\u001b[32m0.02006\u001b[0m\u001b[0m | time: 0.248s\n",
            "| Adam | epoch: 195 | loss: 0.02006 - acc: 0.9917 -- iter: 784/963\n",
            "Training Step: 23573  | total loss: \u001b[1m\u001b[32m0.01805\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 195 | loss: 0.01805 - acc: 0.9925 -- iter: 792/963\n",
            "Training Step: 23574  | total loss: \u001b[1m\u001b[32m0.01626\u001b[0m\u001b[0m | time: 0.252s\n",
            "| Adam | epoch: 195 | loss: 0.01626 - acc: 0.9933 -- iter: 800/963\n",
            "Training Step: 23575  | total loss: \u001b[1m\u001b[32m0.01467\u001b[0m\u001b[0m | time: 0.255s\n",
            "| Adam | epoch: 195 | loss: 0.01467 - acc: 0.9940 -- iter: 808/963\n",
            "Training Step: 23576  | total loss: \u001b[1m\u001b[32m0.02698\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 195 | loss: 0.02698 - acc: 0.9821 -- iter: 816/963\n",
            "Training Step: 23577  | total loss: \u001b[1m\u001b[32m0.02429\u001b[0m\u001b[0m | time: 0.260s\n",
            "| Adam | epoch: 195 | loss: 0.02429 - acc: 0.9839 -- iter: 824/963\n",
            "Training Step: 23578  | total loss: \u001b[1m\u001b[32m0.02189\u001b[0m\u001b[0m | time: 0.262s\n",
            "| Adam | epoch: 195 | loss: 0.02189 - acc: 0.9855 -- iter: 832/963\n",
            "Training Step: 23579  | total loss: \u001b[1m\u001b[32m0.01972\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 195 | loss: 0.01972 - acc: 0.9869 -- iter: 840/963\n",
            "Training Step: 23580  | total loss: \u001b[1m\u001b[32m0.01780\u001b[0m\u001b[0m | time: 0.267s\n",
            "| Adam | epoch: 195 | loss: 0.01780 - acc: 0.9882 -- iter: 848/963\n",
            "Training Step: 23581  | total loss: \u001b[1m\u001b[32m0.01602\u001b[0m\u001b[0m | time: 0.270s\n",
            "| Adam | epoch: 195 | loss: 0.01602 - acc: 0.9894 -- iter: 856/963\n",
            "Training Step: 23582  | total loss: \u001b[1m\u001b[32m0.01443\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 195 | loss: 0.01443 - acc: 0.9905 -- iter: 864/963\n",
            "Training Step: 23583  | total loss: \u001b[1m\u001b[32m0.01300\u001b[0m\u001b[0m | time: 0.274s\n",
            "| Adam | epoch: 195 | loss: 0.01300 - acc: 0.9914 -- iter: 872/963\n",
            "Training Step: 23584  | total loss: \u001b[1m\u001b[32m0.01172\u001b[0m\u001b[0m | time: 0.277s\n",
            "| Adam | epoch: 195 | loss: 0.01172 - acc: 0.9923 -- iter: 880/963\n",
            "Training Step: 23585  | total loss: \u001b[1m\u001b[32m0.01657\u001b[0m\u001b[0m | time: 0.279s\n",
            "| Adam | epoch: 195 | loss: 0.01657 - acc: 0.9931 -- iter: 888/963\n",
            "Training Step: 23586  | total loss: \u001b[1m\u001b[32m0.01492\u001b[0m\u001b[0m | time: 0.282s\n",
            "| Adam | epoch: 195 | loss: 0.01492 - acc: 0.9937 -- iter: 896/963\n",
            "Training Step: 23587  | total loss: \u001b[1m\u001b[32m0.01345\u001b[0m\u001b[0m | time: 0.285s\n",
            "| Adam | epoch: 195 | loss: 0.01345 - acc: 0.9944 -- iter: 904/963\n",
            "Training Step: 23588  | total loss: \u001b[1m\u001b[32m0.01211\u001b[0m\u001b[0m | time: 0.287s\n",
            "| Adam | epoch: 195 | loss: 0.01211 - acc: 0.9949 -- iter: 912/963\n",
            "Training Step: 23589  | total loss: \u001b[1m\u001b[32m0.01091\u001b[0m\u001b[0m | time: 0.290s\n",
            "| Adam | epoch: 195 | loss: 0.01091 - acc: 0.9954 -- iter: 920/963\n",
            "Training Step: 23590  | total loss: \u001b[1m\u001b[32m0.00986\u001b[0m\u001b[0m | time: 0.293s\n",
            "| Adam | epoch: 195 | loss: 0.00986 - acc: 0.9959 -- iter: 928/963\n",
            "Training Step: 23591  | total loss: \u001b[1m\u001b[32m0.00889\u001b[0m\u001b[0m | time: 0.295s\n",
            "| Adam | epoch: 195 | loss: 0.00889 - acc: 0.9963 -- iter: 936/963\n",
            "Training Step: 23592  | total loss: \u001b[1m\u001b[32m0.00801\u001b[0m\u001b[0m | time: 0.298s\n",
            "| Adam | epoch: 195 | loss: 0.00801 - acc: 0.9967 -- iter: 944/963\n",
            "Training Step: 23593  | total loss: \u001b[1m\u001b[32m0.00724\u001b[0m\u001b[0m | time: 0.300s\n",
            "| Adam | epoch: 195 | loss: 0.00724 - acc: 0.9970 -- iter: 952/963\n",
            "Training Step: 23594  | total loss: \u001b[1m\u001b[32m0.00653\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 195 | loss: 0.00653 - acc: 0.9973 -- iter: 960/963\n",
            "Training Step: 23595  | total loss: \u001b[1m\u001b[32m0.00588\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 195 | loss: 0.00588 - acc: 0.9976 -- iter: 963/963\n",
            "--\n",
            "Training Step: 23596  | total loss: \u001b[1m\u001b[32m0.00536\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 196 | loss: 0.00536 - acc: 0.9978 -- iter: 008/963\n",
            "Training Step: 23597  | total loss: \u001b[1m\u001b[32m0.00484\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 196 | loss: 0.00484 - acc: 0.9980 -- iter: 016/963\n",
            "Training Step: 23598  | total loss: \u001b[1m\u001b[32m0.00436\u001b[0m\u001b[0m | time: 0.008s\n",
            "| Adam | epoch: 196 | loss: 0.00436 - acc: 0.9982 -- iter: 024/963\n",
            "Training Step: 23599  | total loss: \u001b[1m\u001b[32m0.00401\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 196 | loss: 0.00401 - acc: 0.9984 -- iter: 032/963\n",
            "Training Step: 23600  | total loss: \u001b[1m\u001b[32m0.00363\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 196 | loss: 0.00363 - acc: 0.9986 -- iter: 040/963\n",
            "Training Step: 23601  | total loss: \u001b[1m\u001b[32m0.00918\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 196 | loss: 0.00918 - acc: 0.9987 -- iter: 048/963\n",
            "Training Step: 23602  | total loss: \u001b[1m\u001b[32m0.00827\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 196 | loss: 0.00827 - acc: 0.9988 -- iter: 056/963\n",
            "Training Step: 23603  | total loss: \u001b[1m\u001b[32m0.00748\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 196 | loss: 0.00748 - acc: 0.9990 -- iter: 064/963\n",
            "Training Step: 23604  | total loss: \u001b[1m\u001b[32m0.00674\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 196 | loss: 0.00674 - acc: 0.9991 -- iter: 072/963\n",
            "Training Step: 23605  | total loss: \u001b[1m\u001b[32m0.00613\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 196 | loss: 0.00613 - acc: 0.9992 -- iter: 080/963\n",
            "Training Step: 23606  | total loss: \u001b[1m\u001b[32m0.00553\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 196 | loss: 0.00553 - acc: 0.9992 -- iter: 088/963\n",
            "Training Step: 23607  | total loss: \u001b[1m\u001b[32m0.00499\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 196 | loss: 0.00499 - acc: 0.9993 -- iter: 096/963\n",
            "Training Step: 23608  | total loss: \u001b[1m\u001b[32m0.00450\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 196 | loss: 0.00450 - acc: 0.9994 -- iter: 104/963\n",
            "Training Step: 23609  | total loss: \u001b[1m\u001b[32m0.00406\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 196 | loss: 0.00406 - acc: 0.9994 -- iter: 112/963\n",
            "Training Step: 23610  | total loss: \u001b[1m\u001b[32m0.00414\u001b[0m\u001b[0m | time: 0.037s\n",
            "| Adam | epoch: 196 | loss: 0.00414 - acc: 0.9995 -- iter: 120/963\n",
            "Training Step: 23611  | total loss: \u001b[1m\u001b[32m0.00373\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 196 | loss: 0.00373 - acc: 0.9996 -- iter: 128/963\n",
            "Training Step: 23612  | total loss: \u001b[1m\u001b[32m0.00337\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 196 | loss: 0.00337 - acc: 0.9996 -- iter: 136/963\n",
            "Training Step: 23613  | total loss: \u001b[1m\u001b[32m0.00305\u001b[0m\u001b[0m | time: 0.046s\n",
            "| Adam | epoch: 196 | loss: 0.00305 - acc: 0.9996 -- iter: 144/963\n",
            "Training Step: 23614  | total loss: \u001b[1m\u001b[32m0.00281\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 196 | loss: 0.00281 - acc: 0.9997 -- iter: 152/963\n",
            "Training Step: 23615  | total loss: \u001b[1m\u001b[32m0.00256\u001b[0m\u001b[0m | time: 0.052s\n",
            "| Adam | epoch: 196 | loss: 0.00256 - acc: 0.9997 -- iter: 160/963\n",
            "Training Step: 23616  | total loss: \u001b[1m\u001b[32m0.03276\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 196 | loss: 0.03276 - acc: 0.9872 -- iter: 168/963\n",
            "Training Step: 23617  | total loss: \u001b[1m\u001b[32m0.02951\u001b[0m\u001b[0m | time: 0.057s\n",
            "| Adam | epoch: 196 | loss: 0.02951 - acc: 0.9885 -- iter: 176/963\n",
            "Training Step: 23618  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 196 | loss: 0.02659 - acc: 0.9897 -- iter: 184/963\n",
            "Training Step: 23619  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 196 | loss: 0.02396 - acc: 0.9907 -- iter: 192/963\n",
            "Training Step: 23620  | total loss: \u001b[1m\u001b[32m0.02159\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 196 | loss: 0.02159 - acc: 0.9925 -- iter: 200/963\n",
            "Training Step: 23621  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.067s\n",
            "| Adam | epoch: 196 | loss: 0.01944 - acc: 0.9925 -- iter: 208/963\n",
            "Training Step: 23622  | total loss: \u001b[1m\u001b[32m0.01752\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 196 | loss: 0.01752 - acc: 0.9932 -- iter: 216/963\n",
            "Training Step: 23623  | total loss: \u001b[1m\u001b[32m0.01579\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 196 | loss: 0.01579 - acc: 0.9939 -- iter: 224/963\n",
            "Training Step: 23624  | total loss: \u001b[1m\u001b[32m0.01425\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 196 | loss: 0.01425 - acc: 0.9945 -- iter: 232/963\n",
            "Training Step: 23625  | total loss: \u001b[1m\u001b[32m0.01283\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 196 | loss: 0.01283 - acc: 0.9951 -- iter: 240/963\n",
            "Training Step: 23626  | total loss: \u001b[1m\u001b[32m0.01156\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 196 | loss: 0.01156 - acc: 0.9955 -- iter: 248/963\n",
            "Training Step: 23627  | total loss: \u001b[1m\u001b[32m0.04759\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 196 | loss: 0.04759 - acc: 0.9835 -- iter: 256/963\n",
            "Training Step: 23628  | total loss: \u001b[1m\u001b[32m0.04284\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 196 | loss: 0.04284 - acc: 0.9851 -- iter: 264/963\n",
            "Training Step: 23629  | total loss: \u001b[1m\u001b[32m0.03858\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 196 | loss: 0.03858 - acc: 0.9866 -- iter: 272/963\n",
            "Training Step: 23630  | total loss: \u001b[1m\u001b[32m0.03474\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 196 | loss: 0.03474 - acc: 0.9880 -- iter: 280/963\n",
            "Training Step: 23631  | total loss: \u001b[1m\u001b[32m0.03245\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 196 | loss: 0.03245 - acc: 0.9892 -- iter: 288/963\n",
            "Training Step: 23632  | total loss: \u001b[1m\u001b[32m0.02923\u001b[0m\u001b[0m | time: 0.095s\n",
            "| Adam | epoch: 196 | loss: 0.02923 - acc: 0.9903 -- iter: 296/963\n",
            "Training Step: 23633  | total loss: \u001b[1m\u001b[32m0.05763\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 196 | loss: 0.05763 - acc: 0.9787 -- iter: 304/963\n",
            "Training Step: 23634  | total loss: \u001b[1m\u001b[32m0.05188\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 196 | loss: 0.05188 - acc: 0.9809 -- iter: 312/963\n",
            "Training Step: 23635  | total loss: \u001b[1m\u001b[32m0.04670\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 196 | loss: 0.04670 - acc: 0.9828 -- iter: 320/963\n",
            "Training Step: 23636  | total loss: \u001b[1m\u001b[32m0.04207\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 196 | loss: 0.04207 - acc: 0.9845 -- iter: 328/963\n",
            "Training Step: 23637  | total loss: \u001b[1m\u001b[32m0.03787\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 196 | loss: 0.03787 - acc: 0.9860 -- iter: 336/963\n",
            "Training Step: 23638  | total loss: \u001b[1m\u001b[32m0.03409\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 196 | loss: 0.03409 - acc: 0.9874 -- iter: 344/963\n",
            "Training Step: 23639  | total loss: \u001b[1m\u001b[32m0.03070\u001b[0m\u001b[0m | time: 0.112s\n",
            "| Adam | epoch: 196 | loss: 0.03070 - acc: 0.9887 -- iter: 352/963\n",
            "Training Step: 23640  | total loss: \u001b[1m\u001b[32m0.02764\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 196 | loss: 0.02764 - acc: 0.9898 -- iter: 360/963\n",
            "Training Step: 23641  | total loss: \u001b[1m\u001b[32m0.02488\u001b[0m\u001b[0m | time: 0.117s\n",
            "| Adam | epoch: 196 | loss: 0.02488 - acc: 0.9908 -- iter: 368/963\n",
            "Training Step: 23642  | total loss: \u001b[1m\u001b[32m0.02240\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 196 | loss: 0.02240 - acc: 0.9918 -- iter: 376/963\n",
            "Training Step: 23643  | total loss: \u001b[1m\u001b[32m0.02050\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 196 | loss: 0.02050 - acc: 0.9926 -- iter: 384/963\n",
            "Training Step: 23644  | total loss: \u001b[1m\u001b[32m0.01847\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 196 | loss: 0.01847 - acc: 0.9933 -- iter: 392/963\n",
            "Training Step: 23645  | total loss: \u001b[1m\u001b[32m0.01664\u001b[0m\u001b[0m | time: 0.126s\n",
            "| Adam | epoch: 196 | loss: 0.01664 - acc: 0.9940 -- iter: 400/963\n",
            "Training Step: 23646  | total loss: \u001b[1m\u001b[32m0.01500\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 196 | loss: 0.01500 - acc: 0.9946 -- iter: 408/963\n",
            "Training Step: 23647  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.131s\n",
            "| Adam | epoch: 196 | loss: 0.01636 - acc: 0.9951 -- iter: 416/963\n",
            "Training Step: 23648  | total loss: \u001b[1m\u001b[32m0.03355\u001b[0m\u001b[0m | time: 0.133s\n",
            "| Adam | epoch: 196 | loss: 0.03355 - acc: 0.9831 -- iter: 424/963\n",
            "Training Step: 23649  | total loss: \u001b[1m\u001b[32m0.03472\u001b[0m\u001b[0m | time: 0.135s\n",
            "| Adam | epoch: 196 | loss: 0.03472 - acc: 0.9848 -- iter: 432/963\n",
            "Training Step: 23650  | total loss: \u001b[1m\u001b[32m0.03128\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 196 | loss: 0.03128 - acc: 0.9863 -- iter: 440/963\n",
            "Training Step: 23651  | total loss: \u001b[1m\u001b[32m0.04036\u001b[0m\u001b[0m | time: 0.140s\n",
            "| Adam | epoch: 196 | loss: 0.04036 - acc: 0.9752 -- iter: 448/963\n",
            "Training Step: 23652  | total loss: \u001b[1m\u001b[32m0.03633\u001b[0m\u001b[0m | time: 0.142s\n",
            "| Adam | epoch: 196 | loss: 0.03633 - acc: 0.9777 -- iter: 456/963\n",
            "Training Step: 23653  | total loss: \u001b[1m\u001b[32m0.03273\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 196 | loss: 0.03273 - acc: 0.9799 -- iter: 464/963\n",
            "Training Step: 23654  | total loss: \u001b[1m\u001b[32m0.02948\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 196 | loss: 0.02948 - acc: 0.9819 -- iter: 472/963\n",
            "Training Step: 23655  | total loss: \u001b[1m\u001b[32m0.02656\u001b[0m\u001b[0m | time: 0.149s\n",
            "| Adam | epoch: 196 | loss: 0.02656 - acc: 0.9837 -- iter: 480/963\n",
            "Training Step: 23656  | total loss: \u001b[1m\u001b[32m0.02391\u001b[0m\u001b[0m | time: 0.151s\n",
            "| Adam | epoch: 196 | loss: 0.02391 - acc: 0.9854 -- iter: 488/963\n",
            "Training Step: 23657  | total loss: \u001b[1m\u001b[32m0.02154\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 196 | loss: 0.02154 - acc: 0.9868 -- iter: 496/963\n",
            "Training Step: 23658  | total loss: \u001b[1m\u001b[32m0.01949\u001b[0m\u001b[0m | time: 0.156s\n",
            "| Adam | epoch: 196 | loss: 0.01949 - acc: 0.9881 -- iter: 504/963\n",
            "Training Step: 23659  | total loss: \u001b[1m\u001b[32m0.01757\u001b[0m\u001b[0m | time: 0.158s\n",
            "| Adam | epoch: 196 | loss: 0.01757 - acc: 0.9893 -- iter: 512/963\n",
            "Training Step: 23660  | total loss: \u001b[1m\u001b[32m0.01584\u001b[0m\u001b[0m | time: 0.160s\n",
            "| Adam | epoch: 196 | loss: 0.01584 - acc: 0.9904 -- iter: 520/963\n",
            "Training Step: 23661  | total loss: \u001b[1m\u001b[32m0.01427\u001b[0m\u001b[0m | time: 0.163s\n",
            "| Adam | epoch: 196 | loss: 0.01427 - acc: 0.9914 -- iter: 528/963\n",
            "Training Step: 23662  | total loss: \u001b[1m\u001b[32m0.01286\u001b[0m\u001b[0m | time: 0.166s\n",
            "| Adam | epoch: 196 | loss: 0.01286 - acc: 0.9922 -- iter: 536/963\n",
            "Training Step: 23663  | total loss: \u001b[1m\u001b[32m0.01158\u001b[0m\u001b[0m | time: 0.169s\n",
            "| Adam | epoch: 196 | loss: 0.01158 - acc: 0.9930 -- iter: 544/963\n",
            "Training Step: 23664  | total loss: \u001b[1m\u001b[32m0.01047\u001b[0m\u001b[0m | time: 0.171s\n",
            "| Adam | epoch: 196 | loss: 0.01047 - acc: 0.9937 -- iter: 552/963\n",
            "Training Step: 23665  | total loss: \u001b[1m\u001b[32m0.03767\u001b[0m\u001b[0m | time: 0.174s\n",
            "| Adam | epoch: 196 | loss: 0.03767 - acc: 0.9818 -- iter: 560/963\n",
            "Training Step: 23666  | total loss: \u001b[1m\u001b[32m0.03390\u001b[0m\u001b[0m | time: 0.177s\n",
            "| Adam | epoch: 196 | loss: 0.03390 - acc: 0.9836 -- iter: 568/963\n",
            "Training Step: 23667  | total loss: \u001b[1m\u001b[32m0.03052\u001b[0m\u001b[0m | time: 0.180s\n",
            "| Adam | epoch: 196 | loss: 0.03052 - acc: 0.9853 -- iter: 576/963\n",
            "Training Step: 23668  | total loss: \u001b[1m\u001b[32m0.04893\u001b[0m\u001b[0m | time: 0.182s\n",
            "| Adam | epoch: 196 | loss: 0.04893 - acc: 0.9868 -- iter: 584/963\n",
            "Training Step: 23669  | total loss: \u001b[1m\u001b[32m0.06162\u001b[0m\u001b[0m | time: 0.184s\n",
            "| Adam | epoch: 196 | loss: 0.06162 - acc: 0.9881 -- iter: 592/963\n",
            "Training Step: 23670  | total loss: \u001b[1m\u001b[32m0.05550\u001b[0m\u001b[0m | time: 0.187s\n",
            "| Adam | epoch: 196 | loss: 0.05550 - acc: 0.9893 -- iter: 600/963\n",
            "Training Step: 23671  | total loss: \u001b[1m\u001b[32m0.05001\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 196 | loss: 0.05001 - acc: 0.9903 -- iter: 608/963\n",
            "Training Step: 23672  | total loss: \u001b[1m\u001b[32m0.04503\u001b[0m\u001b[0m | time: 0.192s\n",
            "| Adam | epoch: 196 | loss: 0.04503 - acc: 0.9913 -- iter: 616/963\n",
            "Training Step: 23673  | total loss: \u001b[1m\u001b[32m0.04055\u001b[0m\u001b[0m | time: 0.194s\n",
            "| Adam | epoch: 196 | loss: 0.04055 - acc: 0.9922 -- iter: 624/963\n",
            "Training Step: 23674  | total loss: \u001b[1m\u001b[32m0.03652\u001b[0m\u001b[0m | time: 0.197s\n",
            "| Adam | epoch: 196 | loss: 0.03652 - acc: 0.9930 -- iter: 632/963\n",
            "Training Step: 23675  | total loss: \u001b[1m\u001b[32m0.03288\u001b[0m\u001b[0m | time: 0.200s\n",
            "| Adam | epoch: 196 | loss: 0.03288 - acc: 0.9937 -- iter: 640/963\n",
            "Training Step: 23676  | total loss: \u001b[1m\u001b[32m0.02960\u001b[0m\u001b[0m | time: 0.202s\n",
            "| Adam | epoch: 196 | loss: 0.02960 - acc: 0.9943 -- iter: 648/963\n",
            "Training Step: 23677  | total loss: \u001b[1m\u001b[32m0.02666\u001b[0m\u001b[0m | time: 0.205s\n",
            "| Adam | epoch: 196 | loss: 0.02666 - acc: 0.9949 -- iter: 656/963\n",
            "Training Step: 23678  | total loss: \u001b[1m\u001b[32m0.02400\u001b[0m\u001b[0m | time: 0.207s\n",
            "| Adam | epoch: 196 | loss: 0.02400 - acc: 0.9954 -- iter: 664/963\n",
            "Training Step: 23679  | total loss: \u001b[1m\u001b[32m0.04949\u001b[0m\u001b[0m | time: 0.210s\n",
            "| Adam | epoch: 196 | loss: 0.04949 - acc: 0.9833 -- iter: 672/963\n",
            "Training Step: 23680  | total loss: \u001b[1m\u001b[32m0.04457\u001b[0m\u001b[0m | time: 0.212s\n",
            "| Adam | epoch: 196 | loss: 0.04457 - acc: 0.9850 -- iter: 680/963\n",
            "Training Step: 23681  | total loss: \u001b[1m\u001b[32m0.04012\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 196 | loss: 0.04012 - acc: 0.9865 -- iter: 688/963\n",
            "Training Step: 23682  | total loss: \u001b[1m\u001b[32m0.03612\u001b[0m\u001b[0m | time: 0.217s\n",
            "| Adam | epoch: 196 | loss: 0.03612 - acc: 0.9879 -- iter: 696/963\n",
            "Training Step: 23683  | total loss: \u001b[1m\u001b[32m0.08620\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 196 | loss: 0.08620 - acc: 0.9766 -- iter: 704/963\n",
            "Training Step: 23684  | total loss: \u001b[1m\u001b[32m0.07759\u001b[0m\u001b[0m | time: 0.222s\n",
            "| Adam | epoch: 196 | loss: 0.07759 - acc: 0.9789 -- iter: 712/963\n",
            "Training Step: 23685  | total loss: \u001b[1m\u001b[32m0.06985\u001b[0m\u001b[0m | time: 0.225s\n",
            "| Adam | epoch: 196 | loss: 0.06985 - acc: 0.9810 -- iter: 720/963\n",
            "Training Step: 23686  | total loss: \u001b[1m\u001b[32m0.06287\u001b[0m\u001b[0m | time: 0.228s\n",
            "| Adam | epoch: 196 | loss: 0.06287 - acc: 0.9829 -- iter: 728/963\n",
            "Training Step: 23687  | total loss: \u001b[1m\u001b[32m0.05904\u001b[0m\u001b[0m | time: 0.230s\n",
            "| Adam | epoch: 196 | loss: 0.05904 - acc: 0.9846 -- iter: 736/963\n",
            "Training Step: 23688  | total loss: \u001b[1m\u001b[32m0.05810\u001b[0m\u001b[0m | time: 0.233s\n",
            "| Adam | epoch: 196 | loss: 0.05810 - acc: 0.9862 -- iter: 744/963\n",
            "Training Step: 23689  | total loss: \u001b[1m\u001b[32m0.05229\u001b[0m\u001b[0m | time: 0.236s\n",
            "| Adam | epoch: 196 | loss: 0.05229 - acc: 0.9875 -- iter: 752/963\n",
            "Training Step: 23690  | total loss: \u001b[1m\u001b[32m0.04709\u001b[0m\u001b[0m | time: 0.239s\n",
            "| Adam | epoch: 196 | loss: 0.04709 - acc: 0.9888 -- iter: 760/963\n",
            "Training Step: 23691  | total loss: \u001b[1m\u001b[32m0.04241\u001b[0m\u001b[0m | time: 0.241s\n",
            "| Adam | epoch: 196 | loss: 0.04241 - acc: 0.9899 -- iter: 768/963\n",
            "Training Step: 23692  | total loss: \u001b[1m\u001b[32m0.03818\u001b[0m\u001b[0m | time: 0.244s\n",
            "| Adam | epoch: 196 | loss: 0.03818 - acc: 0.9909 -- iter: 776/963\n",
            "Training Step: 23693  | total loss: \u001b[1m\u001b[32m0.09517\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 196 | loss: 0.09517 - acc: 0.9793 -- iter: 784/963\n",
            "Training Step: 23694  | total loss: \u001b[1m\u001b[32m0.08566\u001b[0m\u001b[0m | time: 0.249s\n",
            "| Adam | epoch: 196 | loss: 0.08566 - acc: 0.9814 -- iter: 792/963\n",
            "Training Step: 23695  | total loss: \u001b[1m\u001b[32m0.07712\u001b[0m\u001b[0m | time: 0.252s\n",
            "| Adam | epoch: 196 | loss: 0.07712 - acc: 0.9833 -- iter: 800/963\n",
            "Training Step: 23696  | total loss: \u001b[1m\u001b[32m0.06943\u001b[0m\u001b[0m | time: 0.254s\n",
            "| Adam | epoch: 196 | loss: 0.06943 - acc: 0.9849 -- iter: 808/963\n",
            "Training Step: 23697  | total loss: \u001b[1m\u001b[32m0.06249\u001b[0m\u001b[0m | time: 0.257s\n",
            "| Adam | epoch: 196 | loss: 0.06249 - acc: 0.9864 -- iter: 816/963\n",
            "Training Step: 23698  | total loss: \u001b[1m\u001b[32m0.05635\u001b[0m\u001b[0m | time: 0.259s\n",
            "| Adam | epoch: 196 | loss: 0.05635 - acc: 0.9878 -- iter: 824/963\n",
            "Training Step: 23699  | total loss: \u001b[1m\u001b[32m0.05075\u001b[0m\u001b[0m | time: 0.262s\n",
            "| Adam | epoch: 196 | loss: 0.05075 - acc: 0.9890 -- iter: 832/963\n",
            "Training Step: 23700  | total loss: \u001b[1m\u001b[32m0.04570\u001b[0m\u001b[0m | time: 0.266s\n",
            "| Adam | epoch: 196 | loss: 0.04570 - acc: 0.9901 -- iter: 840/963\n",
            "Training Step: 23701  | total loss: \u001b[1m\u001b[32m0.04113\u001b[0m\u001b[0m | time: 0.269s\n",
            "| Adam | epoch: 196 | loss: 0.04113 - acc: 0.9911 -- iter: 848/963\n",
            "Training Step: 23702  | total loss: \u001b[1m\u001b[32m0.03703\u001b[0m\u001b[0m | time: 0.271s\n",
            "| Adam | epoch: 196 | loss: 0.03703 - acc: 0.9920 -- iter: 856/963\n",
            "Training Step: 23703  | total loss: \u001b[1m\u001b[32m0.04244\u001b[0m\u001b[0m | time: 0.279s\n",
            "| Adam | epoch: 196 | loss: 0.04244 - acc: 0.9803 -- iter: 864/963\n",
            "Training Step: 23704  | total loss: \u001b[1m\u001b[32m0.03833\u001b[0m\u001b[0m | time: 0.282s\n",
            "| Adam | epoch: 196 | loss: 0.03833 - acc: 0.9823 -- iter: 872/963\n",
            "Training Step: 23705  | total loss: \u001b[1m\u001b[32m0.03457\u001b[0m\u001b[0m | time: 0.284s\n",
            "| Adam | epoch: 196 | loss: 0.03457 - acc: 0.9840 -- iter: 880/963\n",
            "Training Step: 23706  | total loss: \u001b[1m\u001b[32m0.04303\u001b[0m\u001b[0m | time: 0.287s\n",
            "| Adam | epoch: 196 | loss: 0.04303 - acc: 0.9731 -- iter: 888/963\n",
            "Training Step: 23707  | total loss: \u001b[1m\u001b[32m0.03875\u001b[0m\u001b[0m | time: 0.289s\n",
            "| Adam | epoch: 196 | loss: 0.03875 - acc: 0.9758 -- iter: 896/963\n",
            "Training Step: 23708  | total loss: \u001b[1m\u001b[32m0.03489\u001b[0m\u001b[0m | time: 0.291s\n",
            "| Adam | epoch: 196 | loss: 0.03489 - acc: 0.9782 -- iter: 904/963\n",
            "Training Step: 23709  | total loss: \u001b[1m\u001b[32m0.03141\u001b[0m\u001b[0m | time: 0.294s\n",
            "| Adam | epoch: 196 | loss: 0.03141 - acc: 0.9804 -- iter: 912/963\n",
            "Training Step: 23710  | total loss: \u001b[1m\u001b[32m0.02828\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 196 | loss: 0.02828 - acc: 0.9824 -- iter: 920/963\n",
            "Training Step: 23711  | total loss: \u001b[1m\u001b[32m0.02547\u001b[0m\u001b[0m | time: 0.298s\n",
            "| Adam | epoch: 196 | loss: 0.02547 - acc: 0.9841 -- iter: 928/963\n",
            "Training Step: 23712  | total loss: \u001b[1m\u001b[32m0.02293\u001b[0m\u001b[0m | time: 0.300s\n",
            "| Adam | epoch: 196 | loss: 0.02293 - acc: 0.9857 -- iter: 936/963\n",
            "Training Step: 23713  | total loss: \u001b[1m\u001b[32m0.02066\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 196 | loss: 0.02066 - acc: 0.9871 -- iter: 944/963\n",
            "Training Step: 23714  | total loss: \u001b[1m\u001b[32m0.01861\u001b[0m\u001b[0m | time: 0.305s\n",
            "| Adam | epoch: 196 | loss: 0.01861 - acc: 0.9884 -- iter: 952/963\n",
            "Training Step: 23715  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.308s\n",
            "| Adam | epoch: 196 | loss: 0.01682 - acc: 0.9896 -- iter: 960/963\n",
            "Training Step: 23716  | total loss: \u001b[1m\u001b[32m0.01515\u001b[0m\u001b[0m | time: 0.312s\n",
            "| Adam | epoch: 196 | loss: 0.01515 - acc: 0.9906 -- iter: 963/963\n",
            "--\n",
            "Training Step: 23717  | total loss: \u001b[1m\u001b[32m0.01366\u001b[0m\u001b[0m | time: 0.004s\n",
            "| Adam | epoch: 197 | loss: 0.01366 - acc: 0.9916 -- iter: 008/963\n",
            "Training Step: 23718  | total loss: \u001b[1m\u001b[32m0.01231\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 197 | loss: 0.01231 - acc: 0.9924 -- iter: 016/963\n",
            "Training Step: 23719  | total loss: \u001b[1m\u001b[32m0.01116\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 197 | loss: 0.01116 - acc: 0.9932 -- iter: 024/963\n",
            "Training Step: 23720  | total loss: \u001b[1m\u001b[32m0.01005\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 197 | loss: 0.01005 - acc: 0.9939 -- iter: 032/963\n",
            "Training Step: 23721  | total loss: \u001b[1m\u001b[32m0.00907\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 197 | loss: 0.00907 - acc: 0.9945 -- iter: 040/963\n",
            "Training Step: 23722  | total loss: \u001b[1m\u001b[32m0.00825\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 197 | loss: 0.00825 - acc: 0.9950 -- iter: 048/963\n",
            "Training Step: 23723  | total loss: \u001b[1m\u001b[32m0.00747\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 197 | loss: 0.00747 - acc: 0.9955 -- iter: 056/963\n",
            "Training Step: 23724  | total loss: \u001b[1m\u001b[32m0.00674\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 197 | loss: 0.00674 - acc: 0.9960 -- iter: 064/963\n",
            "Training Step: 23725  | total loss: \u001b[1m\u001b[32m0.00608\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 197 | loss: 0.00608 - acc: 0.9964 -- iter: 072/963\n",
            "Training Step: 23726  | total loss: \u001b[1m\u001b[32m0.00548\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 197 | loss: 0.00548 - acc: 0.9967 -- iter: 080/963\n",
            "Training Step: 23727  | total loss: \u001b[1m\u001b[32m0.00499\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 197 | loss: 0.00499 - acc: 0.9971 -- iter: 088/963\n",
            "Training Step: 23728  | total loss: \u001b[1m\u001b[32m0.00450\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 197 | loss: 0.00450 - acc: 0.9974 -- iter: 096/963\n",
            "Training Step: 23729  | total loss: \u001b[1m\u001b[32m0.00413\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 197 | loss: 0.00413 - acc: 0.9976 -- iter: 104/963\n",
            "Training Step: 23730  | total loss: \u001b[1m\u001b[32m0.03467\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 197 | loss: 0.03467 - acc: 0.9854 -- iter: 112/963\n",
            "Training Step: 23731  | total loss: \u001b[1m\u001b[32m0.03125\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 197 | loss: 0.03125 - acc: 0.9868 -- iter: 120/963\n",
            "Training Step: 23732  | total loss: \u001b[1m\u001b[32m0.02817\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 197 | loss: 0.02817 - acc: 0.9881 -- iter: 128/963\n",
            "Training Step: 23733  | total loss: \u001b[1m\u001b[32m0.02657\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 197 | loss: 0.02657 - acc: 0.9893 -- iter: 136/963\n",
            "Training Step: 23734  | total loss: \u001b[1m\u001b[32m0.02392\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 197 | loss: 0.02392 - acc: 0.9904 -- iter: 144/963\n",
            "Training Step: 23735  | total loss: \u001b[1m\u001b[32m0.02617\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 197 | loss: 0.02617 - acc: 0.9914 -- iter: 152/963\n",
            "Training Step: 23736  | total loss: \u001b[1m\u001b[32m0.02357\u001b[0m\u001b[0m | time: 0.049s\n",
            "| Adam | epoch: 197 | loss: 0.02357 - acc: 0.9922 -- iter: 160/963\n",
            "Training Step: 23737  | total loss: \u001b[1m\u001b[32m0.02122\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 197 | loss: 0.02122 - acc: 0.9930 -- iter: 168/963\n",
            "Training Step: 23738  | total loss: \u001b[1m\u001b[32m0.01911\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 197 | loss: 0.01911 - acc: 0.9937 -- iter: 176/963\n",
            "Training Step: 23739  | total loss: \u001b[1m\u001b[32m0.01721\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 197 | loss: 0.01721 - acc: 0.9943 -- iter: 184/963\n",
            "Training Step: 23740  | total loss: \u001b[1m\u001b[32m0.01715\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 197 | loss: 0.01715 - acc: 0.9949 -- iter: 192/963\n",
            "Training Step: 23741  | total loss: \u001b[1m\u001b[32m0.01545\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 197 | loss: 0.01545 - acc: 0.9954 -- iter: 200/963\n",
            "Training Step: 23742  | total loss: \u001b[1m\u001b[32m0.01393\u001b[0m\u001b[0m | time: 0.062s\n",
            "| Adam | epoch: 197 | loss: 0.01393 - acc: 0.9959 -- iter: 208/963\n",
            "Training Step: 23743  | total loss: \u001b[1m\u001b[32m0.01259\u001b[0m\u001b[0m | time: 0.064s\n",
            "| Adam | epoch: 197 | loss: 0.01259 - acc: 0.9963 -- iter: 216/963\n",
            "Training Step: 23744  | total loss: \u001b[1m\u001b[32m0.01224\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 197 | loss: 0.01224 - acc: 0.9967 -- iter: 224/963\n",
            "Training Step: 23745  | total loss: \u001b[1m\u001b[32m0.01102\u001b[0m\u001b[0m | time: 0.068s\n",
            "| Adam | epoch: 197 | loss: 0.01102 - acc: 0.9970 -- iter: 232/963\n",
            "Training Step: 23746  | total loss: \u001b[1m\u001b[32m0.00994\u001b[0m\u001b[0m | time: 0.070s\n",
            "| Adam | epoch: 197 | loss: 0.00994 - acc: 0.9973 -- iter: 240/963\n",
            "Training Step: 23747  | total loss: \u001b[1m\u001b[32m0.00898\u001b[0m\u001b[0m | time: 0.073s\n",
            "| Adam | epoch: 197 | loss: 0.00898 - acc: 0.9976 -- iter: 248/963\n",
            "Training Step: 23748  | total loss: \u001b[1m\u001b[32m0.00813\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 197 | loss: 0.00813 - acc: 0.9978 -- iter: 256/963\n",
            "Training Step: 23749  | total loss: \u001b[1m\u001b[32m0.00732\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 197 | loss: 0.00732 - acc: 0.9980 -- iter: 264/963\n",
            "Training Step: 23750  | total loss: \u001b[1m\u001b[32m0.00661\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 197 | loss: 0.00661 - acc: 0.9982 -- iter: 272/963\n",
            "Training Step: 23751  | total loss: \u001b[1m\u001b[32m0.00597\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 197 | loss: 0.00597 - acc: 0.9984 -- iter: 280/963\n",
            "Training Step: 23752  | total loss: \u001b[1m\u001b[32m0.00538\u001b[0m\u001b[0m | time: 0.087s\n",
            "| Adam | epoch: 197 | loss: 0.00538 - acc: 0.9986 -- iter: 288/963\n",
            "Training Step: 23753  | total loss: \u001b[1m\u001b[32m0.00485\u001b[0m\u001b[0m | time: 0.090s\n",
            "| Adam | epoch: 197 | loss: 0.00485 - acc: 0.9987 -- iter: 296/963\n",
            "Training Step: 23754  | total loss: \u001b[1m\u001b[32m0.00438\u001b[0m\u001b[0m | time: 0.092s\n",
            "| Adam | epoch: 197 | loss: 0.00438 - acc: 0.9988 -- iter: 304/963\n",
            "Training Step: 23755  | total loss: \u001b[1m\u001b[32m0.00395\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 197 | loss: 0.00395 - acc: 0.9989 -- iter: 312/963\n",
            "Training Step: 23756  | total loss: \u001b[1m\u001b[32m0.00357\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 197 | loss: 0.00357 - acc: 0.9991 -- iter: 320/963\n",
            "Training Step: 23757  | total loss: \u001b[1m\u001b[32m0.00325\u001b[0m\u001b[0m | time: 0.100s\n",
            "| Adam | epoch: 197 | loss: 0.00325 - acc: 0.9991 -- iter: 328/963\n",
            "Training Step: 23758  | total loss: \u001b[1m\u001b[32m0.00295\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 197 | loss: 0.00295 - acc: 0.9992 -- iter: 336/963\n",
            "Training Step: 23759  | total loss: \u001b[1m\u001b[32m0.00266\u001b[0m\u001b[0m | time: 0.104s\n",
            "| Adam | epoch: 197 | loss: 0.00266 - acc: 0.9993 -- iter: 344/963\n",
            "Training Step: 23760  | total loss: \u001b[1m\u001b[32m0.00242\u001b[0m\u001b[0m | time: 0.107s\n",
            "| Adam | epoch: 197 | loss: 0.00242 - acc: 0.9994 -- iter: 352/963\n",
            "Training Step: 23761  | total loss: \u001b[1m\u001b[32m0.00218\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 197 | loss: 0.00218 - acc: 0.9994 -- iter: 360/963\n",
            "Training Step: 23762  | total loss: \u001b[1m\u001b[32m0.04165\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 197 | loss: 0.04165 - acc: 0.9870 -- iter: 368/963\n",
            "Training Step: 23763  | total loss: \u001b[1m\u001b[32m0.03750\u001b[0m\u001b[0m | time: 0.114s\n",
            "| Adam | epoch: 197 | loss: 0.03750 - acc: 0.9883 -- iter: 376/963\n",
            "Training Step: 23764  | total loss: \u001b[1m\u001b[32m0.03396\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 197 | loss: 0.03396 - acc: 0.9895 -- iter: 384/963\n",
            "Training Step: 23765  | total loss: \u001b[1m\u001b[32m0.03058\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 197 | loss: 0.03058 - acc: 0.9905 -- iter: 392/963\n",
            "Training Step: 23766  | total loss: \u001b[1m\u001b[32m0.02756\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 197 | loss: 0.02756 - acc: 0.9915 -- iter: 400/963\n",
            "Training Step: 23767  | total loss: \u001b[1m\u001b[32m0.02482\u001b[0m\u001b[0m | time: 0.123s\n",
            "| Adam | epoch: 197 | loss: 0.02482 - acc: 0.9923 -- iter: 408/963\n",
            "Training Step: 23768  | total loss: \u001b[1m\u001b[32m0.02260\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 197 | loss: 0.02260 - acc: 0.9931 -- iter: 416/963\n",
            "Training Step: 23769  | total loss: \u001b[1m\u001b[32m0.02036\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 197 | loss: 0.02036 - acc: 0.9938 -- iter: 424/963\n",
            "Training Step: 23770  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.130s\n",
            "| Adam | epoch: 197 | loss: 0.01834 - acc: 0.9944 -- iter: 432/963\n",
            "Training Step: 23771  | total loss: \u001b[1m\u001b[32m0.02686\u001b[0m\u001b[0m | time: 0.132s\n",
            "| Adam | epoch: 197 | loss: 0.02686 - acc: 0.9825 -- iter: 440/963\n",
            "Training Step: 23772  | total loss: \u001b[1m\u001b[32m0.02419\u001b[0m\u001b[0m | time: 0.134s\n",
            "| Adam | epoch: 197 | loss: 0.02419 - acc: 0.9842 -- iter: 448/963\n",
            "Training Step: 23773  | total loss: \u001b[1m\u001b[32m0.02179\u001b[0m\u001b[0m | time: 0.137s\n",
            "| Adam | epoch: 197 | loss: 0.02179 - acc: 0.9858 -- iter: 456/963\n",
            "Training Step: 23774  | total loss: \u001b[1m\u001b[32m0.02125\u001b[0m\u001b[0m | time: 0.139s\n",
            "| Adam | epoch: 197 | loss: 0.02125 - acc: 0.9872 -- iter: 464/963\n",
            "Training Step: 23775  | total loss: \u001b[1m\u001b[32m0.01915\u001b[0m\u001b[0m | time: 0.141s\n",
            "| Adam | epoch: 197 | loss: 0.01915 - acc: 0.9885 -- iter: 472/963\n",
            "Training Step: 23776  | total loss: \u001b[1m\u001b[32m0.02905\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 197 | loss: 0.02905 - acc: 0.9771 -- iter: 480/963\n",
            "Training Step: 23777  | total loss: \u001b[1m\u001b[32m0.02631\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 197 | loss: 0.02631 - acc: 0.9794 -- iter: 488/963\n",
            "Training Step: 23778  | total loss: \u001b[1m\u001b[32m0.02370\u001b[0m\u001b[0m | time: 0.149s\n",
            "| Adam | epoch: 197 | loss: 0.02370 - acc: 0.9815 -- iter: 496/963\n",
            "Training Step: 23779  | total loss: \u001b[1m\u001b[32m0.02133\u001b[0m\u001b[0m | time: 0.152s\n",
            "| Adam | epoch: 197 | loss: 0.02133 - acc: 0.9833 -- iter: 504/963\n",
            "Training Step: 23780  | total loss: \u001b[1m\u001b[32m0.01926\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 197 | loss: 0.01926 - acc: 0.9850 -- iter: 512/963\n",
            "Training Step: 23781  | total loss: \u001b[1m\u001b[32m0.01735\u001b[0m\u001b[0m | time: 0.157s\n",
            "| Adam | epoch: 197 | loss: 0.01735 - acc: 0.9865 -- iter: 520/963\n",
            "Training Step: 23782  | total loss: \u001b[1m\u001b[32m0.01569\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 197 | loss: 0.01569 - acc: 0.9879 -- iter: 528/963\n",
            "Training Step: 23783  | total loss: \u001b[1m\u001b[32m0.01413\u001b[0m\u001b[0m | time: 0.162s\n",
            "| Adam | epoch: 197 | loss: 0.01413 - acc: 0.9891 -- iter: 536/963\n",
            "Training Step: 23784  | total loss: \u001b[1m\u001b[32m0.01274\u001b[0m\u001b[0m | time: 0.164s\n",
            "| Adam | epoch: 197 | loss: 0.01274 - acc: 0.9902 -- iter: 544/963\n",
            "Training Step: 23785  | total loss: \u001b[1m\u001b[32m0.01149\u001b[0m\u001b[0m | time: 0.167s\n",
            "| Adam | epoch: 197 | loss: 0.01149 - acc: 0.9911 -- iter: 552/963\n",
            "Training Step: 23786  | total loss: \u001b[1m\u001b[32m0.01609\u001b[0m\u001b[0m | time: 0.170s\n",
            "| Adam | epoch: 197 | loss: 0.01609 - acc: 0.9920 -- iter: 560/963\n",
            "Training Step: 23787  | total loss: \u001b[1m\u001b[32m0.01449\u001b[0m\u001b[0m | time: 0.172s\n",
            "| Adam | epoch: 197 | loss: 0.01449 - acc: 0.9928 -- iter: 568/963\n",
            "Training Step: 23788  | total loss: \u001b[1m\u001b[32m0.01304\u001b[0m\u001b[0m | time: 0.174s\n",
            "| Adam | epoch: 197 | loss: 0.01304 - acc: 0.9935 -- iter: 576/963\n",
            "Training Step: 23789  | total loss: \u001b[1m\u001b[32m0.01178\u001b[0m\u001b[0m | time: 0.177s\n",
            "| Adam | epoch: 197 | loss: 0.01178 - acc: 0.9942 -- iter: 584/963\n",
            "Training Step: 23790  | total loss: \u001b[1m\u001b[32m0.01067\u001b[0m\u001b[0m | time: 0.180s\n",
            "| Adam | epoch: 197 | loss: 0.01067 - acc: 0.9948 -- iter: 592/963\n",
            "Training Step: 23791  | total loss: \u001b[1m\u001b[32m0.00965\u001b[0m\u001b[0m | time: 0.182s\n",
            "| Adam | epoch: 197 | loss: 0.00965 - acc: 0.9953 -- iter: 600/963\n",
            "Training Step: 23792  | total loss: \u001b[1m\u001b[32m0.00871\u001b[0m\u001b[0m | time: 0.186s\n",
            "| Adam | epoch: 197 | loss: 0.00871 - acc: 0.9958 -- iter: 608/963\n",
            "Training Step: 23793  | total loss: \u001b[1m\u001b[32m0.00785\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 197 | loss: 0.00785 - acc: 0.9962 -- iter: 616/963\n",
            "Training Step: 23794  | total loss: \u001b[1m\u001b[32m0.00707\u001b[0m\u001b[0m | time: 0.192s\n",
            "| Adam | epoch: 197 | loss: 0.00707 - acc: 0.9966 -- iter: 624/963\n",
            "Training Step: 23795  | total loss: \u001b[1m\u001b[32m0.00640\u001b[0m\u001b[0m | time: 0.195s\n",
            "| Adam | epoch: 197 | loss: 0.00640 - acc: 0.9969 -- iter: 632/963\n",
            "Training Step: 23796  | total loss: \u001b[1m\u001b[32m0.00578\u001b[0m\u001b[0m | time: 0.197s\n",
            "| Adam | epoch: 197 | loss: 0.00578 - acc: 0.9972 -- iter: 640/963\n",
            "Training Step: 23797  | total loss: \u001b[1m\u001b[32m0.02628\u001b[0m\u001b[0m | time: 0.199s\n",
            "| Adam | epoch: 197 | loss: 0.02628 - acc: 0.9850 -- iter: 648/963\n",
            "Training Step: 23798  | total loss: \u001b[1m\u001b[32m0.02367\u001b[0m\u001b[0m | time: 0.202s\n",
            "| Adam | epoch: 197 | loss: 0.02367 - acc: 0.9865 -- iter: 656/963\n",
            "Training Step: 23799  | total loss: \u001b[1m\u001b[32m0.02134\u001b[0m\u001b[0m | time: 0.204s\n",
            "| Adam | epoch: 197 | loss: 0.02134 - acc: 0.9878 -- iter: 664/963\n",
            "Training Step: 23800  | total loss: \u001b[1m\u001b[32m0.01924\u001b[0m\u001b[0m | time: 0.207s\n",
            "| Adam | epoch: 197 | loss: 0.01924 - acc: 0.9891 -- iter: 672/963\n",
            "Training Step: 23801  | total loss: \u001b[1m\u001b[32m0.01760\u001b[0m\u001b[0m | time: 0.210s\n",
            "| Adam | epoch: 197 | loss: 0.01760 - acc: 0.9902 -- iter: 680/963\n",
            "Training Step: 23802  | total loss: \u001b[1m\u001b[32m0.01587\u001b[0m\u001b[0m | time: 0.212s\n",
            "| Adam | epoch: 197 | loss: 0.01587 - acc: 0.9911 -- iter: 688/963\n",
            "Training Step: 23803  | total loss: \u001b[1m\u001b[32m0.01430\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 197 | loss: 0.01430 - acc: 0.9920 -- iter: 696/963\n",
            "Training Step: 23804  | total loss: \u001b[1m\u001b[32m0.01287\u001b[0m\u001b[0m | time: 0.217s\n",
            "| Adam | epoch: 197 | loss: 0.01287 - acc: 0.9928 -- iter: 704/963\n",
            "Training Step: 23805  | total loss: \u001b[1m\u001b[32m0.01161\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 197 | loss: 0.01161 - acc: 0.9935 -- iter: 712/963\n",
            "Training Step: 23806  | total loss: \u001b[1m\u001b[32m0.01047\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 197 | loss: 0.01047 - acc: 0.9942 -- iter: 720/963\n",
            "Training Step: 23807  | total loss: \u001b[1m\u001b[32m0.00944\u001b[0m\u001b[0m | time: 0.226s\n",
            "| Adam | epoch: 197 | loss: 0.00944 - acc: 0.9948 -- iter: 728/963\n",
            "Training Step: 23808  | total loss: \u001b[1m\u001b[32m0.00850\u001b[0m\u001b[0m | time: 0.229s\n",
            "| Adam | epoch: 197 | loss: 0.00850 - acc: 0.9953 -- iter: 736/963\n",
            "Training Step: 23809  | total loss: \u001b[1m\u001b[32m0.00768\u001b[0m\u001b[0m | time: 0.232s\n",
            "| Adam | epoch: 197 | loss: 0.00768 - acc: 0.9958 -- iter: 744/963\n",
            "Training Step: 23810  | total loss: \u001b[1m\u001b[32m0.00693\u001b[0m\u001b[0m | time: 0.234s\n",
            "| Adam | epoch: 197 | loss: 0.00693 - acc: 0.9962 -- iter: 752/963\n",
            "Training Step: 23811  | total loss: \u001b[1m\u001b[32m0.00626\u001b[0m\u001b[0m | time: 0.237s\n",
            "| Adam | epoch: 197 | loss: 0.00626 - acc: 0.9966 -- iter: 760/963\n",
            "Training Step: 23812  | total loss: \u001b[1m\u001b[32m0.00566\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 197 | loss: 0.00566 - acc: 0.9969 -- iter: 768/963\n",
            "Training Step: 23813  | total loss: \u001b[1m\u001b[32m0.00510\u001b[0m\u001b[0m | time: 0.243s\n",
            "| Adam | epoch: 197 | loss: 0.00510 - acc: 0.9972 -- iter: 776/963\n",
            "Training Step: 23814  | total loss: \u001b[1m\u001b[32m0.01574\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 197 | loss: 0.01574 - acc: 0.9850 -- iter: 784/963\n",
            "Training Step: 23815  | total loss: \u001b[1m\u001b[32m0.01418\u001b[0m\u001b[0m | time: 0.248s\n",
            "| Adam | epoch: 197 | loss: 0.01418 - acc: 0.9865 -- iter: 792/963\n",
            "Training Step: 23816  | total loss: \u001b[1m\u001b[32m0.01278\u001b[0m\u001b[0m | time: 0.253s\n",
            "| Adam | epoch: 197 | loss: 0.01278 - acc: 0.9878 -- iter: 800/963\n",
            "Training Step: 23817  | total loss: \u001b[1m\u001b[32m0.01153\u001b[0m\u001b[0m | time: 0.256s\n",
            "| Adam | epoch: 197 | loss: 0.01153 - acc: 0.9891 -- iter: 808/963\n",
            "Training Step: 23818  | total loss: \u001b[1m\u001b[32m0.01039\u001b[0m\u001b[0m | time: 0.259s\n",
            "| Adam | epoch: 197 | loss: 0.01039 - acc: 0.9902 -- iter: 816/963\n",
            "Training Step: 23819  | total loss: \u001b[1m\u001b[32m0.00937\u001b[0m\u001b[0m | time: 0.263s\n",
            "| Adam | epoch: 197 | loss: 0.00937 - acc: 0.9911 -- iter: 824/963\n",
            "Training Step: 23820  | total loss: \u001b[1m\u001b[32m0.00847\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 197 | loss: 0.00847 - acc: 0.9920 -- iter: 832/963\n",
            "Training Step: 23821  | total loss: \u001b[1m\u001b[32m0.00783\u001b[0m\u001b[0m | time: 0.268s\n",
            "| Adam | epoch: 197 | loss: 0.00783 - acc: 0.9928 -- iter: 840/963\n",
            "Training Step: 23822  | total loss: \u001b[1m\u001b[32m0.01238\u001b[0m\u001b[0m | time: 0.271s\n",
            "| Adam | epoch: 197 | loss: 0.01238 - acc: 0.9935 -- iter: 848/963\n",
            "Training Step: 23823  | total loss: \u001b[1m\u001b[32m0.01118\u001b[0m\u001b[0m | time: 0.277s\n",
            "| Adam | epoch: 197 | loss: 0.01118 - acc: 0.9942 -- iter: 856/963\n",
            "Training Step: 23824  | total loss: \u001b[1m\u001b[32m0.01008\u001b[0m\u001b[0m | time: 0.281s\n",
            "| Adam | epoch: 197 | loss: 0.01008 - acc: 0.9948 -- iter: 864/963\n",
            "Training Step: 23825  | total loss: \u001b[1m\u001b[32m0.00910\u001b[0m\u001b[0m | time: 0.288s\n",
            "| Adam | epoch: 197 | loss: 0.00910 - acc: 0.9953 -- iter: 872/963\n",
            "Training Step: 23826  | total loss: \u001b[1m\u001b[32m0.00875\u001b[0m\u001b[0m | time: 0.292s\n",
            "| Adam | epoch: 197 | loss: 0.00875 - acc: 0.9958 -- iter: 880/963\n",
            "Training Step: 23827  | total loss: \u001b[1m\u001b[32m0.00793\u001b[0m\u001b[0m | time: 0.296s\n",
            "| Adam | epoch: 197 | loss: 0.00793 - acc: 0.9962 -- iter: 888/963\n",
            "Training Step: 23828  | total loss: \u001b[1m\u001b[32m0.00714\u001b[0m\u001b[0m | time: 0.299s\n",
            "| Adam | epoch: 197 | loss: 0.00714 - acc: 0.9966 -- iter: 896/963\n",
            "Training Step: 23829  | total loss: \u001b[1m\u001b[32m0.01302\u001b[0m\u001b[0m | time: 0.304s\n",
            "| Adam | epoch: 197 | loss: 0.01302 - acc: 0.9969 -- iter: 904/963\n",
            "Training Step: 23830  | total loss: \u001b[1m\u001b[32m0.01176\u001b[0m\u001b[0m | time: 0.308s\n",
            "| Adam | epoch: 197 | loss: 0.01176 - acc: 0.9972 -- iter: 912/963\n",
            "Training Step: 23831  | total loss: \u001b[1m\u001b[32m0.01133\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 197 | loss: 0.01133 - acc: 0.9975 -- iter: 920/963\n",
            "Training Step: 23832  | total loss: \u001b[1m\u001b[32m0.01021\u001b[0m\u001b[0m | time: 0.316s\n",
            "| Adam | epoch: 197 | loss: 0.01021 - acc: 0.9977 -- iter: 928/963\n",
            "Training Step: 23833  | total loss: \u001b[1m\u001b[32m0.00920\u001b[0m\u001b[0m | time: 0.319s\n",
            "| Adam | epoch: 197 | loss: 0.00920 - acc: 0.9980 -- iter: 936/963\n",
            "Training Step: 23834  | total loss: \u001b[1m\u001b[32m0.00829\u001b[0m\u001b[0m | time: 0.324s\n",
            "| Adam | epoch: 197 | loss: 0.00829 - acc: 0.9982 -- iter: 944/963\n",
            "Training Step: 23835  | total loss: \u001b[1m\u001b[32m0.00748\u001b[0m\u001b[0m | time: 0.326s\n",
            "| Adam | epoch: 197 | loss: 0.00748 - acc: 0.9984 -- iter: 952/963\n",
            "Training Step: 23836  | total loss: \u001b[1m\u001b[32m0.00677\u001b[0m\u001b[0m | time: 0.329s\n",
            "| Adam | epoch: 197 | loss: 0.00677 - acc: 0.9985 -- iter: 960/963\n",
            "Training Step: 23837  | total loss: \u001b[1m\u001b[32m0.00609\u001b[0m\u001b[0m | time: 0.331s\n",
            "| Adam | epoch: 197 | loss: 0.00609 - acc: 0.9987 -- iter: 963/963\n",
            "--\n",
            "Training Step: 23838  | total loss: \u001b[1m\u001b[32m0.00549\u001b[0m\u001b[0m | time: 0.003s\n",
            "| Adam | epoch: 198 | loss: 0.00549 - acc: 0.9988 -- iter: 008/963\n",
            "Training Step: 23839  | total loss: \u001b[1m\u001b[32m0.00495\u001b[0m\u001b[0m | time: 0.006s\n",
            "| Adam | epoch: 198 | loss: 0.00495 - acc: 0.9989 -- iter: 016/963\n",
            "Training Step: 23840  | total loss: \u001b[1m\u001b[32m0.00446\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 198 | loss: 0.00446 - acc: 0.9991 -- iter: 024/963\n",
            "Training Step: 23841  | total loss: \u001b[1m\u001b[32m0.00405\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 198 | loss: 0.00405 - acc: 0.9991 -- iter: 032/963\n",
            "Training Step: 23842  | total loss: \u001b[1m\u001b[32m0.00368\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 198 | loss: 0.00368 - acc: 0.9992 -- iter: 040/963\n",
            "Training Step: 23843  | total loss: \u001b[1m\u001b[32m0.00342\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 198 | loss: 0.00342 - acc: 0.9993 -- iter: 048/963\n",
            "Training Step: 23844  | total loss: \u001b[1m\u001b[32m0.00309\u001b[0m\u001b[0m | time: 0.021s\n",
            "| Adam | epoch: 198 | loss: 0.00309 - acc: 0.9994 -- iter: 056/963\n",
            "Training Step: 23845  | total loss: \u001b[1m\u001b[32m0.00291\u001b[0m\u001b[0m | time: 0.024s\n",
            "| Adam | epoch: 198 | loss: 0.00291 - acc: 0.9994 -- iter: 064/963\n",
            "Training Step: 23846  | total loss: \u001b[1m\u001b[32m0.00263\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 198 | loss: 0.00263 - acc: 0.9995 -- iter: 072/963\n",
            "Training Step: 23847  | total loss: \u001b[1m\u001b[32m0.00239\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 198 | loss: 0.00239 - acc: 0.9995 -- iter: 080/963\n",
            "Training Step: 23848  | total loss: \u001b[1m\u001b[32m0.00270\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 198 | loss: 0.00270 - acc: 0.9996 -- iter: 088/963\n",
            "Training Step: 23849  | total loss: \u001b[1m\u001b[32m0.00245\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 198 | loss: 0.00245 - acc: 0.9996 -- iter: 096/963\n",
            "Training Step: 23850  | total loss: \u001b[1m\u001b[32m0.00224\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 198 | loss: 0.00224 - acc: 0.9997 -- iter: 104/963\n",
            "Training Step: 23851  | total loss: \u001b[1m\u001b[32m0.00207\u001b[0m\u001b[0m | time: 0.041s\n",
            "| Adam | epoch: 198 | loss: 0.00207 - acc: 0.9997 -- iter: 112/963\n",
            "Training Step: 23852  | total loss: \u001b[1m\u001b[32m0.00192\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 198 | loss: 0.00192 - acc: 0.9997 -- iter: 120/963\n",
            "Training Step: 23853  | total loss: \u001b[1m\u001b[32m0.00176\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 198 | loss: 0.00176 - acc: 0.9998 -- iter: 128/963\n",
            "Training Step: 23854  | total loss: \u001b[1m\u001b[32m0.00159\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 198 | loss: 0.00159 - acc: 0.9998 -- iter: 136/963\n",
            "Training Step: 23855  | total loss: \u001b[1m\u001b[32m0.04054\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 198 | loss: 0.04054 - acc: 0.9873 -- iter: 144/963\n",
            "Training Step: 23856  | total loss: \u001b[1m\u001b[32m0.03681\u001b[0m\u001b[0m | time: 0.060s\n",
            "| Adam | epoch: 198 | loss: 0.03681 - acc: 0.9886 -- iter: 152/963\n",
            "Training Step: 23857  | total loss: \u001b[1m\u001b[32m0.03317\u001b[0m\u001b[0m | time: 0.065s\n",
            "| Adam | epoch: 198 | loss: 0.03317 - acc: 0.9897 -- iter: 160/963\n",
            "Training Step: 23858  | total loss: \u001b[1m\u001b[32m0.02987\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 198 | loss: 0.02987 - acc: 0.9907 -- iter: 168/963\n",
            "Training Step: 23859  | total loss: \u001b[1m\u001b[32m0.02692\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 198 | loss: 0.02692 - acc: 0.9917 -- iter: 176/963\n",
            "Training Step: 23860  | total loss: \u001b[1m\u001b[32m0.02429\u001b[0m\u001b[0m | time: 0.078s\n",
            "| Adam | epoch: 198 | loss: 0.02429 - acc: 0.9925 -- iter: 184/963\n",
            "Training Step: 23861  | total loss: \u001b[1m\u001b[32m0.02188\u001b[0m\u001b[0m | time: 0.084s\n",
            "| Adam | epoch: 198 | loss: 0.02188 - acc: 0.9933 -- iter: 192/963\n",
            "Training Step: 23862  | total loss: \u001b[1m\u001b[32m0.01994\u001b[0m\u001b[0m | time: 0.098s\n",
            "| Adam | epoch: 198 | loss: 0.01994 - acc: 0.9939 -- iter: 200/963\n",
            "Training Step: 23863  | total loss: \u001b[1m\u001b[32m0.01797\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 198 | loss: 0.01797 - acc: 0.9945 -- iter: 208/963\n",
            "Training Step: 23864  | total loss: \u001b[1m\u001b[32m0.01620\u001b[0m\u001b[0m | time: 0.106s\n",
            "| Adam | epoch: 198 | loss: 0.01620 - acc: 0.9951 -- iter: 216/963\n",
            "Training Step: 23865  | total loss: \u001b[1m\u001b[32m0.01512\u001b[0m\u001b[0m | time: 0.110s\n",
            "| Adam | epoch: 198 | loss: 0.01512 - acc: 0.9956 -- iter: 224/963\n",
            "Training Step: 23866  | total loss: \u001b[1m\u001b[32m0.01362\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 198 | loss: 0.01362 - acc: 0.9960 -- iter: 232/963\n",
            "Training Step: 23867  | total loss: \u001b[1m\u001b[32m0.01228\u001b[0m\u001b[0m | time: 0.115s\n",
            "| Adam | epoch: 198 | loss: 0.01228 - acc: 0.9964 -- iter: 240/963\n",
            "Training Step: 23868  | total loss: \u001b[1m\u001b[32m0.05157\u001b[0m\u001b[0m | time: 0.118s\n",
            "| Adam | epoch: 198 | loss: 0.05157 - acc: 0.9843 -- iter: 248/963\n",
            "Training Step: 23869  | total loss: \u001b[1m\u001b[32m0.08062\u001b[0m\u001b[0m | time: 0.122s\n",
            "| Adam | epoch: 198 | loss: 0.08062 - acc: 0.9733 -- iter: 256/963\n",
            "Training Step: 23870  | total loss: \u001b[1m\u001b[32m0.07260\u001b[0m\u001b[0m | time: 0.125s\n",
            "| Adam | epoch: 198 | loss: 0.07260 - acc: 0.9760 -- iter: 264/963\n",
            "Training Step: 23871  | total loss: \u001b[1m\u001b[32m0.06534\u001b[0m\u001b[0m | time: 0.128s\n",
            "| Adam | epoch: 198 | loss: 0.06534 - acc: 0.9784 -- iter: 272/963\n",
            "Training Step: 23872  | total loss: \u001b[1m\u001b[32m0.05881\u001b[0m\u001b[0m | time: 0.132s\n",
            "| Adam | epoch: 198 | loss: 0.05881 - acc: 0.9806 -- iter: 280/963\n",
            "Training Step: 23873  | total loss: \u001b[1m\u001b[32m0.05295\u001b[0m\u001b[0m | time: 0.135s\n",
            "| Adam | epoch: 198 | loss: 0.05295 - acc: 0.9825 -- iter: 288/963\n",
            "Training Step: 23874  | total loss: \u001b[1m\u001b[32m0.04772\u001b[0m\u001b[0m | time: 0.138s\n",
            "| Adam | epoch: 198 | loss: 0.04772 - acc: 0.9843 -- iter: 296/963\n",
            "Training Step: 23875  | total loss: \u001b[1m\u001b[32m0.04296\u001b[0m\u001b[0m | time: 0.141s\n",
            "| Adam | epoch: 198 | loss: 0.04296 - acc: 0.9858 -- iter: 304/963\n",
            "Training Step: 23876  | total loss: \u001b[1m\u001b[32m0.03867\u001b[0m\u001b[0m | time: 0.144s\n",
            "| Adam | epoch: 198 | loss: 0.03867 - acc: 0.9873 -- iter: 312/963\n",
            "Training Step: 23877  | total loss: \u001b[1m\u001b[32m0.03485\u001b[0m\u001b[0m | time: 0.148s\n",
            "| Adam | epoch: 198 | loss: 0.03485 - acc: 0.9885 -- iter: 320/963\n",
            "Training Step: 23878  | total loss: \u001b[1m\u001b[32m0.07188\u001b[0m\u001b[0m | time: 0.151s\n",
            "| Adam | epoch: 198 | loss: 0.07188 - acc: 0.9772 -- iter: 328/963\n",
            "Training Step: 23879  | total loss: \u001b[1m\u001b[32m0.06475\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 198 | loss: 0.06475 - acc: 0.9795 -- iter: 336/963\n",
            "Training Step: 23880  | total loss: \u001b[1m\u001b[32m0.05828\u001b[0m\u001b[0m | time: 0.156s\n",
            "| Adam | epoch: 198 | loss: 0.05828 - acc: 0.9815 -- iter: 344/963\n",
            "Training Step: 23881  | total loss: \u001b[1m\u001b[32m0.05252\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 198 | loss: 0.05252 - acc: 0.9834 -- iter: 352/963\n",
            "Training Step: 23882  | total loss: \u001b[1m\u001b[32m0.04728\u001b[0m\u001b[0m | time: 0.163s\n",
            "| Adam | epoch: 198 | loss: 0.04728 - acc: 0.9850 -- iter: 360/963\n",
            "Training Step: 23883  | total loss: \u001b[1m\u001b[32m0.04257\u001b[0m\u001b[0m | time: 0.165s\n",
            "| Adam | epoch: 198 | loss: 0.04257 - acc: 0.9865 -- iter: 368/963\n",
            "Training Step: 23884  | total loss: \u001b[1m\u001b[32m0.06303\u001b[0m\u001b[0m | time: 0.168s\n",
            "| Adam | epoch: 198 | loss: 0.06303 - acc: 0.9754 -- iter: 376/963\n",
            "Training Step: 23885  | total loss: \u001b[1m\u001b[32m0.05679\u001b[0m\u001b[0m | time: 0.171s\n",
            "| Adam | epoch: 198 | loss: 0.05679 - acc: 0.9778 -- iter: 384/963\n",
            "Training Step: 23886  | total loss: \u001b[1m\u001b[32m0.06601\u001b[0m\u001b[0m | time: 0.174s\n",
            "| Adam | epoch: 198 | loss: 0.06601 - acc: 0.9675 -- iter: 392/963\n",
            "Training Step: 23887  | total loss: \u001b[1m\u001b[32m0.05947\u001b[0m\u001b[0m | time: 0.176s\n",
            "| Adam | epoch: 198 | loss: 0.05947 - acc: 0.9708 -- iter: 400/963\n",
            "Training Step: 23888  | total loss: \u001b[1m\u001b[32m0.05353\u001b[0m\u001b[0m | time: 0.179s\n",
            "| Adam | epoch: 198 | loss: 0.05353 - acc: 0.9737 -- iter: 408/963\n",
            "Training Step: 23889  | total loss: \u001b[1m\u001b[32m0.04820\u001b[0m\u001b[0m | time: 0.181s\n",
            "| Adam | epoch: 198 | loss: 0.04820 - acc: 0.9763 -- iter: 416/963\n",
            "Training Step: 23890  | total loss: \u001b[1m\u001b[32m0.04339\u001b[0m\u001b[0m | time: 0.183s\n",
            "| Adam | epoch: 198 | loss: 0.04339 - acc: 0.9787 -- iter: 424/963\n",
            "Training Step: 23891  | total loss: \u001b[1m\u001b[32m0.03908\u001b[0m\u001b[0m | time: 0.188s\n",
            "| Adam | epoch: 198 | loss: 0.03908 - acc: 0.9808 -- iter: 432/963\n",
            "Training Step: 23892  | total loss: \u001b[1m\u001b[32m0.04611\u001b[0m\u001b[0m | time: 0.191s\n",
            "| Adam | epoch: 198 | loss: 0.04611 - acc: 0.9703 -- iter: 440/963\n",
            "Training Step: 23893  | total loss: \u001b[1m\u001b[32m0.04153\u001b[0m\u001b[0m | time: 0.194s\n",
            "| Adam | epoch: 198 | loss: 0.04153 - acc: 0.9732 -- iter: 448/963\n",
            "Training Step: 23894  | total loss: \u001b[1m\u001b[32m0.03739\u001b[0m\u001b[0m | time: 0.200s\n",
            "| Adam | epoch: 198 | loss: 0.03739 - acc: 0.9759 -- iter: 456/963\n",
            "Training Step: 23895  | total loss: \u001b[1m\u001b[32m0.03366\u001b[0m\u001b[0m | time: 0.204s\n",
            "| Adam | epoch: 198 | loss: 0.03366 - acc: 0.9783 -- iter: 464/963\n",
            "Training Step: 23896  | total loss: \u001b[1m\u001b[32m0.03030\u001b[0m\u001b[0m | time: 0.208s\n",
            "| Adam | epoch: 198 | loss: 0.03030 - acc: 0.9805 -- iter: 472/963\n",
            "Training Step: 23897  | total loss: \u001b[1m\u001b[32m0.02731\u001b[0m\u001b[0m | time: 0.211s\n",
            "| Adam | epoch: 198 | loss: 0.02731 - acc: 0.9824 -- iter: 480/963\n",
            "Training Step: 23898  | total loss: \u001b[1m\u001b[32m0.02478\u001b[0m\u001b[0m | time: 0.216s\n",
            "| Adam | epoch: 198 | loss: 0.02478 - acc: 0.9842 -- iter: 488/963\n",
            "Training Step: 23899  | total loss: \u001b[1m\u001b[32m0.02231\u001b[0m\u001b[0m | time: 0.221s\n",
            "| Adam | epoch: 198 | loss: 0.02231 - acc: 0.9858 -- iter: 496/963\n",
            "Training Step: 23900  | total loss: \u001b[1m\u001b[32m0.02010\u001b[0m\u001b[0m | time: 0.224s\n",
            "| Adam | epoch: 198 | loss: 0.02010 - acc: 0.9872 -- iter: 504/963\n",
            "Training Step: 23901  | total loss: \u001b[1m\u001b[32m0.01817\u001b[0m\u001b[0m | time: 0.228s\n",
            "| Adam | epoch: 198 | loss: 0.01817 - acc: 0.9885 -- iter: 512/963\n",
            "Training Step: 23902  | total loss: \u001b[1m\u001b[32m0.01637\u001b[0m\u001b[0m | time: 0.231s\n",
            "| Adam | epoch: 198 | loss: 0.01637 - acc: 0.9896 -- iter: 520/963\n",
            "Training Step: 23903  | total loss: \u001b[1m\u001b[32m0.03580\u001b[0m\u001b[0m | time: 0.234s\n",
            "| Adam | epoch: 198 | loss: 0.03580 - acc: 0.9782 -- iter: 528/963\n",
            "Training Step: 23904  | total loss: \u001b[1m\u001b[32m0.03225\u001b[0m\u001b[0m | time: 0.237s\n",
            "| Adam | epoch: 198 | loss: 0.03225 - acc: 0.9803 -- iter: 536/963\n",
            "Training Step: 23905  | total loss: \u001b[1m\u001b[32m0.02903\u001b[0m\u001b[0m | time: 0.240s\n",
            "| Adam | epoch: 198 | loss: 0.02903 - acc: 0.9823 -- iter: 544/963\n",
            "Training Step: 23906  | total loss: \u001b[1m\u001b[32m0.03284\u001b[0m\u001b[0m | time: 0.244s\n",
            "| Adam | epoch: 198 | loss: 0.03284 - acc: 0.9841 -- iter: 552/963\n",
            "Training Step: 23907  | total loss: \u001b[1m\u001b[32m0.02957\u001b[0m\u001b[0m | time: 0.247s\n",
            "| Adam | epoch: 198 | loss: 0.02957 - acc: 0.9857 -- iter: 560/963\n",
            "Training Step: 23908  | total loss: \u001b[1m\u001b[32m0.02662\u001b[0m\u001b[0m | time: 0.252s\n",
            "| Adam | epoch: 198 | loss: 0.02662 - acc: 0.9871 -- iter: 568/963\n",
            "Training Step: 23909  | total loss: \u001b[1m\u001b[32m0.02158\u001b[0m\u001b[0m | time: 0.256s\n",
            "| Adam | epoch: 198 | loss: 0.02158 - acc: 0.9884 -- iter: 576/963\n",
            "Training Step: 23910  | total loss: \u001b[1m\u001b[32m0.02158\u001b[0m\u001b[0m | time: 0.259s\n",
            "| Adam | epoch: 198 | loss: 0.02158 - acc: 0.9896 -- iter: 584/963\n",
            "Training Step: 23911  | total loss: \u001b[1m\u001b[32m0.01944\u001b[0m\u001b[0m | time: 0.262s\n",
            "| Adam | epoch: 198 | loss: 0.01944 - acc: 0.9906 -- iter: 592/963\n",
            "Training Step: 23912  | total loss: \u001b[1m\u001b[32m0.01750\u001b[0m\u001b[0m | time: 0.265s\n",
            "| Adam | epoch: 198 | loss: 0.01750 - acc: 0.9915 -- iter: 600/963\n",
            "Training Step: 23913  | total loss: \u001b[1m\u001b[32m0.01576\u001b[0m\u001b[0m | time: 0.269s\n",
            "| Adam | epoch: 198 | loss: 0.01576 - acc: 0.9924 -- iter: 608/963\n",
            "Training Step: 23914  | total loss: \u001b[1m\u001b[32m0.01419\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 198 | loss: 0.01419 - acc: 0.9931 -- iter: 616/963\n",
            "Training Step: 23915  | total loss: \u001b[1m\u001b[32m0.01288\u001b[0m\u001b[0m | time: 0.276s\n",
            "| Adam | epoch: 198 | loss: 0.01288 - acc: 0.9938 -- iter: 624/963\n",
            "Training Step: 23916  | total loss: \u001b[1m\u001b[32m0.01161\u001b[0m\u001b[0m | time: 0.280s\n",
            "| Adam | epoch: 198 | loss: 0.01161 - acc: 0.9944 -- iter: 632/963\n",
            "Training Step: 23917  | total loss: \u001b[1m\u001b[32m0.04237\u001b[0m\u001b[0m | time: 0.284s\n",
            "| Adam | epoch: 198 | loss: 0.04237 - acc: 0.9843 -- iter: 640/963\n",
            "Training Step: 23918  | total loss: \u001b[1m\u001b[32m0.03817\u001b[0m\u001b[0m | time: 0.288s\n",
            "| Adam | epoch: 198 | loss: 0.03817 - acc: 0.9843 -- iter: 648/963\n",
            "Training Step: 23919  | total loss: \u001b[1m\u001b[32m0.03437\u001b[0m\u001b[0m | time: 0.293s\n",
            "| Adam | epoch: 198 | loss: 0.03437 - acc: 0.9858 -- iter: 656/963\n",
            "Training Step: 23920  | total loss: \u001b[1m\u001b[32m0.03094\u001b[0m\u001b[0m | time: 0.297s\n",
            "| Adam | epoch: 198 | loss: 0.03094 - acc: 0.9872 -- iter: 664/963\n",
            "Training Step: 23921  | total loss: \u001b[1m\u001b[32m0.05415\u001b[0m\u001b[0m | time: 0.301s\n",
            "| Adam | epoch: 198 | loss: 0.05415 - acc: 0.9760 -- iter: 672/963\n",
            "Training Step: 23922  | total loss: \u001b[1m\u001b[32m0.04878\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 198 | loss: 0.04878 - acc: 0.9784 -- iter: 680/963\n",
            "Training Step: 23923  | total loss: \u001b[1m\u001b[32m0.04391\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 198 | loss: 0.04391 - acc: 0.9806 -- iter: 688/963\n",
            "Training Step: 23924  | total loss: \u001b[1m\u001b[32m0.03954\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 198 | loss: 0.03954 - acc: 0.9825 -- iter: 696/963\n",
            "Training Step: 23925  | total loss: \u001b[1m\u001b[32m0.03559\u001b[0m\u001b[0m | time: 0.311s\n",
            "| Adam | epoch: 198 | loss: 0.03559 - acc: 0.9843 -- iter: 704/963\n",
            "Training Step: 23926  | total loss: \u001b[1m\u001b[32m0.03204\u001b[0m\u001b[0m | time: 0.314s\n",
            "| Adam | epoch: 198 | loss: 0.03204 - acc: 0.9858 -- iter: 712/963\n",
            "Training Step: 23927  | total loss: \u001b[1m\u001b[32m0.02886\u001b[0m\u001b[0m | time: 0.317s\n",
            "| Adam | epoch: 198 | loss: 0.02886 - acc: 0.9873 -- iter: 720/963\n",
            "Training Step: 23928  | total loss: \u001b[1m\u001b[32m0.02621\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 198 | loss: 0.02621 - acc: 0.9885 -- iter: 728/963\n",
            "Training Step: 23929  | total loss: \u001b[1m\u001b[32m0.02361\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 198 | loss: 0.02361 - acc: 0.9897 -- iter: 736/963\n",
            "Training Step: 23930  | total loss: \u001b[1m\u001b[32m0.02128\u001b[0m\u001b[0m | time: 0.328s\n",
            "| Adam | epoch: 198 | loss: 0.02128 - acc: 0.9907 -- iter: 744/963\n",
            "Training Step: 23931  | total loss: \u001b[1m\u001b[32m0.01917\u001b[0m\u001b[0m | time: 0.331s\n",
            "| Adam | epoch: 198 | loss: 0.01917 - acc: 0.9916 -- iter: 752/963\n",
            "Training Step: 23932  | total loss: \u001b[1m\u001b[32m0.01731\u001b[0m\u001b[0m | time: 0.334s\n",
            "| Adam | epoch: 198 | loss: 0.01731 - acc: 0.9925 -- iter: 760/963\n",
            "Training Step: 23933  | total loss: \u001b[1m\u001b[32m0.01560\u001b[0m\u001b[0m | time: 0.338s\n",
            "| Adam | epoch: 198 | loss: 0.01560 - acc: 0.9932 -- iter: 768/963\n",
            "Training Step: 23934  | total loss: \u001b[1m\u001b[32m0.01406\u001b[0m\u001b[0m | time: 0.341s\n",
            "| Adam | epoch: 198 | loss: 0.01406 - acc: 0.9939 -- iter: 776/963\n",
            "Training Step: 23935  | total loss: \u001b[1m\u001b[32m0.01266\u001b[0m\u001b[0m | time: 0.344s\n",
            "| Adam | epoch: 198 | loss: 0.01266 - acc: 0.9945 -- iter: 784/963\n",
            "Training Step: 23936  | total loss: \u001b[1m\u001b[32m0.01883\u001b[0m\u001b[0m | time: 0.347s\n",
            "| Adam | epoch: 198 | loss: 0.01883 - acc: 0.9951 -- iter: 792/963\n",
            "Training Step: 23937  | total loss: \u001b[1m\u001b[32m0.02016\u001b[0m\u001b[0m | time: 0.350s\n",
            "| Adam | epoch: 198 | loss: 0.02016 - acc: 0.9956 -- iter: 800/963\n",
            "Training Step: 23938  | total loss: \u001b[1m\u001b[32m0.01815\u001b[0m\u001b[0m | time: 0.353s\n",
            "| Adam | epoch: 198 | loss: 0.01815 - acc: 0.9960 -- iter: 808/963\n",
            "Training Step: 23939  | total loss: \u001b[1m\u001b[32m0.01634\u001b[0m\u001b[0m | time: 0.355s\n",
            "| Adam | epoch: 198 | loss: 0.01634 - acc: 0.9964 -- iter: 816/963\n",
            "Training Step: 23940  | total loss: \u001b[1m\u001b[32m0.01471\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 198 | loss: 0.01471 - acc: 0.9968 -- iter: 824/963\n",
            "Training Step: 23941  | total loss: \u001b[1m\u001b[32m0.01329\u001b[0m\u001b[0m | time: 0.361s\n",
            "| Adam | epoch: 198 | loss: 0.01329 - acc: 0.9971 -- iter: 832/963\n",
            "Training Step: 23942  | total loss: \u001b[1m\u001b[32m0.01198\u001b[0m\u001b[0m | time: 0.364s\n",
            "| Adam | epoch: 198 | loss: 0.01198 - acc: 0.9974 -- iter: 840/963\n",
            "Training Step: 23943  | total loss: \u001b[1m\u001b[32m0.01079\u001b[0m\u001b[0m | time: 0.367s\n",
            "| Adam | epoch: 198 | loss: 0.01079 - acc: 0.9976 -- iter: 848/963\n",
            "Training Step: 23944  | total loss: \u001b[1m\u001b[32m0.00972\u001b[0m\u001b[0m | time: 0.370s\n",
            "| Adam | epoch: 198 | loss: 0.00972 - acc: 0.9979 -- iter: 856/963\n",
            "Training Step: 23945  | total loss: \u001b[1m\u001b[32m0.00892\u001b[0m\u001b[0m | time: 0.373s\n",
            "| Adam | epoch: 198 | loss: 0.00892 - acc: 0.9981 -- iter: 864/963\n",
            "Training Step: 23946  | total loss: \u001b[1m\u001b[32m0.00805\u001b[0m\u001b[0m | time: 0.377s\n",
            "| Adam | epoch: 198 | loss: 0.00805 - acc: 0.9983 -- iter: 872/963\n",
            "Training Step: 23947  | total loss: \u001b[1m\u001b[32m0.00725\u001b[0m\u001b[0m | time: 0.380s\n",
            "| Adam | epoch: 198 | loss: 0.00725 - acc: 0.9985 -- iter: 880/963\n",
            "Training Step: 23948  | total loss: \u001b[1m\u001b[32m0.01818\u001b[0m\u001b[0m | time: 0.383s\n",
            "| Adam | epoch: 198 | loss: 0.01818 - acc: 0.9861 -- iter: 888/963\n",
            "Training Step: 23949  | total loss: \u001b[1m\u001b[32m0.01636\u001b[0m\u001b[0m | time: 0.386s\n",
            "| Adam | epoch: 198 | loss: 0.01636 - acc: 0.9875 -- iter: 896/963\n",
            "Training Step: 23950  | total loss: \u001b[1m\u001b[32m0.01476\u001b[0m\u001b[0m | time: 0.388s\n",
            "| Adam | epoch: 198 | loss: 0.01476 - acc: 0.9887 -- iter: 904/963\n",
            "Training Step: 23951  | total loss: \u001b[1m\u001b[32m0.01331\u001b[0m\u001b[0m | time: 0.391s\n",
            "| Adam | epoch: 198 | loss: 0.01331 - acc: 0.9899 -- iter: 912/963\n",
            "Training Step: 23952  | total loss: \u001b[1m\u001b[32m0.04144\u001b[0m\u001b[0m | time: 0.393s\n",
            "| Adam | epoch: 198 | loss: 0.04144 - acc: 0.9784 -- iter: 920/963\n",
            "Training Step: 23953  | total loss: \u001b[1m\u001b[32m0.03731\u001b[0m\u001b[0m | time: 0.396s\n",
            "| Adam | epoch: 198 | loss: 0.03731 - acc: 0.9805 -- iter: 928/963\n",
            "Training Step: 23954  | total loss: \u001b[1m\u001b[32m0.03358\u001b[0m\u001b[0m | time: 0.400s\n",
            "| Adam | epoch: 198 | loss: 0.03358 - acc: 0.9825 -- iter: 936/963\n",
            "Training Step: 23955  | total loss: \u001b[1m\u001b[32m0.03024\u001b[0m\u001b[0m | time: 0.403s\n",
            "| Adam | epoch: 198 | loss: 0.03024 - acc: 0.9842 -- iter: 944/963\n",
            "Training Step: 23956  | total loss: \u001b[1m\u001b[32m0.02735\u001b[0m\u001b[0m | time: 0.405s\n",
            "| Adam | epoch: 198 | loss: 0.02735 - acc: 0.9858 -- iter: 952/963\n",
            "Training Step: 23957  | total loss: \u001b[1m\u001b[32m0.06134\u001b[0m\u001b[0m | time: 0.408s\n",
            "| Adam | epoch: 198 | loss: 0.06134 - acc: 0.9747 -- iter: 960/963\n",
            "Training Step: 23958  | total loss: \u001b[1m\u001b[32m0.05524\u001b[0m\u001b[0m | time: 0.411s\n",
            "| Adam | epoch: 198 | loss: 0.05524 - acc: 0.9773 -- iter: 963/963\n",
            "--\n",
            "Training Step: 23959  | total loss: \u001b[1m\u001b[32m0.04974\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 199 | loss: 0.04974 - acc: 0.9795 -- iter: 008/963\n",
            "Training Step: 23960  | total loss: \u001b[1m\u001b[32m0.04480\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 199 | loss: 0.04480 - acc: 0.9816 -- iter: 016/963\n",
            "Training Step: 23961  | total loss: \u001b[1m\u001b[32m0.05010\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 199 | loss: 0.05010 - acc: 0.9834 -- iter: 024/963\n",
            "Training Step: 23962  | total loss: \u001b[1m\u001b[32m0.04511\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 199 | loss: 0.04511 - acc: 0.9851 -- iter: 032/963\n",
            "Training Step: 23963  | total loss: \u001b[1m\u001b[32m0.04063\u001b[0m\u001b[0m | time: 0.012s\n",
            "| Adam | epoch: 199 | loss: 0.04063 - acc: 0.9866 -- iter: 040/963\n",
            "Training Step: 23964  | total loss: \u001b[1m\u001b[32m0.03660\u001b[0m\u001b[0m | time: 0.015s\n",
            "| Adam | epoch: 199 | loss: 0.03660 - acc: 0.9879 -- iter: 048/963\n",
            "Training Step: 23965  | total loss: \u001b[1m\u001b[32m0.03298\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 199 | loss: 0.03298 - acc: 0.9891 -- iter: 056/963\n",
            "Training Step: 23966  | total loss: \u001b[1m\u001b[32m0.02969\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 199 | loss: 0.02969 - acc: 0.9902 -- iter: 064/963\n",
            "Training Step: 23967  | total loss: \u001b[1m\u001b[32m0.02674\u001b[0m\u001b[0m | time: 0.023s\n",
            "| Adam | epoch: 199 | loss: 0.02674 - acc: 0.9912 -- iter: 072/963\n",
            "Training Step: 23968  | total loss: \u001b[1m\u001b[32m0.04986\u001b[0m\u001b[0m | time: 0.027s\n",
            "| Adam | epoch: 199 | loss: 0.04986 - acc: 0.9796 -- iter: 080/963\n",
            "Training Step: 23969  | total loss: \u001b[1m\u001b[32m0.04489\u001b[0m\u001b[0m | time: 0.030s\n",
            "| Adam | epoch: 199 | loss: 0.04489 - acc: 0.9816 -- iter: 088/963\n",
            "Training Step: 23970  | total loss: \u001b[1m\u001b[32m0.04041\u001b[0m\u001b[0m | time: 0.032s\n",
            "| Adam | epoch: 199 | loss: 0.04041 - acc: 0.9835 -- iter: 096/963\n",
            "Training Step: 23971  | total loss: \u001b[1m\u001b[32m0.03639\u001b[0m\u001b[0m | time: 0.035s\n",
            "| Adam | epoch: 199 | loss: 0.03639 - acc: 0.9866 -- iter: 104/963\n",
            "Training Step: 23972  | total loss: \u001b[1m\u001b[32m0.03278\u001b[0m\u001b[0m | time: 0.038s\n",
            "| Adam | epoch: 199 | loss: 0.03278 - acc: 0.9866 -- iter: 112/963\n",
            "Training Step: 23973  | total loss: \u001b[1m\u001b[32m0.02951\u001b[0m\u001b[0m | time: 0.040s\n",
            "| Adam | epoch: 199 | loss: 0.02951 - acc: 0.9879 -- iter: 120/963\n",
            "Training Step: 23974  | total loss: \u001b[1m\u001b[32m0.02659\u001b[0m\u001b[0m | time: 0.043s\n",
            "| Adam | epoch: 199 | loss: 0.02659 - acc: 0.9891 -- iter: 128/963\n",
            "Training Step: 23975  | total loss: \u001b[1m\u001b[32m0.02396\u001b[0m\u001b[0m | time: 0.045s\n",
            "| Adam | epoch: 199 | loss: 0.02396 - acc: 0.9902 -- iter: 136/963\n",
            "Training Step: 23976  | total loss: \u001b[1m\u001b[32m0.02166\u001b[0m\u001b[0m | time: 0.048s\n",
            "| Adam | epoch: 199 | loss: 0.02166 - acc: 0.9912 -- iter: 144/963\n",
            "Training Step: 23977  | total loss: \u001b[1m\u001b[32m0.01957\u001b[0m\u001b[0m | time: 0.051s\n",
            "| Adam | epoch: 199 | loss: 0.01957 - acc: 0.9921 -- iter: 152/963\n",
            "Training Step: 23978  | total loss: \u001b[1m\u001b[32m0.01763\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 199 | loss: 0.01763 - acc: 0.9929 -- iter: 160/963\n",
            "Training Step: 23979  | total loss: \u001b[1m\u001b[32m0.01588\u001b[0m\u001b[0m | time: 0.056s\n",
            "| Adam | epoch: 199 | loss: 0.01588 - acc: 0.9936 -- iter: 168/963\n",
            "Training Step: 23980  | total loss: \u001b[1m\u001b[32m0.01432\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 199 | loss: 0.01432 - acc: 0.9942 -- iter: 176/963\n",
            "Training Step: 23981  | total loss: \u001b[1m\u001b[32m0.01290\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 199 | loss: 0.01290 - acc: 0.9948 -- iter: 184/963\n",
            "Training Step: 23982  | total loss: \u001b[1m\u001b[32m0.04281\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 199 | loss: 0.04281 - acc: 0.9828 -- iter: 192/963\n",
            "Training Step: 23983  | total loss: \u001b[1m\u001b[32m0.03857\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 199 | loss: 0.03857 - acc: 0.9845 -- iter: 200/963\n",
            "Training Step: 23984  | total loss: \u001b[1m\u001b[32m0.03473\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 199 | loss: 0.03473 - acc: 0.9861 -- iter: 208/963\n",
            "Training Step: 23985  | total loss: \u001b[1m\u001b[32m0.03127\u001b[0m\u001b[0m | time: 0.072s\n",
            "| Adam | epoch: 199 | loss: 0.03127 - acc: 0.9875 -- iter: 216/963\n",
            "Training Step: 23986  | total loss: \u001b[1m\u001b[32m0.02815\u001b[0m\u001b[0m | time: 0.075s\n",
            "| Adam | epoch: 199 | loss: 0.02815 - acc: 0.9887 -- iter: 224/963\n",
            "Training Step: 23987  | total loss: \u001b[1m\u001b[32m0.02535\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 199 | loss: 0.02535 - acc: 0.9899 -- iter: 232/963\n",
            "Training Step: 23988  | total loss: \u001b[1m\u001b[32m0.02284\u001b[0m\u001b[0m | time: 0.080s\n",
            "| Adam | epoch: 199 | loss: 0.02284 - acc: 0.9909 -- iter: 240/963\n",
            "Training Step: 23989  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.083s\n",
            "| Adam | epoch: 199 | loss: 0.02057 - acc: 0.9918 -- iter: 248/963\n",
            "Training Step: 23990  | total loss: \u001b[1m\u001b[32m0.01852\u001b[0m\u001b[0m | time: 0.086s\n",
            "| Adam | epoch: 199 | loss: 0.01852 - acc: 0.9926 -- iter: 256/963\n",
            "Training Step: 23991  | total loss: \u001b[1m\u001b[32m0.01669\u001b[0m\u001b[0m | time: 0.089s\n",
            "| Adam | epoch: 199 | loss: 0.01669 - acc: 0.9933 -- iter: 264/963\n",
            "Training Step: 23992  | total loss: \u001b[1m\u001b[32m0.01528\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 199 | loss: 0.01528 - acc: 0.9940 -- iter: 272/963\n",
            "Training Step: 23993  | total loss: \u001b[1m\u001b[32m0.01377\u001b[0m\u001b[0m | time: 0.094s\n",
            "| Adam | epoch: 199 | loss: 0.01377 - acc: 0.9946 -- iter: 280/963\n",
            "Training Step: 23994  | total loss: \u001b[1m\u001b[32m0.01241\u001b[0m\u001b[0m | time: 0.097s\n",
            "| Adam | epoch: 199 | loss: 0.01241 - acc: 0.9951 -- iter: 288/963\n",
            "Training Step: 23995  | total loss: \u001b[1m\u001b[32m0.01118\u001b[0m\u001b[0m | time: 0.101s\n",
            "| Adam | epoch: 199 | loss: 0.01118 - acc: 0.9956 -- iter: 296/963\n",
            "Training Step: 23996  | total loss: \u001b[1m\u001b[32m0.01007\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 199 | loss: 0.01007 - acc: 0.9961 -- iter: 304/963\n",
            "Training Step: 23997  | total loss: \u001b[1m\u001b[32m0.00907\u001b[0m\u001b[0m | time: 0.109s\n",
            "| Adam | epoch: 199 | loss: 0.00907 - acc: 0.9965 -- iter: 312/963\n",
            "Training Step: 23998  | total loss: \u001b[1m\u001b[32m0.00817\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 199 | loss: 0.00817 - acc: 0.9968 -- iter: 320/963\n",
            "Training Step: 23999  | total loss: \u001b[1m\u001b[32m0.00737\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 199 | loss: 0.00737 - acc: 0.9971 -- iter: 328/963\n",
            "Training Step: 24000  | total loss: \u001b[1m\u001b[32m0.00665\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 199 | loss: 0.00665 - acc: 0.9974 -- iter: 336/963\n",
            "Training Step: 24001  | total loss: \u001b[1m\u001b[32m0.00601\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 199 | loss: 0.00601 - acc: 0.9977 -- iter: 344/963\n",
            "Training Step: 24002  | total loss: \u001b[1m\u001b[32m0.00546\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 199 | loss: 0.00546 - acc: 0.9979 -- iter: 352/963\n",
            "Training Step: 24003  | total loss: \u001b[1m\u001b[32m0.00492\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 199 | loss: 0.00492 - acc: 0.9981 -- iter: 360/963\n",
            "Training Step: 24004  | total loss: \u001b[1m\u001b[32m0.00445\u001b[0m\u001b[0m | time: 0.133s\n",
            "| Adam | epoch: 199 | loss: 0.00445 - acc: 0.9983 -- iter: 368/963\n",
            "Training Step: 24005  | total loss: \u001b[1m\u001b[32m0.00402\u001b[0m\u001b[0m | time: 0.136s\n",
            "| Adam | epoch: 199 | loss: 0.00402 - acc: 0.9985 -- iter: 376/963\n",
            "Training Step: 24006  | total loss: \u001b[1m\u001b[32m0.00376\u001b[0m\u001b[0m | time: 0.140s\n",
            "| Adam | epoch: 199 | loss: 0.00376 - acc: 0.9986 -- iter: 384/963\n",
            "Training Step: 24007  | total loss: \u001b[1m\u001b[32m0.00342\u001b[0m\u001b[0m | time: 0.143s\n",
            "| Adam | epoch: 199 | loss: 0.00342 - acc: 0.9988 -- iter: 392/963\n",
            "Training Step: 24008  | total loss: \u001b[1m\u001b[32m0.00310\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 199 | loss: 0.00310 - acc: 0.9989 -- iter: 400/963\n",
            "Training Step: 24009  | total loss: \u001b[1m\u001b[32m0.00283\u001b[0m\u001b[0m | time: 0.150s\n",
            "| Adam | epoch: 199 | loss: 0.00283 - acc: 0.9990 -- iter: 408/963\n",
            "Training Step: 24010  | total loss: \u001b[1m\u001b[32m0.00256\u001b[0m\u001b[0m | time: 0.154s\n",
            "| Adam | epoch: 199 | loss: 0.00256 - acc: 0.9991 -- iter: 416/963\n",
            "Training Step: 24011  | total loss: \u001b[1m\u001b[32m0.00231\u001b[0m\u001b[0m | time: 0.156s\n",
            "| Adam | epoch: 199 | loss: 0.00231 - acc: 0.9992 -- iter: 424/963\n",
            "Training Step: 24012  | total loss: \u001b[1m\u001b[32m0.00209\u001b[0m\u001b[0m | time: 0.159s\n",
            "| Adam | epoch: 199 | loss: 0.00209 - acc: 0.9993 -- iter: 432/963\n",
            "Training Step: 24013  | total loss: \u001b[1m\u001b[32m0.00190\u001b[0m\u001b[0m | time: 0.162s\n",
            "| Adam | epoch: 199 | loss: 0.00190 - acc: 0.9993 -- iter: 440/963\n",
            "Training Step: 24014  | total loss: \u001b[1m\u001b[32m0.00173\u001b[0m\u001b[0m | time: 0.165s\n",
            "| Adam | epoch: 199 | loss: 0.00173 - acc: 0.9994 -- iter: 448/963\n",
            "Training Step: 24015  | total loss: \u001b[1m\u001b[32m0.00157\u001b[0m\u001b[0m | time: 0.168s\n",
            "| Adam | epoch: 199 | loss: 0.00157 - acc: 0.9995 -- iter: 456/963\n",
            "Training Step: 24016  | total loss: \u001b[1m\u001b[32m0.00143\u001b[0m\u001b[0m | time: 0.171s\n",
            "| Adam | epoch: 199 | loss: 0.00143 - acc: 0.9995 -- iter: 464/963\n",
            "Training Step: 24017  | total loss: \u001b[1m\u001b[32m0.00129\u001b[0m\u001b[0m | time: 0.174s\n",
            "| Adam | epoch: 199 | loss: 0.00129 - acc: 0.9996 -- iter: 472/963\n",
            "Training Step: 24018  | total loss: \u001b[1m\u001b[32m0.00118\u001b[0m\u001b[0m | time: 0.176s\n",
            "| Adam | epoch: 199 | loss: 0.00118 - acc: 0.9996 -- iter: 480/963\n",
            "Training Step: 24019  | total loss: \u001b[1m\u001b[32m0.00107\u001b[0m\u001b[0m | time: 0.179s\n",
            "| Adam | epoch: 199 | loss: 0.00107 - acc: 0.9997 -- iter: 488/963\n",
            "Training Step: 24020  | total loss: \u001b[1m\u001b[32m0.00098\u001b[0m\u001b[0m | time: 0.182s\n",
            "| Adam | epoch: 199 | loss: 0.00098 - acc: 0.9997 -- iter: 496/963\n",
            "Training Step: 24021  | total loss: \u001b[1m\u001b[32m0.00089\u001b[0m\u001b[0m | time: 0.186s\n",
            "| Adam | epoch: 199 | loss: 0.00089 - acc: 0.9997 -- iter: 504/963\n",
            "Training Step: 24022  | total loss: \u001b[1m\u001b[32m0.01128\u001b[0m\u001b[0m | time: 0.189s\n",
            "| Adam | epoch: 199 | loss: 0.01128 - acc: 0.9872 -- iter: 512/963\n",
            "Training Step: 24023  | total loss: \u001b[1m\u001b[32m0.01016\u001b[0m\u001b[0m | time: 0.192s\n",
            "| Adam | epoch: 199 | loss: 0.01016 - acc: 0.9885 -- iter: 520/963\n",
            "Training Step: 24024  | total loss: \u001b[1m\u001b[32m0.00917\u001b[0m\u001b[0m | time: 0.195s\n",
            "| Adam | epoch: 199 | loss: 0.00917 - acc: 0.9897 -- iter: 528/963\n",
            "Training Step: 24025  | total loss: \u001b[1m\u001b[32m0.02060\u001b[0m\u001b[0m | time: 0.198s\n",
            "| Adam | epoch: 199 | loss: 0.02060 - acc: 0.9782 -- iter: 536/963\n",
            "Training Step: 24026  | total loss: \u001b[1m\u001b[32m0.01856\u001b[0m\u001b[0m | time: 0.201s\n",
            "| Adam | epoch: 199 | loss: 0.01856 - acc: 0.9804 -- iter: 544/963\n",
            "Training Step: 24027  | total loss: \u001b[1m\u001b[32m0.01673\u001b[0m\u001b[0m | time: 0.204s\n",
            "| Adam | epoch: 199 | loss: 0.01673 - acc: 0.9823 -- iter: 552/963\n",
            "Training Step: 24028  | total loss: \u001b[1m\u001b[32m0.02487\u001b[0m\u001b[0m | time: 0.207s\n",
            "| Adam | epoch: 199 | loss: 0.02487 - acc: 0.9716 -- iter: 560/963\n",
            "Training Step: 24029  | total loss: \u001b[1m\u001b[32m0.02240\u001b[0m\u001b[0m | time: 0.210s\n",
            "| Adam | epoch: 199 | loss: 0.02240 - acc: 0.9744 -- iter: 568/963\n",
            "Training Step: 24030  | total loss: \u001b[1m\u001b[32m0.02025\u001b[0m\u001b[0m | time: 0.212s\n",
            "| Adam | epoch: 199 | loss: 0.02025 - acc: 0.9770 -- iter: 576/963\n",
            "Training Step: 24031  | total loss: \u001b[1m\u001b[32m0.01823\u001b[0m\u001b[0m | time: 0.215s\n",
            "| Adam | epoch: 199 | loss: 0.01823 - acc: 0.9793 -- iter: 584/963\n",
            "Training Step: 24032  | total loss: \u001b[1m\u001b[32m0.01642\u001b[0m\u001b[0m | time: 0.217s\n",
            "| Adam | epoch: 199 | loss: 0.01642 - acc: 0.9814 -- iter: 592/963\n",
            "Training Step: 24033  | total loss: \u001b[1m\u001b[32m0.01480\u001b[0m\u001b[0m | time: 0.220s\n",
            "| Adam | epoch: 199 | loss: 0.01480 - acc: 0.9832 -- iter: 600/963\n",
            "Training Step: 24034  | total loss: \u001b[1m\u001b[32m0.01336\u001b[0m\u001b[0m | time: 0.223s\n",
            "| Adam | epoch: 199 | loss: 0.01336 - acc: 0.9849 -- iter: 608/963\n",
            "Training Step: 24035  | total loss: \u001b[1m\u001b[32m0.01205\u001b[0m\u001b[0m | time: 0.226s\n",
            "| Adam | epoch: 199 | loss: 0.01205 - acc: 0.9864 -- iter: 616/963\n",
            "Training Step: 24036  | total loss: \u001b[1m\u001b[32m0.01086\u001b[0m\u001b[0m | time: 0.228s\n",
            "| Adam | epoch: 199 | loss: 0.01086 - acc: 0.9878 -- iter: 624/963\n",
            "Training Step: 24037  | total loss: \u001b[1m\u001b[32m0.00978\u001b[0m\u001b[0m | time: 0.232s\n",
            "| Adam | epoch: 199 | loss: 0.00978 - acc: 0.9890 -- iter: 632/963\n",
            "Training Step: 24038  | total loss: \u001b[1m\u001b[32m0.01728\u001b[0m\u001b[0m | time: 0.236s\n",
            "| Adam | epoch: 199 | loss: 0.01728 - acc: 0.9901 -- iter: 640/963\n",
            "Training Step: 24039  | total loss: \u001b[1m\u001b[32m0.01557\u001b[0m\u001b[0m | time: 0.238s\n",
            "| Adam | epoch: 199 | loss: 0.01557 - acc: 0.9911 -- iter: 648/963\n",
            "Training Step: 24040  | total loss: \u001b[1m\u001b[32m0.01402\u001b[0m\u001b[0m | time: 0.241s\n",
            "| Adam | epoch: 199 | loss: 0.01402 - acc: 0.9920 -- iter: 656/963\n",
            "Training Step: 24041  | total loss: \u001b[1m\u001b[32m0.01263\u001b[0m\u001b[0m | time: 0.244s\n",
            "| Adam | epoch: 199 | loss: 0.01263 - acc: 0.9928 -- iter: 664/963\n",
            "Training Step: 24042  | total loss: \u001b[1m\u001b[32m0.01138\u001b[0m\u001b[0m | time: 0.247s\n",
            "| Adam | epoch: 199 | loss: 0.01138 - acc: 0.9935 -- iter: 672/963\n",
            "Training Step: 24043  | total loss: \u001b[1m\u001b[32m0.01026\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 199 | loss: 0.01026 - acc: 0.9942 -- iter: 680/963\n",
            "Training Step: 24044  | total loss: \u001b[1m\u001b[32m0.01116\u001b[0m\u001b[0m | time: 0.253s\n",
            "| Adam | epoch: 199 | loss: 0.01116 - acc: 0.9947 -- iter: 688/963\n",
            "Training Step: 24045  | total loss: \u001b[1m\u001b[32m0.01006\u001b[0m\u001b[0m | time: 0.255s\n",
            "| Adam | epoch: 199 | loss: 0.01006 - acc: 0.9953 -- iter: 696/963\n",
            "Training Step: 24046  | total loss: \u001b[1m\u001b[32m0.00913\u001b[0m\u001b[0m | time: 0.258s\n",
            "| Adam | epoch: 199 | loss: 0.00913 - acc: 0.9957 -- iter: 704/963\n",
            "Training Step: 24047  | total loss: \u001b[1m\u001b[32m0.00823\u001b[0m\u001b[0m | time: 0.260s\n",
            "| Adam | epoch: 199 | loss: 0.00823 - acc: 0.9962 -- iter: 712/963\n",
            "Training Step: 24048  | total loss: \u001b[1m\u001b[32m0.00741\u001b[0m\u001b[0m | time: 0.263s\n",
            "| Adam | epoch: 199 | loss: 0.00741 - acc: 0.9965 -- iter: 720/963\n",
            "Training Step: 24049  | total loss: \u001b[1m\u001b[32m0.06168\u001b[0m\u001b[0m | time: 0.266s\n",
            "| Adam | epoch: 199 | loss: 0.06168 - acc: 0.9719 -- iter: 728/963\n",
            "Training Step: 24050  | total loss: \u001b[1m\u001b[32m0.05552\u001b[0m\u001b[0m | time: 0.268s\n",
            "| Adam | epoch: 199 | loss: 0.05552 - acc: 0.9747 -- iter: 736/963\n",
            "Training Step: 24051  | total loss: \u001b[1m\u001b[32m0.05002\u001b[0m\u001b[0m | time: 0.271s\n",
            "| Adam | epoch: 199 | loss: 0.05002 - acc: 0.9772 -- iter: 744/963\n",
            "Training Step: 24052  | total loss: \u001b[1m\u001b[32m0.05250\u001b[0m\u001b[0m | time: 0.273s\n",
            "| Adam | epoch: 199 | loss: 0.05250 - acc: 0.9795 -- iter: 752/963\n",
            "Training Step: 24053  | total loss: \u001b[1m\u001b[32m0.04725\u001b[0m\u001b[0m | time: 0.276s\n",
            "| Adam | epoch: 199 | loss: 0.04725 - acc: 0.9816 -- iter: 760/963\n",
            "Training Step: 24054  | total loss: \u001b[1m\u001b[32m0.04254\u001b[0m\u001b[0m | time: 0.278s\n",
            "| Adam | epoch: 199 | loss: 0.04254 - acc: 0.9834 -- iter: 768/963\n",
            "Training Step: 24055  | total loss: \u001b[1m\u001b[32m0.03839\u001b[0m\u001b[0m | time: 0.281s\n",
            "| Adam | epoch: 199 | loss: 0.03839 - acc: 0.9851 -- iter: 776/963\n",
            "Training Step: 24056  | total loss: \u001b[1m\u001b[32m0.03456\u001b[0m\u001b[0m | time: 0.283s\n",
            "| Adam | epoch: 199 | loss: 0.03456 - acc: 0.9866 -- iter: 784/963\n",
            "Training Step: 24057  | total loss: \u001b[1m\u001b[32m0.03112\u001b[0m\u001b[0m | time: 0.287s\n",
            "| Adam | epoch: 199 | loss: 0.03112 - acc: 0.9879 -- iter: 792/963\n",
            "Training Step: 24058  | total loss: \u001b[1m\u001b[32m0.02803\u001b[0m\u001b[0m | time: 0.290s\n",
            "| Adam | epoch: 199 | loss: 0.02803 - acc: 0.9891 -- iter: 800/963\n",
            "Training Step: 24059  | total loss: \u001b[1m\u001b[32m0.02549\u001b[0m\u001b[0m | time: 0.292s\n",
            "| Adam | epoch: 199 | loss: 0.02549 - acc: 0.9902 -- iter: 808/963\n",
            "Training Step: 24060  | total loss: \u001b[1m\u001b[32m0.02299\u001b[0m\u001b[0m | time: 0.295s\n",
            "| Adam | epoch: 199 | loss: 0.02299 - acc: 0.9912 -- iter: 816/963\n",
            "Training Step: 24061  | total loss: \u001b[1m\u001b[32m0.02073\u001b[0m\u001b[0m | time: 0.298s\n",
            "| Adam | epoch: 199 | loss: 0.02073 - acc: 0.9921 -- iter: 824/963\n",
            "Training Step: 24062  | total loss: \u001b[1m\u001b[32m0.01869\u001b[0m\u001b[0m | time: 0.301s\n",
            "| Adam | epoch: 199 | loss: 0.01869 - acc: 0.9929 -- iter: 832/963\n",
            "Training Step: 24063  | total loss: \u001b[1m\u001b[32m0.01726\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 199 | loss: 0.01726 - acc: 0.9936 -- iter: 840/963\n",
            "Training Step: 24064  | total loss: \u001b[1m\u001b[32m0.01554\u001b[0m\u001b[0m | time: 0.306s\n",
            "| Adam | epoch: 199 | loss: 0.01554 - acc: 0.9942 -- iter: 848/963\n",
            "Training Step: 24065  | total loss: \u001b[1m\u001b[32m0.01401\u001b[0m\u001b[0m | time: 0.309s\n",
            "| Adam | epoch: 199 | loss: 0.01401 - acc: 0.9948 -- iter: 856/963\n",
            "Training Step: 24066  | total loss: \u001b[1m\u001b[32m0.03898\u001b[0m\u001b[0m | time: 0.311s\n",
            "| Adam | epoch: 199 | loss: 0.03898 - acc: 0.9828 -- iter: 864/963\n",
            "Training Step: 24067  | total loss: \u001b[1m\u001b[32m0.03509\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 199 | loss: 0.03509 - acc: 0.9845 -- iter: 872/963\n",
            "Training Step: 24068  | total loss: \u001b[1m\u001b[32m0.03159\u001b[0m\u001b[0m | time: 0.316s\n",
            "| Adam | epoch: 199 | loss: 0.03159 - acc: 0.9861 -- iter: 880/963\n",
            "Training Step: 24069  | total loss: \u001b[1m\u001b[32m0.04751\u001b[0m\u001b[0m | time: 0.318s\n",
            "| Adam | epoch: 199 | loss: 0.04751 - acc: 0.9750 -- iter: 888/963\n",
            "Training Step: 24070  | total loss: \u001b[1m\u001b[32m0.04278\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 199 | loss: 0.04278 - acc: 0.9775 -- iter: 896/963\n",
            "Training Step: 24071  | total loss: \u001b[1m\u001b[32m0.03854\u001b[0m\u001b[0m | time: 0.323s\n",
            "| Adam | epoch: 199 | loss: 0.03854 - acc: 0.9797 -- iter: 904/963\n",
            "Training Step: 24072  | total loss: \u001b[1m\u001b[32m0.03471\u001b[0m\u001b[0m | time: 0.325s\n",
            "| Adam | epoch: 199 | loss: 0.03471 - acc: 0.9818 -- iter: 912/963\n",
            "Training Step: 24073  | total loss: \u001b[1m\u001b[32m0.03583\u001b[0m\u001b[0m | time: 0.327s\n",
            "| Adam | epoch: 199 | loss: 0.03583 - acc: 0.9836 -- iter: 920/963\n",
            "Training Step: 24074  | total loss: \u001b[1m\u001b[32m0.03226\u001b[0m\u001b[0m | time: 0.330s\n",
            "| Adam | epoch: 199 | loss: 0.03226 - acc: 0.9852 -- iter: 928/963\n",
            "Training Step: 24075  | total loss: \u001b[1m\u001b[32m0.04687\u001b[0m\u001b[0m | time: 0.332s\n",
            "| Adam | epoch: 199 | loss: 0.04687 - acc: 0.9742 -- iter: 936/963\n",
            "Training Step: 24076  | total loss: \u001b[1m\u001b[32m0.04230\u001b[0m\u001b[0m | time: 0.334s\n",
            "| Adam | epoch: 199 | loss: 0.04230 - acc: 0.9768 -- iter: 944/963\n",
            "Training Step: 24077  | total loss: \u001b[1m\u001b[32m0.03808\u001b[0m\u001b[0m | time: 0.336s\n",
            "| Adam | epoch: 199 | loss: 0.03808 - acc: 0.9791 -- iter: 952/963\n",
            "Training Step: 24078  | total loss: \u001b[1m\u001b[32m0.03436\u001b[0m\u001b[0m | time: 0.339s\n",
            "| Adam | epoch: 199 | loss: 0.03436 - acc: 0.9812 -- iter: 960/963\n",
            "Training Step: 24079  | total loss: \u001b[1m\u001b[32m0.03096\u001b[0m\u001b[0m | time: 0.341s\n",
            "| Adam | epoch: 199 | loss: 0.03096 - acc: 0.9831 -- iter: 963/963\n",
            "--\n",
            "Training Step: 24080  | total loss: \u001b[1m\u001b[32m0.02787\u001b[0m\u001b[0m | time: 0.002s\n",
            "| Adam | epoch: 200 | loss: 0.02787 - acc: 0.9848 -- iter: 008/963\n",
            "Training Step: 24081  | total loss: \u001b[1m\u001b[32m0.02512\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 200 | loss: 0.02512 - acc: 0.9863 -- iter: 016/963\n",
            "Training Step: 24082  | total loss: \u001b[1m\u001b[32m0.02268\u001b[0m\u001b[0m | time: 0.007s\n",
            "| Adam | epoch: 200 | loss: 0.02268 - acc: 0.9877 -- iter: 024/963\n",
            "Training Step: 24083  | total loss: \u001b[1m\u001b[32m0.02043\u001b[0m\u001b[0m | time: 0.009s\n",
            "| Adam | epoch: 200 | loss: 0.02043 - acc: 0.9889 -- iter: 032/963\n",
            "Training Step: 24084  | total loss: \u001b[1m\u001b[32m0.01846\u001b[0m\u001b[0m | time: 0.011s\n",
            "| Adam | epoch: 200 | loss: 0.01846 - acc: 0.9900 -- iter: 040/963\n",
            "Training Step: 24085  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.013s\n",
            "| Adam | epoch: 200 | loss: 0.01661 - acc: 0.9910 -- iter: 048/963\n",
            "Training Step: 24086  | total loss: \u001b[1m\u001b[32m0.01497\u001b[0m\u001b[0m | time: 0.016s\n",
            "| Adam | epoch: 200 | loss: 0.01497 - acc: 0.9919 -- iter: 056/963\n",
            "Training Step: 24087  | total loss: \u001b[1m\u001b[32m0.01348\u001b[0m\u001b[0m | time: 0.018s\n",
            "| Adam | epoch: 200 | loss: 0.01348 - acc: 0.9927 -- iter: 064/963\n",
            "Training Step: 24088  | total loss: \u001b[1m\u001b[32m0.01215\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 200 | loss: 0.01215 - acc: 0.9934 -- iter: 072/963\n",
            "Training Step: 24089  | total loss: \u001b[1m\u001b[32m0.01095\u001b[0m\u001b[0m | time: 0.022s\n",
            "| Adam | epoch: 200 | loss: 0.01095 - acc: 0.9941 -- iter: 080/963\n",
            "Training Step: 24090  | total loss: \u001b[1m\u001b[32m0.00987\u001b[0m\u001b[0m | time: 0.025s\n",
            "| Adam | epoch: 200 | loss: 0.00987 - acc: 0.9947 -- iter: 088/963\n",
            "Training Step: 24091  | total loss: \u001b[1m\u001b[32m0.00893\u001b[0m\u001b[0m | time: 0.028s\n",
            "| Adam | epoch: 200 | loss: 0.00893 - acc: 0.9952 -- iter: 096/963\n",
            "Training Step: 24092  | total loss: \u001b[1m\u001b[32m0.01027\u001b[0m\u001b[0m | time: 0.031s\n",
            "| Adam | epoch: 200 | loss: 0.01027 - acc: 0.9957 -- iter: 104/963\n",
            "Training Step: 24093  | total loss: \u001b[1m\u001b[32m0.00926\u001b[0m\u001b[0m | time: 0.034s\n",
            "| Adam | epoch: 200 | loss: 0.00926 - acc: 0.9961 -- iter: 112/963\n",
            "Training Step: 24094  | total loss: \u001b[1m\u001b[32m0.00844\u001b[0m\u001b[0m | time: 0.036s\n",
            "| Adam | epoch: 200 | loss: 0.00844 - acc: 0.9965 -- iter: 120/963\n",
            "Training Step: 24095  | total loss: \u001b[1m\u001b[32m0.00761\u001b[0m\u001b[0m | time: 0.039s\n",
            "| Adam | epoch: 200 | loss: 0.00761 - acc: 0.9969 -- iter: 128/963\n",
            "Training Step: 24096  | total loss: \u001b[1m\u001b[32m0.02525\u001b[0m\u001b[0m | time: 0.042s\n",
            "| Adam | epoch: 200 | loss: 0.02525 - acc: 0.9847 -- iter: 136/963\n",
            "Training Step: 24097  | total loss: \u001b[1m\u001b[32m0.02274\u001b[0m\u001b[0m | time: 0.044s\n",
            "| Adam | epoch: 200 | loss: 0.02274 - acc: 0.9862 -- iter: 144/963\n",
            "Training Step: 24098  | total loss: \u001b[1m\u001b[32m0.02048\u001b[0m\u001b[0m | time: 0.047s\n",
            "| Adam | epoch: 200 | loss: 0.02048 - acc: 0.9876 -- iter: 152/963\n",
            "Training Step: 24099  | total loss: \u001b[1m\u001b[32m0.01844\u001b[0m\u001b[0m | time: 0.050s\n",
            "| Adam | epoch: 200 | loss: 0.01844 - acc: 0.9888 -- iter: 160/963\n",
            "Training Step: 24100  | total loss: \u001b[1m\u001b[32m0.01661\u001b[0m\u001b[0m | time: 0.053s\n",
            "| Adam | epoch: 200 | loss: 0.01661 - acc: 0.9899 -- iter: 168/963\n",
            "Training Step: 24101  | total loss: \u001b[1m\u001b[32m0.01504\u001b[0m\u001b[0m | time: 0.055s\n",
            "| Adam | epoch: 200 | loss: 0.01504 - acc: 0.9910 -- iter: 176/963\n",
            "Training Step: 24102  | total loss: \u001b[1m\u001b[32m0.01354\u001b[0m\u001b[0m | time: 0.058s\n",
            "| Adam | epoch: 200 | loss: 0.01354 - acc: 0.9919 -- iter: 184/963\n",
            "Training Step: 24103  | total loss: \u001b[1m\u001b[32m0.01220\u001b[0m\u001b[0m | time: 0.061s\n",
            "| Adam | epoch: 200 | loss: 0.01220 - acc: 0.9927 -- iter: 192/963\n",
            "Training Step: 24104  | total loss: \u001b[1m\u001b[32m0.01100\u001b[0m\u001b[0m | time: 0.063s\n",
            "| Adam | epoch: 200 | loss: 0.01100 - acc: 0.9934 -- iter: 200/963\n",
            "Training Step: 24105  | total loss: \u001b[1m\u001b[32m0.01738\u001b[0m\u001b[0m | time: 0.066s\n",
            "| Adam | epoch: 200 | loss: 0.01738 - acc: 0.9941 -- iter: 208/963\n",
            "Training Step: 24106  | total loss: \u001b[1m\u001b[32m0.01565\u001b[0m\u001b[0m | time: 0.069s\n",
            "| Adam | epoch: 200 | loss: 0.01565 - acc: 0.9947 -- iter: 216/963\n",
            "Training Step: 24107  | total loss: \u001b[1m\u001b[32m0.01410\u001b[0m\u001b[0m | time: 0.071s\n",
            "| Adam | epoch: 200 | loss: 0.01410 - acc: 0.9952 -- iter: 224/963\n",
            "Training Step: 24108  | total loss: \u001b[1m\u001b[32m0.01271\u001b[0m\u001b[0m | time: 0.074s\n",
            "| Adam | epoch: 200 | loss: 0.01271 - acc: 0.9957 -- iter: 232/963\n",
            "Training Step: 24109  | total loss: \u001b[1m\u001b[32m0.01146\u001b[0m\u001b[0m | time: 0.077s\n",
            "| Adam | epoch: 200 | loss: 0.01146 - acc: 0.9961 -- iter: 240/963\n",
            "Training Step: 24110  | total loss: \u001b[1m\u001b[32m0.01035\u001b[0m\u001b[0m | time: 0.079s\n",
            "| Adam | epoch: 200 | loss: 0.01035 - acc: 0.9965 -- iter: 248/963\n",
            "Training Step: 24111  | total loss: \u001b[1m\u001b[32m0.00932\u001b[0m\u001b[0m | time: 0.082s\n",
            "| Adam | epoch: 200 | loss: 0.00932 - acc: 0.9968 -- iter: 256/963\n",
            "Training Step: 24112  | total loss: \u001b[1m\u001b[32m0.00853\u001b[0m\u001b[0m | time: 0.085s\n",
            "| Adam | epoch: 200 | loss: 0.00853 - acc: 0.9972 -- iter: 264/963\n",
            "Training Step: 24113  | total loss: \u001b[1m\u001b[32m0.00772\u001b[0m\u001b[0m | time: 0.088s\n",
            "| Adam | epoch: 200 | loss: 0.00772 - acc: 0.9974 -- iter: 272/963\n",
            "Training Step: 24114  | total loss: \u001b[1m\u001b[32m0.00696\u001b[0m\u001b[0m | time: 0.091s\n",
            "| Adam | epoch: 200 | loss: 0.00696 - acc: 0.9977 -- iter: 280/963\n",
            "Training Step: 24115  | total loss: \u001b[1m\u001b[32m0.00632\u001b[0m\u001b[0m | time: 0.093s\n",
            "| Adam | epoch: 200 | loss: 0.00632 - acc: 0.9979 -- iter: 288/963\n",
            "Training Step: 24116  | total loss: \u001b[1m\u001b[32m0.00570\u001b[0m\u001b[0m | time: 0.096s\n",
            "| Adam | epoch: 200 | loss: 0.00570 - acc: 0.9981 -- iter: 296/963\n",
            "Training Step: 24117  | total loss: \u001b[1m\u001b[32m0.04299\u001b[0m\u001b[0m | time: 0.099s\n",
            "| Adam | epoch: 200 | loss: 0.04299 - acc: 0.9858 -- iter: 304/963\n",
            "Training Step: 24118  | total loss: \u001b[1m\u001b[32m0.03876\u001b[0m\u001b[0m | time: 0.102s\n",
            "| Adam | epoch: 200 | loss: 0.03876 - acc: 0.9872 -- iter: 312/963\n",
            "Training Step: 24119  | total loss: \u001b[1m\u001b[32m0.03489\u001b[0m\u001b[0m | time: 0.105s\n",
            "| Adam | epoch: 200 | loss: 0.03489 - acc: 0.9885 -- iter: 320/963\n",
            "Training Step: 24120  | total loss: \u001b[1m\u001b[32m0.03142\u001b[0m\u001b[0m | time: 0.108s\n",
            "| Adam | epoch: 200 | loss: 0.03142 - acc: 0.9897 -- iter: 328/963\n",
            "Training Step: 24121  | total loss: \u001b[1m\u001b[32m0.02829\u001b[0m\u001b[0m | time: 0.111s\n",
            "| Adam | epoch: 200 | loss: 0.02829 - acc: 0.9907 -- iter: 336/963\n",
            "Training Step: 24122  | total loss: \u001b[1m\u001b[32m0.02557\u001b[0m\u001b[0m | time: 0.113s\n",
            "| Adam | epoch: 200 | loss: 0.02557 - acc: 0.9916 -- iter: 344/963\n",
            "Training Step: 24123  | total loss: \u001b[1m\u001b[32m0.02302\u001b[0m\u001b[0m | time: 0.116s\n",
            "| Adam | epoch: 200 | loss: 0.02302 - acc: 0.9925 -- iter: 352/963\n",
            "Training Step: 24124  | total loss: \u001b[1m\u001b[32m0.02078\u001b[0m\u001b[0m | time: 0.119s\n",
            "| Adam | epoch: 200 | loss: 0.02078 - acc: 0.9932 -- iter: 360/963\n",
            "Training Step: 24125  | total loss: \u001b[1m\u001b[32m0.01871\u001b[0m\u001b[0m | time: 0.121s\n",
            "| Adam | epoch: 200 | loss: 0.01871 - acc: 0.9939 -- iter: 368/963\n",
            "Training Step: 24126  | total loss: \u001b[1m\u001b[32m0.01685\u001b[0m\u001b[0m | time: 0.124s\n",
            "| Adam | epoch: 200 | loss: 0.01685 - acc: 0.9945 -- iter: 376/963\n",
            "Training Step: 24127  | total loss: \u001b[1m\u001b[32m0.01518\u001b[0m\u001b[0m | time: 0.127s\n",
            "| Adam | epoch: 200 | loss: 0.01518 - acc: 0.9951 -- iter: 384/963\n",
            "Training Step: 24128  | total loss: \u001b[1m\u001b[32m0.01367\u001b[0m\u001b[0m | time: 0.129s\n",
            "| Adam | epoch: 200 | loss: 0.01367 - acc: 0.9956 -- iter: 392/963\n",
            "Training Step: 24129  | total loss: \u001b[1m\u001b[32m0.01234\u001b[0m\u001b[0m | time: 0.132s\n",
            "| Adam | epoch: 200 | loss: 0.01234 - acc: 0.9960 -- iter: 400/963\n",
            "Training Step: 24130  | total loss: \u001b[1m\u001b[32m0.01114\u001b[0m\u001b[0m | time: 0.134s\n",
            "| Adam | epoch: 200 | loss: 0.01114 - acc: 0.9964 -- iter: 408/963\n",
            "Training Step: 24131  | total loss: \u001b[1m\u001b[32m0.01005\u001b[0m\u001b[0m | time: 0.137s\n",
            "| Adam | epoch: 200 | loss: 0.01005 - acc: 0.9968 -- iter: 416/963\n",
            "Training Step: 24132  | total loss: \u001b[1m\u001b[32m0.00905\u001b[0m\u001b[0m | time: 0.140s\n",
            "| Adam | epoch: 200 | loss: 0.00905 - acc: 0.9971 -- iter: 424/963\n",
            "Training Step: 24133  | total loss: \u001b[1m\u001b[32m0.00816\u001b[0m\u001b[0m | time: 0.143s\n",
            "| Adam | epoch: 200 | loss: 0.00816 - acc: 0.9974 -- iter: 432/963\n",
            "Training Step: 24134  | total loss: \u001b[1m\u001b[32m0.03770\u001b[0m\u001b[0m | time: 0.145s\n",
            "| Adam | epoch: 200 | loss: 0.03770 - acc: 0.9851 -- iter: 440/963\n",
            "Training Step: 24135  | total loss: \u001b[1m\u001b[32m0.06318\u001b[0m\u001b[0m | time: 0.147s\n",
            "| Adam | epoch: 200 | loss: 0.06318 - acc: 0.9741 -- iter: 448/963\n",
            "Training Step: 24136  | total loss: \u001b[1m\u001b[32m0.05689\u001b[0m\u001b[0m | time: 0.150s\n",
            "| Adam | epoch: 200 | loss: 0.05689 - acc: 0.9767 -- iter: 456/963\n",
            "Training Step: 24137  | total loss: \u001b[1m\u001b[32m0.05129\u001b[0m\u001b[0m | time: 0.152s\n",
            "| Adam | epoch: 200 | loss: 0.05129 - acc: 0.9790 -- iter: 464/963\n",
            "Training Step: 24138  | total loss: \u001b[1m\u001b[32m0.04617\u001b[0m\u001b[0m | time: 0.155s\n",
            "| Adam | epoch: 200 | loss: 0.04617 - acc: 0.9811 -- iter: 472/963\n",
            "Training Step: 24139  | total loss: \u001b[1m\u001b[32m0.04156\u001b[0m\u001b[0m | time: 0.157s\n",
            "| Adam | epoch: 200 | loss: 0.04156 - acc: 0.9830 -- iter: 480/963\n",
            "Training Step: 24140  | total loss: \u001b[1m\u001b[32m0.03742\u001b[0m\u001b[0m | time: 0.160s\n",
            "| Adam | epoch: 200 | loss: 0.03742 - acc: 0.9847 -- iter: 488/963\n",
            "Training Step: 24141  | total loss: \u001b[1m\u001b[32m0.03369\u001b[0m\u001b[0m | time: 0.162s\n",
            "| Adam | epoch: 200 | loss: 0.03369 - acc: 0.9862 -- iter: 496/963\n",
            "Training Step: 24142  | total loss: \u001b[1m\u001b[32m0.03097\u001b[0m\u001b[0m | time: 0.165s\n",
            "| Adam | epoch: 200 | loss: 0.03097 - acc: 0.9876 -- iter: 504/963\n",
            "Training Step: 24143  | total loss: \u001b[1m\u001b[32m0.02791\u001b[0m\u001b[0m | time: 0.170s\n",
            "| Adam | epoch: 200 | loss: 0.02791 - acc: 0.9889 -- iter: 512/963\n",
            "Training Step: 24144  | total loss: \u001b[1m\u001b[32m0.02515\u001b[0m\u001b[0m | time: 0.173s\n",
            "| Adam | epoch: 200 | loss: 0.02515 - acc: 0.9900 -- iter: 520/963\n",
            "Training Step: 24145  | total loss: \u001b[1m\u001b[32m0.02267\u001b[0m\u001b[0m | time: 0.177s\n",
            "| Adam | epoch: 200 | loss: 0.02267 - acc: 0.9910 -- iter: 528/963\n",
            "Training Step: 24146  | total loss: \u001b[1m\u001b[32m0.02043\u001b[0m\u001b[0m | time: 0.180s\n",
            "| Adam | epoch: 200 | loss: 0.02043 - acc: 0.9919 -- iter: 536/963\n",
            "Training Step: 24147  | total loss: \u001b[1m\u001b[32m0.01839\u001b[0m\u001b[0m | time: 0.183s\n",
            "| Adam | epoch: 200 | loss: 0.01839 - acc: 0.9927 -- iter: 544/963\n",
            "Training Step: 24148  | total loss: \u001b[1m\u001b[32m0.01657\u001b[0m\u001b[0m | time: 0.187s\n",
            "| Adam | epoch: 200 | loss: 0.01657 - acc: 0.9934 -- iter: 552/963\n",
            "Training Step: 24149  | total loss: \u001b[1m\u001b[32m0.01492\u001b[0m\u001b[0m | time: 0.190s\n",
            "| Adam | epoch: 200 | loss: 0.01492 - acc: 0.9941 -- iter: 560/963\n",
            "Training Step: 24150  | total loss: \u001b[1m\u001b[32m0.01772\u001b[0m\u001b[0m | time: 0.194s\n",
            "| Adam | epoch: 200 | loss: 0.01772 - acc: 0.9947 -- iter: 568/963\n",
            "Training Step: 24151  | total loss: \u001b[1m\u001b[32m0.01595\u001b[0m\u001b[0m | time: 0.197s\n",
            "| Adam | epoch: 200 | loss: 0.01595 - acc: 0.9952 -- iter: 576/963\n",
            "Training Step: 24152  | total loss: \u001b[1m\u001b[32m0.01437\u001b[0m\u001b[0m | time: 0.201s\n",
            "| Adam | epoch: 200 | loss: 0.01437 - acc: 0.9957 -- iter: 584/963\n",
            "Training Step: 24153  | total loss: \u001b[1m\u001b[32m0.01294\u001b[0m\u001b[0m | time: 0.205s\n",
            "| Adam | epoch: 200 | loss: 0.01294 - acc: 0.9961 -- iter: 592/963\n",
            "Training Step: 24154  | total loss: \u001b[1m\u001b[32m0.01207\u001b[0m\u001b[0m | time: 0.208s\n",
            "| Adam | epoch: 200 | loss: 0.01207 - acc: 0.9965 -- iter: 600/963\n",
            "Training Step: 24155  | total loss: \u001b[1m\u001b[32m0.01090\u001b[0m\u001b[0m | time: 0.210s\n",
            "| Adam | epoch: 200 | loss: 0.01090 - acc: 0.9969 -- iter: 608/963\n",
            "Training Step: 24156  | total loss: \u001b[1m\u001b[32m0.00985\u001b[0m\u001b[0m | time: 0.213s\n",
            "| Adam | epoch: 200 | loss: 0.00985 - acc: 0.9972 -- iter: 616/963\n",
            "Training Step: 24157  | total loss: \u001b[1m\u001b[32m0.00890\u001b[0m\u001b[0m | time: 0.216s\n",
            "| Adam | epoch: 200 | loss: 0.00890 - acc: 0.9975 -- iter: 624/963\n",
            "Training Step: 24158  | total loss: \u001b[1m\u001b[32m0.00802\u001b[0m\u001b[0m | time: 0.219s\n",
            "| Adam | epoch: 200 | loss: 0.00802 - acc: 0.9977 -- iter: 632/963\n",
            "Training Step: 24159  | total loss: \u001b[1m\u001b[32m0.00722\u001b[0m\u001b[0m | time: 0.222s\n",
            "| Adam | epoch: 200 | loss: 0.00722 - acc: 0.9979 -- iter: 640/963\n",
            "Training Step: 24160  | total loss: \u001b[1m\u001b[32m0.00685\u001b[0m\u001b[0m | time: 0.225s\n",
            "| Adam | epoch: 200 | loss: 0.00685 - acc: 0.9981 -- iter: 648/963\n",
            "Training Step: 24161  | total loss: \u001b[1m\u001b[32m0.00619\u001b[0m\u001b[0m | time: 0.228s\n",
            "| Adam | epoch: 200 | loss: 0.00619 - acc: 0.9983 -- iter: 656/963\n",
            "Training Step: 24162  | total loss: \u001b[1m\u001b[32m0.00558\u001b[0m\u001b[0m | time: 0.231s\n",
            "| Adam | epoch: 200 | loss: 0.00558 - acc: 0.9985 -- iter: 664/963\n",
            "Training Step: 24163  | total loss: \u001b[1m\u001b[32m0.00505\u001b[0m\u001b[0m | time: 0.233s\n",
            "| Adam | epoch: 200 | loss: 0.00505 - acc: 0.9986 -- iter: 672/963\n",
            "Training Step: 24164  | total loss: \u001b[1m\u001b[32m0.00456\u001b[0m\u001b[0m | time: 0.243s\n",
            "| Adam | epoch: 200 | loss: 0.00456 - acc: 0.9988 -- iter: 680/963\n",
            "Training Step: 24165  | total loss: \u001b[1m\u001b[32m0.00412\u001b[0m\u001b[0m | time: 0.246s\n",
            "| Adam | epoch: 200 | loss: 0.00412 - acc: 0.9989 -- iter: 688/963\n",
            "Training Step: 24166  | total loss: \u001b[1m\u001b[32m0.00371\u001b[0m\u001b[0m | time: 0.250s\n",
            "| Adam | epoch: 200 | loss: 0.00371 - acc: 0.9990 -- iter: 696/963\n",
            "Training Step: 24167  | total loss: \u001b[1m\u001b[32m0.00336\u001b[0m\u001b[0m | time: 0.253s\n",
            "| Adam | epoch: 200 | loss: 0.00336 - acc: 0.9991 -- iter: 704/963\n",
            "Training Step: 24168  | total loss: \u001b[1m\u001b[32m0.01067\u001b[0m\u001b[0m | time: 0.255s\n",
            "| Adam | epoch: 200 | loss: 0.01067 - acc: 0.9992 -- iter: 712/963\n",
            "Training Step: 24169  | total loss: \u001b[1m\u001b[32m0.00964\u001b[0m\u001b[0m | time: 0.259s\n",
            "| Adam | epoch: 200 | loss: 0.00964 - acc: 0.9993 -- iter: 720/963\n",
            "Training Step: 24170  | total loss: \u001b[1m\u001b[32m0.00871\u001b[0m\u001b[0m | time: 0.262s\n",
            "| Adam | epoch: 200 | loss: 0.00871 - acc: 0.9994 -- iter: 728/963\n",
            "Training Step: 24171  | total loss: \u001b[1m\u001b[32m0.01639\u001b[0m\u001b[0m | time: 0.264s\n",
            "| Adam | epoch: 200 | loss: 0.01639 - acc: 0.9994 -- iter: 736/963\n",
            "Training Step: 24172  | total loss: \u001b[1m\u001b[32m0.05442\u001b[0m\u001b[0m | time: 0.267s\n",
            "| Adam | epoch: 200 | loss: 0.05442 - acc: 0.9870 -- iter: 744/963\n",
            "Training Step: 24173  | total loss: \u001b[1m\u001b[32m0.04899\u001b[0m\u001b[0m | time: 0.270s\n",
            "| Adam | epoch: 200 | loss: 0.04899 - acc: 0.9883 -- iter: 752/963\n",
            "Training Step: 24174  | total loss: \u001b[1m\u001b[32m0.04410\u001b[0m\u001b[0m | time: 0.272s\n",
            "| Adam | epoch: 200 | loss: 0.04410 - acc: 0.9895 -- iter: 760/963\n",
            "Training Step: 24175  | total loss: \u001b[1m\u001b[32m0.04077\u001b[0m\u001b[0m | time: 0.276s\n",
            "| Adam | epoch: 200 | loss: 0.04077 - acc: 0.9905 -- iter: 768/963\n",
            "Training Step: 24176  | total loss: \u001b[1m\u001b[32m0.03671\u001b[0m\u001b[0m | time: 0.280s\n",
            "| Adam | epoch: 200 | loss: 0.03671 - acc: 0.9915 -- iter: 776/963\n",
            "Training Step: 24177  | total loss: \u001b[1m\u001b[32m0.03304\u001b[0m\u001b[0m | time: 0.283s\n",
            "| Adam | epoch: 200 | loss: 0.03304 - acc: 0.9923 -- iter: 784/963\n",
            "Training Step: 24178  | total loss: \u001b[1m\u001b[32m0.03772\u001b[0m\u001b[0m | time: 0.288s\n",
            "| Adam | epoch: 200 | loss: 0.03772 - acc: 0.9931 -- iter: 792/963\n",
            "Training Step: 24179  | total loss: \u001b[1m\u001b[32m0.03396\u001b[0m\u001b[0m | time: 0.291s\n",
            "| Adam | epoch: 200 | loss: 0.03396 - acc: 0.9938 -- iter: 800/963\n",
            "Training Step: 24180  | total loss: \u001b[1m\u001b[32m0.03057\u001b[0m\u001b[0m | time: 0.294s\n",
            "| Adam | epoch: 200 | loss: 0.03057 - acc: 0.9944 -- iter: 808/963\n",
            "Training Step: 24181  | total loss: \u001b[1m\u001b[32m0.02752\u001b[0m\u001b[0m | time: 0.297s\n",
            "| Adam | epoch: 200 | loss: 0.02752 - acc: 0.9950 -- iter: 816/963\n",
            "Training Step: 24182  | total loss: \u001b[1m\u001b[32m0.02478\u001b[0m\u001b[0m | time: 0.300s\n",
            "| Adam | epoch: 200 | loss: 0.02478 - acc: 0.9955 -- iter: 824/963\n",
            "Training Step: 24183  | total loss: \u001b[1m\u001b[32m0.02277\u001b[0m\u001b[0m | time: 0.303s\n",
            "| Adam | epoch: 200 | loss: 0.02277 - acc: 0.9959 -- iter: 832/963\n",
            "Training Step: 24184  | total loss: \u001b[1m\u001b[32m0.05331\u001b[0m\u001b[0m | time: 0.307s\n",
            "| Adam | epoch: 200 | loss: 0.05331 - acc: 0.9838 -- iter: 840/963\n",
            "Training Step: 24185  | total loss: \u001b[1m\u001b[32m0.06769\u001b[0m\u001b[0m | time: 0.310s\n",
            "| Adam | epoch: 200 | loss: 0.06769 - acc: 0.9729 -- iter: 848/963\n",
            "Training Step: 24186  | total loss: \u001b[1m\u001b[32m0.06110\u001b[0m\u001b[0m | time: 0.313s\n",
            "| Adam | epoch: 200 | loss: 0.06110 - acc: 0.9756 -- iter: 856/963\n",
            "Training Step: 24187  | total loss: \u001b[1m\u001b[32m0.05499\u001b[0m\u001b[0m | time: 0.315s\n",
            "| Adam | epoch: 200 | loss: 0.05499 - acc: 0.9781 -- iter: 864/963\n",
            "Training Step: 24188  | total loss: \u001b[1m\u001b[32m0.04951\u001b[0m\u001b[0m | time: 0.318s\n",
            "| Adam | epoch: 200 | loss: 0.04951 - acc: 0.9803 -- iter: 872/963\n",
            "Training Step: 24189  | total loss: \u001b[1m\u001b[32m0.04457\u001b[0m\u001b[0m | time: 0.320s\n",
            "| Adam | epoch: 200 | loss: 0.04457 - acc: 0.9822 -- iter: 880/963\n",
            "Training Step: 24190  | total loss: \u001b[1m\u001b[32m0.04013\u001b[0m\u001b[0m | time: 0.323s\n",
            "| Adam | epoch: 200 | loss: 0.04013 - acc: 0.9840 -- iter: 888/963\n",
            "Training Step: 24191  | total loss: \u001b[1m\u001b[32m0.03614\u001b[0m\u001b[0m | time: 0.326s\n",
            "| Adam | epoch: 200 | loss: 0.03614 - acc: 0.9856 -- iter: 896/963\n",
            "Training Step: 24192  | total loss: \u001b[1m\u001b[32m0.03254\u001b[0m\u001b[0m | time: 0.329s\n",
            "| Adam | epoch: 200 | loss: 0.03254 - acc: 0.9871 -- iter: 904/963\n",
            "Training Step: 24193  | total loss: \u001b[1m\u001b[32m0.02930\u001b[0m\u001b[0m | time: 0.332s\n",
            "| Adam | epoch: 200 | loss: 0.02930 - acc: 0.9884 -- iter: 912/963\n",
            "Training Step: 24194  | total loss: \u001b[1m\u001b[32m0.02641\u001b[0m\u001b[0m | time: 0.334s\n",
            "| Adam | epoch: 200 | loss: 0.02641 - acc: 0.9895 -- iter: 920/963\n",
            "Training Step: 24195  | total loss: \u001b[1m\u001b[32m0.03083\u001b[0m\u001b[0m | time: 0.338s\n",
            "| Adam | epoch: 200 | loss: 0.03083 - acc: 0.9906 -- iter: 928/963\n",
            "Training Step: 24196  | total loss: \u001b[1m\u001b[32m0.02776\u001b[0m\u001b[0m | time: 0.343s\n",
            "| Adam | epoch: 200 | loss: 0.02776 - acc: 0.9915 -- iter: 936/963\n",
            "Training Step: 24197  | total loss: \u001b[1m\u001b[32m0.02501\u001b[0m\u001b[0m | time: 0.347s\n",
            "| Adam | epoch: 200 | loss: 0.02501 - acc: 0.9924 -- iter: 944/963\n",
            "Training Step: 24198  | total loss: \u001b[1m\u001b[32m0.02256\u001b[0m\u001b[0m | time: 0.354s\n",
            "| Adam | epoch: 200 | loss: 0.02256 - acc: 0.9931 -- iter: 952/963\n",
            "Training Step: 24199  | total loss: \u001b[1m\u001b[32m0.02033\u001b[0m\u001b[0m | time: 0.358s\n",
            "| Adam | epoch: 200 | loss: 0.02033 - acc: 0.9938 -- iter: 960/963\n",
            "Training Step: 24200  | total loss: \u001b[1m\u001b[32m0.01834\u001b[0m\u001b[0m | time: 0.363s\n",
            "| Adam | epoch: 200 | loss: 0.01834 - acc: 0.9944 -- iter: 963/963\n",
            "--\n",
            "model created None\n"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "model.fit(train_x, train_y, n_epoch=200, batch_size=8, show_metric=True)\n",
        "model.save('model.tflearn')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wBsDmK_FdaI4"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open(\"training_data\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tCdwKwoSs2ub"
      },
      "outputs": [],
      "source": [
        "data = pickle.load( open( \"training_data\", \"rb\") )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DcjT-rHitr8z"
      },
      "outputs": [],
      "source": [
        "with open('intents.json') as json_data:\n",
        "  intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mToYM8Qetscp"
      },
      "outputs": [],
      "source": [
        "model.load('./model.tflearn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jZDNowMQtsek"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "  return sentence_words\n",
        "\n",
        "def bow(sentence, words, show_details=False):\n",
        "  sentence_words = clean_up_sentence(sentence)\n",
        "  bag = [0]*len(words)\n",
        "  for s in sentence_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w == s:\n",
        "        bag[i] = 1\n",
        "        if show_details:\n",
        "          print(\"found in bag: %s\" % w)\n",
        "\n",
        "  return(np.array(bag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SOPJZM_KtsiX"
      },
      "outputs": [],
      "source": [
        "context ={}\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "  results = model.predict([bow(sentence, words)])[0]\n",
        "  results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "  results.sort(key=lambda x: x[1], reverse=True)\n",
        "  return_list = []\n",
        "  for r in results:\n",
        "    return_list.append((classes[r[0]], r[1]))\n",
        "  return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "  results = classify(sentence)\n",
        "  if results:\n",
        "    while results:\n",
        "      for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        # a random response from the intent\n",
        "                        return print(random.choice(i['responses']))\n",
        "\n",
        "      results.pop(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4nrv-Isvxp6s",
        "outputId": "1dbdd880-da8b-40da-c969-8471b953e25e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('stock', 0.9956429)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classify('What is Stock?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-aTnGMTRyYuw",
        "outputId": "f67c0a01-c988-4d2c-9623-cfd5ca2405b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A stock is a type of investment in a company. It is also known as equity, is a security that represents the ownership of a fraction of the issuing corporation.\n",
            " Also we can say, A stock is a general term used to describe the ownership certificates of any company.\n",
            " you can find more info on this link: https://economictimes.indiatimes.com/definition/stocks\n"
          ]
        }
      ],
      "source": [
        "response('stock')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Yuc2b9unzBoX",
        "outputId": "a58c1b89-a561-4b50-8edf-07a311b9e909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A share, on the other hand, refers to the stock certificate of a particular company. Holding a particular company's share makes you a shareholder.\n",
            " for more info: https://www.investopedia.com/terms/s/shares.asp\n"
          ]
        }
      ],
      "source": [
        "response('shares')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UD6C4ulA1_Iu",
        "outputId": "b752f12a-0232-48a2-a402-e3bc8403c389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tag: greetings\n",
            "What Can I Do For You?\n"
          ]
        }
      ],
      "source": [
        "response('hello', show_details=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}